{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10789676,"sourceType":"datasetVersion","datasetId":6695691}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom transformers import BertTokenizer, BertModel\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"820a0c9c-9169-4aff-a454-26eb58d5ecd8","_cell_guid":"4556d261-d0b0-4b8a-a180-ed6bea44ba55","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T16:35:30.755563Z","iopub.execute_input":"2025-03-26T16:35:30.755766Z","iopub.status.idle":"2025-03-26T16:36:05.184637Z","shell.execute_reply.started":"2025-03-26T16:35:30.755744Z","shell.execute_reply":"2025-03-26T16:36:05.183960Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/filtered-and-translated-nlp/filr.csv')","metadata":{"_uuid":"b12b1693-96de-438f-b5bd-0be80408e2de","_cell_guid":"8aea64a9-6ad6-43c8-833b-ab922fafa30c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T16:36:05.185450Z","iopub.execute_input":"2025-03-26T16:36:05.186010Z","iopub.status.idle":"2025-03-26T16:36:05.238931Z","shell.execute_reply.started":"2025-03-26T16:36:05.185984Z","shell.execute_reply":"2025-03-26T16:36:05.238310Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"X = data['Translated']\ny = data['Label']","metadata":{"_uuid":"f324c517-6ce5-4951-9dca-a423158612f8","_cell_guid":"bc70eafa-cd6d-4fb7-be0c-4482f90eebb2","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T16:36:05.239803Z","iopub.execute_input":"2025-03-26T16:36:05.240072Z","iopub.status.idle":"2025-03-26T16:36:05.249233Z","shell.execute_reply.started":"2025-03-26T16:36:05.240037Z","shell.execute_reply":"2025-03-26T16:36:05.248457Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"X","metadata":{"_uuid":"f7ac0f97-3ada-4ef2-bbfb-c2fab978d7b4","_cell_guid":"c03d2dcb-94e4-4f34-a030-670e062b5b26","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T13:19:53.646209Z","iopub.execute_input":"2025-03-26T13:19:53.646416Z","iopub.status.idle":"2025-03-26T13:19:53.667971Z","shell.execute_reply.started":"2025-03-26T13:19:53.646398Z","shell.execute_reply":"2025-03-26T13:19:53.667020Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"0      After hacking state TV by replacing propaganda...\n1      #flowers #lovers\\nMake love not war.\\nMarc Cha...\n2      If only we all showed more love and understand...\n3      Who are the soldiers we see in the videos? Are...\n4      I didn't think #Salvini could make his positio...\n                             ...                        \n475    If I write that Ms. #Zelensky was allegedly sp...\n476    #Zelensky and his wife #OlenaZelenska bought a...\n477    ALL UNITED AGAINST DRAGONS\\nAGAINST WAR\\nAGAIN...\n478    ALL UNITED AGAINST DRAGONS\\nAGAINST WAR\\nAGAIN...\n479    ALL UNITED AGAINST DRAGONS\\nAGAINST WAR\\nAGAIN...\nName: Translated, Length: 480, dtype: object"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"y","metadata":{"_uuid":"fd473d00-ea62-4e6e-97dd-e5be0e554983","_cell_guid":"7dc367df-0e09-49ba-a522-0c957629e44e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T13:19:53.668905Z","iopub.execute_input":"2025-03-26T13:19:53.669217Z","iopub.status.idle":"2025-03-26T13:19:53.686321Z","shell.execute_reply.started":"2025-03-26T13:19:53.669186Z","shell.execute_reply":"2025-03-26T13:19:53.685506Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"0      1\n1      2\n2      2\n3      2\n4      1\n      ..\n475    1\n476    0\n477    2\n478    2\n479    2\nName: Label, Length: 480, dtype: int64"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')","metadata":{"_uuid":"c172be2d-0f81-4396-82ec-d3bb6c89ec92","_cell_guid":"ce50c640-2218-4b16-8d4e-99ab22e3bf5d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T13:19:53.688123Z","iopub.execute_input":"2025-03-26T13:19:53.688308Z","iopub.status.idle":"2025-03-26T13:19:57.076997Z","shell.execute_reply.started":"2025-03-26T13:19:53.688292Z","shell.execute_reply":"2025-03-26T13:19:57.076108Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2d5a99d755b418c89b7b839a57b2496"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13b4b91806974c78bdc6c56388a4cf69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bf8d79fd7544ed4b768fa52b10eb86b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0d9d400ace7406b803f3cf642f1e84a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"829d11fbb179419782a1ab7ad7f0e6be"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"def generate_embeddings(texts):\n    model.eval()\n    embeddings = []\n    with torch.no_grad():\n        for text in texts:\n            inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n            outputs = model(**inputs)\n            pooled_output = outputs.pooler_output.squeeze().numpy()\n            embeddings.append(pooled_output)\n    return np.array(embeddings)\n\n# Generate embeddings\nprint(\"Generating BERT embeddings...\")\nX_embeddings = generate_embeddings(X)","metadata":{"_uuid":"d89783d0-215b-4acb-b854-991b4bef4f25","_cell_guid":"c9f614b1-157e-4b5c-a6f6-d0f563de464c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T13:19:57.078214Z","iopub.execute_input":"2025-03-26T13:19:57.078536Z","iopub.status.idle":"2025-03-26T13:20:45.705011Z","shell.execute_reply.started":"2025-03-26T13:19:57.078506Z","shell.execute_reply":"2025-03-26T13:20:45.703896Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Generating BERT embeddings...\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X_embeddings, y, test_size=0.2, random_state=42)\n\n# Train an SVM classifier\nprint(\"Training SVM...\")\nclf = SVC(kernel='linear')\nclf.fit(X_train, y_train)\n\n# Predict and evaluate\ny_pred = clf.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy * 100:.2f}%\")","metadata":{"_uuid":"3ce84c7e-9bfa-4cf4-809d-91e668faffc0","_cell_guid":"21686bc9-715c-4054-827b-915e792dfee3","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T13:20:45.706074Z","iopub.execute_input":"2025-03-26T13:20:45.706483Z","iopub.status.idle":"2025-03-26T13:20:45.837889Z","shell.execute_reply.started":"2025-03-26T13:20:45.706445Z","shell.execute_reply":"2025-03-26T13:20:45.836849Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Training SVM...\nAccuracy: 41.67%\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"_uuid":"a275a7b8-2e9d-4393-9dc0-5ece45107d95","_cell_guid":"e9a07a01-fa9f-43fd-a3b0-81e73e5b41dc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Follow a repo \nFor finetuning","metadata":{"_uuid":"747f90a3-1ba4-4579-97b8-c0217ef96b83","_cell_guid":"3dcc202b-2f28-46b1-9ac8-8ce25bebecbb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"!pip install transformers","metadata":{"_uuid":"c8faea0e-f580-4287-9d64-039cfb8bc4c8","_cell_guid":"26863294-31a6-4121-a618-186dd0b9aa48","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T02:50:32.387609Z","iopub.execute_input":"2025-03-26T02:50:32.387906Z","iopub.status.idle":"2025-03-26T02:50:38.027333Z","shell.execute_reply.started":"2025-03-26T02:50:32.387879Z","shell.execute_reply":"2025-03-26T02:50:38.026133Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.12.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nimport transformers\nfrom transformers import AutoModel, BertTokenizerFast\n\n# specify GPU\ndevice = torch.device(\"cuda\")","metadata":{"_uuid":"3a86378d-0af3-4900-bf8b-50bf7745eca4","_cell_guid":"243292f3-0838-4162-bc86-dc96bffff454","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T02:50:38.028669Z","iopub.execute_input":"2025-03-26T02:50:38.029035Z","iopub.status.idle":"2025-03-26T02:50:38.062060Z","shell.execute_reply.started":"2025-03-26T02:50:38.028997Z","shell.execute_reply":"2025-03-26T02:50:38.061468Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/filtered-and-translated-nlp/filr.csv')","metadata":{"_uuid":"b01a6be1-06de-4075-96f2-74e1410308e3","_cell_guid":"0e477476-d020-46ed-bc4f-08325cc77a81","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T02:50:38.062842Z","iopub.execute_input":"2025-03-26T02:50:38.063059Z","iopub.status.idle":"2025-03-26T02:50:38.096223Z","shell.execute_reply.started":"2025-03-26T02:50:38.063039Z","shell.execute_reply":"2025-03-26T02:50:38.095161Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"X = data['Translated']\ny = data['Label']","metadata":{"_uuid":"4f417171-73cd-44b9-b235-1728ce9deda0","_cell_guid":"146c98c0-99f1-4386-8217-03c6aa60587e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T02:50:38.097180Z","iopub.execute_input":"2025-03-26T02:50:38.097499Z","iopub.status.idle":"2025-03-26T02:50:38.101177Z","shell.execute_reply.started":"2025-03-26T02:50:38.097469Z","shell.execute_reply":"2025-03-26T02:50:38.100534Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"y.value_counts(normalize = True)","metadata":{"_uuid":"b084b2e3-645a-4785-91fd-0669c3830dae","_cell_guid":"96863ecc-335a-4d1e-b53a-f0d3ca9998bd","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T02:50:38.102062Z","iopub.execute_input":"2025-03-26T02:50:38.102294Z","iopub.status.idle":"2025-03-26T02:50:38.132968Z","shell.execute_reply.started":"2025-03-26T02:50:38.102274Z","shell.execute_reply":"2025-03-26T02:50:38.132338Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"Label\n2    0.431250\n1    0.235417\n3    0.172917\n0    0.160417\nName: proportion, dtype: float64"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"train_text, temp_text, train_labels, temp_labels = train_test_split(X, y, \n                                                                    random_state=2018, \n                                                                    test_size=0.3, \n                                                                    stratify=y)\n\n# we will use temp_text and temp_labels to create validation and test set\nval_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n                                                                random_state=2018, \n                                                                test_size=0.5, \n                                                                stratify=temp_labels)","metadata":{"_uuid":"70d68f67-0c46-49be-946c-e157623a89f8","_cell_guid":"ed7a0d9d-697c-477a-a5a0-68eed7a7b5ea","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T02:50:38.133877Z","iopub.execute_input":"2025-03-26T02:50:38.134145Z","iopub.status.idle":"2025-03-26T02:50:38.159054Z","shell.execute_reply.started":"2025-03-26T02:50:38.134117Z","shell.execute_reply":"2025-03-26T02:50:38.158375Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"bert = AutoModel.from_pretrained('bert-base-uncased')\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')","metadata":{"_uuid":"6f71cb1e-bbef-4dc6-bff7-f5dc6421f0a6","_cell_guid":"fab5b8a2-0757-41e2-ad42-168d324074b8","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T02:50:38.160014Z","iopub.execute_input":"2025-03-26T02:50:38.160301Z","iopub.status.idle":"2025-03-26T02:50:39.180125Z","shell.execute_reply.started":"2025-03-26T02:50:38.160272Z","shell.execute_reply":"2025-03-26T02:50:39.179095Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# sample data\ntext = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\n\n# encode text\nsent_id = tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False)","metadata":{"_uuid":"9102ce92-0c4d-4112-bd84-bf978c41fc3a","_cell_guid":"5f73af2d-5b96-4334-821e-638759dbc5f9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T02:50:39.181131Z","iopub.execute_input":"2025-03-26T02:50:39.181507Z","iopub.status.idle":"2025-03-26T02:50:39.194673Z","shell.execute_reply.started":"2025-03-26T02:50:39.181472Z","shell.execute_reply":"2025-03-26T02:50:39.193809Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"print(sent_id)","metadata":{"_uuid":"74e693e5-1f6e-4c4b-b685-d2da776dade9","_cell_guid":"bd5ffb2f-071f-42b2-940e-c01d96994ea5","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T02:50:39.195692Z","iopub.execute_input":"2025-03-26T02:50:39.195944Z","iopub.status.idle":"2025-03-26T02:50:39.215019Z","shell.execute_reply.started":"2025-03-26T02:50:39.195921Z","shell.execute_reply":"2025-03-26T02:50:39.214026Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"{'input_ids': [[101, 2023, 2003, 1037, 14324, 2944, 14924, 4818, 102, 0], [101, 2057, 2097, 2986, 1011, 8694, 1037, 14324, 2944, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"_uuid":"4d31201f-22bb-4bd2-8d14-db9fc13393ff","_cell_guid":"ea5e5d9f-8a5e-4720-bec4-df6d51e1afc4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tokenization","metadata":{"_uuid":"b91d4331-f26f-407c-ad2b-350fdbd1dffd","_cell_guid":"da966a1d-9f21-4161-b929-723f78b01ffb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"seq_len = [len(i.split()) for i in train_text]\n\npd.Series(seq_len).hist(bins = 30)","metadata":{"_uuid":"7a47ed2d-59cd-4688-9fe6-4a97c26483b4","_cell_guid":"88bbc7f3-13ae-477d-ac41-e8b4374bfb21","trusted":true,"collapsed":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:39.216125Z","iopub.execute_input":"2025-03-26T02:50:39.216476Z","iopub.status.idle":"2025-03-26T02:50:39.601109Z","shell.execute_reply.started":"2025-03-26T02:50:39.216437Z","shell.execute_reply":"2025-03-26T02:50:39.600386Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"<Axes: >"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"max_seq_len = max(seq_len)","metadata":{"_uuid":"1d5c036a-5119-4c70-843f-39d1f4180a84","_cell_guid":"3ef2a485-2291-4258-b6c1-97e7ded974b9","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:39.601878Z","iopub.execute_input":"2025-03-26T02:50:39.602142Z","iopub.status.idle":"2025-03-26T02:50:39.606033Z","shell.execute_reply.started":"2025-03-26T02:50:39.602119Z","shell.execute_reply":"2025-03-26T02:50:39.605081Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"max_seq_len","metadata":{"_uuid":"31fa9e1b-46ea-4748-a534-41614c67a1e2","_cell_guid":"1e36a491-8104-42ad-9477-2c4e337322b6","trusted":true,"collapsed":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:39.610061Z","iopub.execute_input":"2025-03-26T02:50:39.610291Z","iopub.status.idle":"2025-03-26T02:50:39.627931Z","shell.execute_reply.started":"2025-03-26T02:50:39.610270Z","shell.execute_reply":"2025-03-26T02:50:39.627267Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"58"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"# tokenize and encode sequences in the training set\ntokens_train = tokenizer.batch_encode_plus(\n    train_text.tolist(),\n    max_length = max_seq_len,\n    pad_to_max_length=True,\n    truncation=True,\n    return_token_type_ids=False\n)\n\n# tokenize and encode sequences in the validation set\ntokens_val = tokenizer.batch_encode_plus(\n    val_text.tolist(),\n    max_length = max_seq_len,\n    pad_to_max_length=True,\n    truncation=True,\n    return_token_type_ids=False\n)\n\n# tokenize and encode sequences in the test set\ntokens_test = tokenizer.batch_encode_plus(\n    test_text.tolist(),\n    max_length = max_seq_len,\n    pad_to_max_length=True,\n    truncation=True,\n    return_token_type_ids=False\n)","metadata":{"_uuid":"1150ca83-91fd-4a36-af81-d8c45e7f5b07","_cell_guid":"cd5d472c-30d7-4f91-acd2-2ef48907ae00","trusted":true,"collapsed":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:39.629689Z","iopub.execute_input":"2025-03-26T02:50:39.629883Z","iopub.status.idle":"2025-03-26T02:50:39.689739Z","shell.execute_reply.started":"2025-03-26T02:50:39.629866Z","shell.execute_reply":"2025-03-26T02:50:39.688789Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2673: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"\n# for train set\ntrain_seq = torch.tensor(tokens_train['input_ids'])\ntrain_mask = torch.tensor(tokens_train['attention_mask'])\ntrain_y = torch.tensor(train_labels.tolist())\n\n# for validation set\nval_seq = torch.tensor(tokens_val['input_ids'])\nval_mask = torch.tensor(tokens_val['attention_mask'])\nval_y = torch.tensor(val_labels.tolist())\n\n# for test set\ntest_seq = torch.tensor(tokens_test['input_ids'])\ntest_mask = torch.tensor(tokens_test['attention_mask'])\ntest_y = torch.tensor(test_labels.tolist())","metadata":{"_uuid":"ff701258-6d64-4002-bad9-415b147ef0b9","_cell_guid":"31c9a293-a1dd-44b4-89b3-0484f379e058","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:39.690292Z","iopub.execute_input":"2025-03-26T02:50:39.690534Z","iopub.status.idle":"2025-03-26T02:50:39.704164Z","shell.execute_reply.started":"2025-03-26T02:50:39.690515Z","shell.execute_reply":"2025-03-26T02:50:39.703323Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n#define a batch size\nbatch_size = 32\n\n# wrap tensors\ntrain_data = TensorDataset(train_seq, train_mask, train_y)\n\n# sampler for sampling the data during training\ntrain_sampler = RandomSampler(train_data)\n\n# dataLoader for train set\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# wrap tensors\nval_data = TensorDataset(val_seq, val_mask, val_y)\n\n# sampler for sampling the data during training\nval_sampler = SequentialSampler(val_data)\n\n# dataLoader for validation set\nval_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)","metadata":{"_uuid":"d7a932f3-26c8-4528-aafd-4f1f54365554","_cell_guid":"ebc4e20b-2b60-4d44-998b-96f7d1cdcd40","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:39.705091Z","iopub.execute_input":"2025-03-26T02:50:39.705393Z","iopub.status.idle":"2025-03-26T02:50:39.730961Z","shell.execute_reply.started":"2025-03-26T02:50:39.705367Z","shell.execute_reply":"2025-03-26T02:50:39.730094Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"\n\n# freeze all the parameters\nfor param in bert.parameters():\n    param.requires_grad = False","metadata":{"_uuid":"dbe27428-dd88-4121-a104-76e4ec16070a","_cell_guid":"87712a31-e7fb-4a18-991c-23e6529cf232","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:39.731796Z","iopub.execute_input":"2025-03-26T02:50:39.732114Z","iopub.status.idle":"2025-03-26T02:50:39.750541Z","shell.execute_reply.started":"2025-03-26T02:50:39.732091Z","shell.execute_reply":"2025-03-26T02:50:39.749727Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"class BERT_Arch(nn.Module):\n\n    def __init__(self, bert):\n      \n      super(BERT_Arch, self).__init__()\n\n      self.bert = bert \n      \n      # dropout layer\n      self.dropout = nn.Dropout(0.1)\n      \n      # relu activation function\n      self.relu =  nn.ReLU()\n\n      # dense layer 1\n      self.fc1 = nn.Linear(768,512)\n      \n      # dense layer 2 (Output layer)\n      self.fc2 = nn.Linear(512,4)\n\n      #softmax activation function\n      self.softmax = nn.LogSoftmax(dim=1)\n\n    #define the forward pass\n    def forward(self, sent_id, mask):\n    # Pass the inputs to the model\n        outputs = self.bert(sent_id, attention_mask=mask)\n        cls_hs = outputs.pooler_output\n        \n        # Pass through fully connected layers\n        x = self.fc1(cls_hs)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.softmax(x)\n    \n        return x","metadata":{"_uuid":"572f1a89-03f0-4377-a7b4-3f3e8b42be5d","_cell_guid":"f7a49a59-88c9-41fd-84fb-9a3977f402c5","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:39.751272Z","iopub.execute_input":"2025-03-26T02:50:39.751524Z","iopub.status.idle":"2025-03-26T02:50:39.772998Z","shell.execute_reply.started":"2025-03-26T02:50:39.751505Z","shell.execute_reply":"2025-03-26T02:50:39.772387Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"print(device)","metadata":{"_uuid":"678ef220-2a2e-4cf2-ba29-5681397e3432","_cell_guid":"463de2d8-2351-468d-b853-010208da3b34","trusted":true,"collapsed":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:39.773633Z","iopub.execute_input":"2025-03-26T02:50:39.773874Z","iopub.status.idle":"2025-03-26T02:50:39.801485Z","shell.execute_reply.started":"2025-03-26T02:50:39.773855Z","shell.execute_reply":"2025-03-26T02:50:39.800701Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# pass the pre-trained BERT to our define architecture\nmodel = BERT_Arch(bert)\n\n# push the model to GPU\nmodel = model.to(device)","metadata":{"_uuid":"ada4338a-cead-42a3-8561-b127e0ab9f86","_cell_guid":"9ab44a81-852e-4c6f-951c-cb3a0f061307","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:39.802221Z","iopub.execute_input":"2025-03-26T02:50:39.802473Z","iopub.status.idle":"2025-03-26T02:50:40.382539Z","shell.execute_reply.started":"2025-03-26T02:50:39.802448Z","shell.execute_reply":"2025-03-26T02:50:40.381586Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"\n# optimizer from hugging face transformers\nfrom transformers import AdamW\n\n# define the optimizer\noptimizer = AdamW(model.parameters(), lr = 1e-3)","metadata":{"_uuid":"7ad419b7-9bd7-4b53-9f4c-c0330a3dbc6c","_cell_guid":"4f27c952-e7fa-41b5-ac5d-3f06ed84847c","trusted":true,"collapsed":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:40.385693Z","iopub.execute_input":"2025-03-26T02:50:40.385904Z","iopub.status.idle":"2025-03-26T02:50:40.419028Z","shell.execute_reply.started":"2025-03-26T02:50:40.385885Z","shell.execute_reply":"2025-03-26T02:50:40.418384Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"\nfrom sklearn.utils.class_weight import compute_class_weight\n\n#compute the class weights\nclass_wts = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n\nprint(class_wts)","metadata":{"_uuid":"2c481e8a-0b84-4b9e-9076-da4c3c5b968e","_cell_guid":"b90934cb-f593-42ad-85a9-dafc6acde987","trusted":true,"collapsed":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:40.419797Z","iopub.execute_input":"2025-03-26T02:50:40.420046Z","iopub.status.idle":"2025-03-26T02:50:40.426465Z","shell.execute_reply.started":"2025-03-26T02:50:40.420014Z","shell.execute_reply":"2025-03-26T02:50:40.425542Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"[1.55555556 1.06329114 0.57931034 1.44827586]\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"\n# convert class weights to tensor\nweights= torch.tensor(class_wts,dtype=torch.float)\nweights = weights.to(device)\n\n# loss function\ncross_entropy  = nn.NLLLoss(weight=weights) \n\n# number of training epochs\nepochs = 10","metadata":{"_uuid":"962890b1-e917-4763-aaaa-3917bd924423","_cell_guid":"4d62881d-dca7-4c04-b4a6-e07c6ad39ead","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:40.427242Z","iopub.execute_input":"2025-03-26T02:50:40.427547Z","iopub.status.idle":"2025-03-26T02:50:40.451741Z","shell.execute_reply.started":"2025-03-26T02:50:40.427525Z","shell.execute_reply":"2025-03-26T02:50:40.450925Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# function to train the model\ndef train():\n  \n  model.train()\n\n  total_loss, total_accuracy = 0, 0\n  \n  # empty list to save model predictions\n  total_preds=[]\n  \n  # iterate over batches\n  for step,batch in enumerate(train_dataloader):\n    \n    # progress update after every 50 batches.\n    if step % 50 == 0 and not step == 0:\n      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n\n    # push the batch to gpu\n    batch = [r.to(device) for r in batch]\n \n    sent_id, mask, labels = batch\n\n    # clear previously calculated gradients \n    model.zero_grad()        \n\n    # get model predictions for the current batch\n    preds = model(sent_id, mask)\n\n    # compute the loss between actual and predicted values\n    loss = cross_entropy(preds, labels)\n\n    # add on to the total loss\n    total_loss = total_loss + loss.item()\n\n    # backward pass to calculate the gradients\n    loss.backward()\n\n    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n    # update parameters\n    optimizer.step()\n\n    # model predictions are stored on GPU. So, push it to CPU\n    preds=preds.detach().cpu().numpy()\n\n    # append the model predictions\n    total_preds.append(preds)\n\n  # compute the training loss of the epoch\n  avg_loss = total_loss / len(train_dataloader)\n  \n  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n  # reshape the predictions in form of (number of samples, no. of classes)\n  total_preds  = np.concatenate(total_preds, axis=0)\n\n  #returns the loss and predictions\n  return avg_loss, total_preds","metadata":{"_uuid":"e93dc8bf-76a0-4a4a-b1b1-21114e4b563d","_cell_guid":"8a0cb4f2-9db5-4055-a1cd-c889414405e8","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:40.452735Z","iopub.execute_input":"2025-03-26T02:50:40.453036Z","iopub.status.idle":"2025-03-26T02:50:40.468984Z","shell.execute_reply.started":"2025-03-26T02:50:40.453006Z","shell.execute_reply":"2025-03-26T02:50:40.468387Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# function for evaluating the model\ndef evaluate():\n  \n  print(\"\\nEvaluating...\")\n  \n  # deactivate dropout layers\n  model.eval()\n\n  total_loss, total_accuracy = 0, 0\n  \n  # empty list to save the model predictions\n  total_preds = []\n\n  # iterate over batches\n  for step,batch in enumerate(val_dataloader):\n    \n    # Progress update every 50 batches.\n    if step % 50 == 0 and not step == 0:\n      \n      # Calculate elapsed time in minutes.\n      elapsed = format_time(time.time() - t0)\n            \n      # Report progress.\n      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n\n    # push the batch to gpu\n    batch = [t.to(device) for t in batch]\n\n    sent_id, mask, labels = batch\n\n    # deactivate autograd\n    with torch.no_grad():\n      \n      # model predictions\n      preds = model(sent_id, mask)\n\n      # compute the validation loss between actual and predicted values\n      loss = cross_entropy(preds,labels)\n\n      total_loss = total_loss + loss.item()\n\n      preds = preds.detach().cpu().numpy()\n\n      total_preds.append(preds)\n\n  # compute the validation loss of the epoch\n  avg_loss = total_loss / len(val_dataloader) \n\n  # reshape the predictions in form of (number of samples, no. of classes)\n  total_preds  = np.concatenate(total_preds, axis=0)\n\n  return avg_loss, total_preds","metadata":{"_uuid":"fa7facde-ed9b-4840-9aff-8f9f5fe1eefe","_cell_guid":"d3903f69-6be0-4fda-8a5d-9a30e25e33e7","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:40.469658Z","iopub.execute_input":"2025-03-26T02:50:40.469896Z","iopub.status.idle":"2025-03-26T02:50:40.496353Z","shell.execute_reply.started":"2025-03-26T02:50:40.469876Z","shell.execute_reply":"2025-03-26T02:50:40.495680Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"\n# set initial loss to infinite\nbest_valid_loss = float('inf')\n\n# empty lists to store training and validation loss of each epoch\ntrain_losses=[]\nvalid_losses=[]\n\n#for each epoch\nfor epoch in range(epochs):\n     \n    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n    \n    #train model\n    train_loss, _ = train()\n    \n    #evaluate model\n    valid_loss, _ = evaluate()\n    \n    #save the best model\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'saved_weights.pt')\n    \n    # append training and validation loss\n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n    \n    print(f'\\nTraining Loss: {train_loss:.3f}')\n    print(f'Validation Loss: {valid_loss:.3f}')","metadata":{"_uuid":"d1d0ba8c-fafb-40b2-a022-a8ed323828b3","_cell_guid":"d997cba5-a241-444d-a038-b6b3bb7a21ac","trusted":true,"collapsed":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:40.497235Z","iopub.execute_input":"2025-03-26T02:50:40.497554Z","iopub.status.idle":"2025-03-26T02:50:56.604411Z","shell.execute_reply.started":"2025-03-26T02:50:40.497524Z","shell.execute_reply":"2025-03-26T02:50:56.603377Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"\n Epoch 1 / 10\n\nEvaluating...\n\nTraining Loss: 1.582\nValidation Loss: 1.349\n\n Epoch 2 / 10\n\nEvaluating...\n\nTraining Loss: 1.437\nValidation Loss: 1.392\n\n Epoch 3 / 10\n\nEvaluating...\n\nTraining Loss: 1.372\nValidation Loss: 1.420\n\n Epoch 4 / 10\n\nEvaluating...\n\nTraining Loss: 1.391\nValidation Loss: 1.344\n\n Epoch 5 / 10\n\nEvaluating...\n\nTraining Loss: 1.418\nValidation Loss: 1.371\n\n Epoch 6 / 10\n\nEvaluating...\n\nTraining Loss: 1.376\nValidation Loss: 1.349\n\n Epoch 7 / 10\n\nEvaluating...\n\nTraining Loss: 1.326\nValidation Loss: 1.327\n\n Epoch 8 / 10\n\nEvaluating...\n\nTraining Loss: 1.310\nValidation Loss: 1.373\n\n Epoch 9 / 10\n\nEvaluating...\n\nTraining Loss: 1.376\nValidation Loss: 1.383\n\n Epoch 10 / 10\n\nEvaluating...\n\nTraining Loss: 1.345\nValidation Loss: 1.368\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"\n\n# get predictions for test data\nwith torch.no_grad():\n  preds = model(test_seq.to(device), test_mask.to(device))\n  preds = preds.detach().cpu().numpy()\n     \n\n# model's performance\npreds = np.argmax(preds, axis = 1)\nprint(classification_report(test_y, preds))","metadata":{"_uuid":"399cba0d-5899-491a-85a3-a3f987960f3e","_cell_guid":"c6e87c55-0574-46fc-afba-247f319cb470","trusted":true,"collapsed":true,"jupyter":{"source_hidden":true,"outputs_hidden":true},"execution":{"iopub.status.busy":"2025-03-26T02:50:56.605532Z","iopub.execute_input":"2025-03-26T02:50:56.605873Z","iopub.status.idle":"2025-03-26T02:50:56.820150Z","shell.execute_reply.started":"2025-03-26T02:50:56.605843Z","shell.execute_reply":"2025-03-26T02:50:56.819254Z"}},"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.26      0.75      0.38        12\n           1       0.00      0.00      0.00        17\n           2       0.50      0.45      0.47        31\n           3       0.44      0.33      0.38        12\n\n    accuracy                           0.38        72\n   macro avg       0.30      0.38      0.31        72\nweighted avg       0.33      0.38      0.33        72\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ChatGPT","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer, BertModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Custom Dataset\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len=512):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, index):\n        text = self.texts.iloc[index]  # Using .iloc for safe indexing\n        label = self.labels.iloc[index]\n        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt')\n\n        return {\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n\n\n# Define Model\nclass BERTClassifier(nn.Module):\n    def __init__(self, num_classes=4):\n        super(BERTClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.fc = nn.Linear(768, num_classes)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        logits = self.fc(outputs.pooler_output)\n        return logits\n\n# Prepare Data\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\ntrain_dataset = TextDataset(X_train, y_train, tokenizer)\ntest_dataset = TextDataset(X_test, y_test, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n\n# Initialize Model and Parameters\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = BERTClassifier(num_classes=4).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=2e-5)\n\n# Training Loop\ndef train_model(epochs=3):\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        for batch in train_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n\n            optimizer.zero_grad()\n            outputs = model(input_ids, attention_mask)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}\")\n\n# Evaluation\ndef evaluate_model():\n    model.eval()\n    all_preds = []\n    all_labels = []\n    with torch.no_grad():\n        for batch in test_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n\n            outputs = model(input_ids, attention_mask)\n            predictions = torch.argmax(outputs, dim=1)\n            all_preds.extend(predictions.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n\n# Run Training and Evaluation\ntrain_model(25)\nevaluate_model()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T13:58:40.699341Z","iopub.execute_input":"2025-03-26T13:58:40.699685Z","iopub.status.idle":"2025-03-26T14:14:52.772456Z","shell.execute_reply.started":"2025-03-26T13:58:40.699656Z","shell.execute_reply":"2025-03-26T14:14:52.771550Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 1.3202871928612392\nEpoch 2, Loss: 1.1803770437836647\nEpoch 3, Loss: 1.0171967794497807\nEpoch 4, Loss: 0.776843490699927\nEpoch 5, Loss: 0.5988089914123217\nEpoch 6, Loss: 0.5111452552179495\nEpoch 7, Loss: 0.3892643637955189\nEpoch 8, Loss: 0.2932945806533098\nEpoch 9, Loss: 0.22148984453330436\nEpoch 10, Loss: 0.14605277481799325\nEpoch 11, Loss: 0.11839799738178651\nEpoch 12, Loss: 0.08262243463347356\nEpoch 13, Loss: 0.06385140710820754\nEpoch 14, Loss: 0.051120028753454484\nEpoch 15, Loss: 0.05385881934004525\nEpoch 16, Loss: 0.04409429019627472\nEpoch 17, Loss: 0.032616524530264236\nEpoch 18, Loss: 0.02854304830543697\nEpoch 19, Loss: 0.03043265885207802\nEpoch 20, Loss: 0.025352997045653563\nEpoch 21, Loss: 0.025301651699313272\nEpoch 22, Loss: 0.02350107211774836\nEpoch 23, Loss: 0.018318536012278248\nEpoch 24, Loss: 0.02259548307241251\nEpoch 25, Loss: 0.02261593899068733\nTest Accuracy: 46.88%\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import os\n\ndef save_model(model, path='/kaggle/working/model_state_dict.pt'):\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n    torch.save(model.state_dict(), path)\n    print(f\"Model state dict saved to {path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:14:52.773883Z","iopub.execute_input":"2025-03-26T14:14:52.774267Z","iopub.status.idle":"2025-03-26T14:14:52.778361Z","shell.execute_reply.started":"2025-03-26T14:14:52.774232Z","shell.execute_reply":"2025-03-26T14:14:52.777437Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"save_model(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:14:52.779793Z","iopub.execute_input":"2025-03-26T14:14:52.779987Z","iopub.status.idle":"2025-03-26T14:14:53.884112Z","shell.execute_reply.started":"2025-03-26T14:14:52.779971Z","shell.execute_reply":"2025-03-26T14:14:53.883037Z"}},"outputs":[{"name":"stdout","text":"Model state dict saved to /kaggle/working/model_state_dict.pt\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"def save_tokenizer(tokenizer, path='/kaggle/working/tokenizer'):\n    os.makedirs(path, exist_ok=True)\n    tokenizer.save_pretrained(path)\n    print(f\"Tokenizer saved to {path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:14:53.885821Z","iopub.execute_input":"2025-03-26T14:14:53.886133Z","iopub.status.idle":"2025-03-26T14:14:53.890346Z","shell.execute_reply.started":"2025-03-26T14:14:53.886111Z","shell.execute_reply":"2025-03-26T14:14:53.889483Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"save_tokenizer(tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:14:53.891264Z","iopub.execute_input":"2025-03-26T14:14:53.891559Z","iopub.status.idle":"2025-03-26T14:14:53.930982Z","shell.execute_reply.started":"2025-03-26T14:14:53.891531Z","shell.execute_reply":"2025-03-26T14:14:53.930271Z"}},"outputs":[{"name":"stdout","text":"Tokenizer saved to /kaggle/working/tokenizer\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import numpy as np\n\ndef generate_embeddings(texts, tokenizer, model, batch_size=16, max_len=512):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    model.eval()\n\n    embeddings = []\n\n    with torch.no_grad():\n        for i in range(0, len(texts), batch_size):\n            batch_texts = texts[i:i+batch_size]\n\n            inputs = tokenizer(batch_texts, padding=True, truncation=True, max_length=max_len, return_tensors='pt')\n            input_ids = inputs['input_ids'].to(device)\n            attention_mask = inputs['attention_mask'].to(device)\n            \n            outputs = model.bert(input_ids=input_ids, attention_mask=attention_mask)\n\n            # If pooler_output is available, use it; otherwise, use mean of last hidden state\n            if hasattr(outputs, 'pooler_output'):\n                batch_embeddings = outputs.pooler_output.cpu().numpy()\n            else:\n                batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n\n            embeddings.extend(batch_embeddings)\n    \n    return np.array(embeddings)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:14:53.931878Z","iopub.execute_input":"2025-03-26T14:14:53.932198Z","iopub.status.idle":"2025-03-26T14:14:53.938597Z","shell.execute_reply.started":"2025-03-26T14:14:53.932169Z","shell.execute_reply":"2025-03-26T14:14:53.937503Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n\n# Assuming X_train, X_test, y_train, y_test are available\nX_train_embeddings = generate_embeddings(X_train.tolist(), tokenizer, model)\nX_test_embeddings = generate_embeddings(X_test.tolist(), tokenizer, model)\n\n# Train the SVM\nsvm_classifier = SVC(kernel='linear', class_weight = 'balanced')\nsvm_classifier.fit(X_train_embeddings, y_train)\n\n# Predict and Evaluate\ny_pred = svm_classifier.predict(X_test_embeddings)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(f\"Test Accuracy with SVM: {accuracy * 100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:14:53.939316Z","iopub.execute_input":"2025-03-26T14:14:53.939559Z","iopub.status.idle":"2025-03-26T14:14:56.645812Z","shell.execute_reply.started":"2025-03-26T14:14:53.939540Z","shell.execute_reply":"2025-03-26T14:14:56.644821Z"}},"outputs":[{"name":"stdout","text":"Test Accuracy with SVM: 43.75%\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve\n\n# Predict using SVM\ny_pred = svm_classifier.predict(X_test_embeddings)\n\n# Confusion Matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(8,6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()\n\n# Classification Report\nprint(\"Classification Report:\\n\")\nprint(classification_report(y_test, y_pred))\n\n# AUC-ROC Score\nif len(set(y_test)) <= 2:\n    auc_score = roc_auc_score(y_test, svm_classifier.decision_function(X_test_embeddings))\n    print(f\"AUC-ROC Score: {auc_score:.2f}\")\n\n    # Plot ROC Curve\n    fpr, tpr, _ = roc_curve(y_test, svm_classifier.decision_function(X_test_embeddings))\n    plt.figure(figsize=(8,6))\n    plt.plot(fpr, tpr, color='blue', label=f\"AUC = {auc_score:.2f}\")\n    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.title(\"ROC Curve\")\n    plt.legend(loc=\"lower right\")\n    plt.show()\nelse:\n    print(\"AUC-ROC is not applicable for multi-class classification.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:14:56.647690Z","iopub.execute_input":"2025-03-26T14:14:56.647951Z","iopub.status.idle":"2025-03-26T14:14:56.875687Z","shell.execute_reply.started":"2025-03-26T14:14:56.647929Z","shell.execute_reply":"2025-03-26T14:14:56.875051Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 2 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"Classification Report:\n\n              precision    recall  f1-score   support\n\n           0       0.50      0.06      0.11        16\n           1       0.28      0.35      0.31        20\n           2       0.50      0.67      0.57        43\n           3       0.45      0.29      0.36        17\n\n    accuracy                           0.44        96\n   macro avg       0.43      0.35      0.34        96\nweighted avg       0.45      0.44      0.40        96\n\nAUC-ROC is not applicable for multi-class classification.\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"print('hi')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:14:56.876469Z","iopub.execute_input":"2025-03-26T14:14:56.876713Z","iopub.status.idle":"2025-03-26T14:14:56.880659Z","shell.execute_reply.started":"2025-03-26T14:14:56.876688Z","shell.execute_reply":"2025-03-26T14:14:56.879957Z"}},"outputs":[{"name":"stdout","text":"hi\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"misclassified_indices = np.where(y_test != y_pred)[0]\nfor idx in misclassified_indices[:10]:  # Inspect first 10 misclassified samples\n    print(f\"Text: {X_test.iloc[idx]}\")\n    print(f\"True Label: {y_test.iloc[idx]}, Predicted Label: {y_pred[idx]}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:14:56.881523Z","iopub.execute_input":"2025-03-26T14:14:56.881857Z","iopub.status.idle":"2025-03-26T14:14:56.899878Z","shell.execute_reply.started":"2025-03-26T14:14:56.881835Z","shell.execute_reply":"2025-03-26T14:14:56.899236Z"}},"outputs":[{"name":"stdout","text":"Text: From symbol of peace to megaphone for war, #Putin rewrites history of #Luzniki stadium\nTrue Label: 3, Predicted Label: 2\n\nText: It's #11thApril! So what?\n\n* Day of war #47 in #Ukraine.\n* Austrian PM #Nehammer in Moscow from #Putin.\n* #Draghi to #Algiers for #gas deal.\n* National #sea day\n\nApril 11, 1961 Bob #Dylan's first performance.\nTrue Label: 2, Predicted Label: 3\n\nText: LA #GUERRA GIUSTA NON ESISTE!\nINVOCARE #PACE INVIANDO #ARMI NON È PER L'EROE CHE COMBATTE, MA PER IL POPOLO CHE VIENE MACELLATO!\nSIAMO UN PAESE DI COGLIONI INVASATI!\nSveglia!!!\n#StopTheWar\n#DraghiVatteneSubito\n#NonInMioNome\nTrue Label: 1, Predicted Label: 2\n\nText: This #April25 I will go to the parade.\nI had the honor of knowing that generation of #Liberals who took up arms to defend the Fatherland, and I know they would have no problem today recognizing the #partisans of that time in the Ukrainians and Ukrainians of today.\n \nTrue Label: 2, Predicted Label: 3\n\nText: But how nice,with a war going on,with bombs falling,with civilians being killed we also have time to have a laugh. All scene!!! Fuck you shit actors #putin #actors #bugs #UkraineRussiaWar #Ukraine #aveterottoilcaxxo\nTrue Label: 1, Predicted Label: 2\n\nText: How do voters of German parties answer the question, whether weapons should be supplied to Ukraine:\nInteresting - liberals and conservatives are more skeptical than Greens and Social Democrats.\nObvious: AfD is against it.\n\nThe progressive word is: freedom and life for Ukraine.\nTrue Label: 2, Predicted Label: 1\n\nText: Moscow does not rule out Putin-Zelensky meeting, but after understanding #Ukraine war #UkraineRussianWar #russia #putin #bucha #onu #Zelensky\nTrue Label: 3, Predicted Label: 2\n\nText: YOU POINT THE FINGER AT THOSE IN WAR WHO ACCIDENTALLY KILL A CHILD\n\nI POINT THE FINGER AT EUROPE AND YOU PARENTS WHO MAKE THEM RISK THEIR LIVES WITHOUT VALID MEDICAL REASONS\nTrue Label: 0, Predicted Label: 1\n\nText: \"The little girl with the candy\"-and the rifle!\nMore and more propaganda and less and less journalism.\nInstead of charades Ukrainians need free voices. A heartfelt appeal on my site:\n \n#RussiansAreOurBrothers #UkrainiansAreOurBrothers.\nTrue Label: 1, Predicted Label: 3\n\nText: Far away I will go; over sea and land, to say no to war | to those I will see. If there is blood to spill Spill only your own; I no longer follow you. And if you find me, with me I carry no weapons: courage, up, gendarmes, fire upon me\n\n🖊️Boris Vian\n🎨from the web\n\n#WarPeace\nTrue Label: 3, Predicted Label: 2\n\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"# Early Stopping","metadata":{}},{"cell_type":"code","source":"class BERTClassifierWithAttention(nn.Module):\n    def __init__(self, num_classes=4):\n        super(BERTClassifierWithAttention, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)\n        self.fc = nn.Linear(768, num_classes)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        logits = self.fc(outputs.pooler_output)\n        \n        # Extract attention weights (already normalized)\n        attentions = outputs.attentions  # Tuple of attention weights for each layer\n        \n        # Compute attention scores (unnormalized)\n        attention_scores = []\n        for layer in self.bert.encoder.layer:\n            attn = layer.attention.self\n            q = attn.query(input_ids)\n            k = attn.key(input_ids)\n            scores = torch.matmul(q, k.transpose(-2, -1)) / (q.size(-1) ** 0.5)\n            attention_scores.append(scores.detach())  # Store unnormalized scores\n        \n        return logits, attentions, attention_scores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:35:21.888874Z","iopub.execute_input":"2025-03-26T14:35:21.889189Z","iopub.status.idle":"2025-03-26T14:35:21.895265Z","shell.execute_reply.started":"2025-03-26T14:35:21.889166Z","shell.execute_reply":"2025-03-26T14:35:21.894446Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef plot_attention_scores(scores, title=\"Attention Scores\"):\n    # Normalize scores for visualization\n    scores = scores[0].detach().numpy()  # Shape: (num_heads, seq_len, seq_len)\n    \n    plt.figure(figsize=(10, 8))\n    sns.heatmap(scores[0], annot=True, fmt=\".2f\", cmap=\"viridis\")  # First head\n    plt.title(title)\n    plt.xlabel(\"Keys\")\n    plt.ylabel(\"Queries\")\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:35:31.939554Z","iopub.execute_input":"2025-03-26T14:35:31.939909Z","iopub.status.idle":"2025-03-26T14:35:31.944892Z","shell.execute_reply.started":"2025-03-26T14:35:31.939884Z","shell.execute_reply":"2025-03-26T14:35:31.943965Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Attention","metadata":{}},{"cell_type":"markdown","source":"## Initialize bert and set output attentions to True","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import BertModel, BertTokenizer\n\nclass BERTClassifierWithAttention(nn.Module):\n    def __init__(self, num_classes=4):\n        super(BERTClassifierWithAttention, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)\n        self.fc = nn.Linear(768, num_classes)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        logits = self.fc(outputs.pooler_output)\n        \n        # Extract attention weights (normalized scores)\n        attentions = outputs.attentions  # Tuple of attention weights for each layer\n        \n        # Extract Q, K, V matrices manually\n        qkv_values = []\n        for layer in self.bert.encoder.layer:\n            attn = layer.attention.self\n            q = attn.query(outputs.last_hidden_state)  # Query projection\n            k = attn.key(outputs.last_hidden_state)    # Key projection\n            v = attn.value(outputs.last_hidden_state)  # Value projection\n            qkv_values.append((q.detach(), k.detach(), v.detach()))\n        \n        return logits, attentions, qkv_values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T16:36:05.253534Z","iopub.execute_input":"2025-03-26T16:36:05.253816Z","iopub.status.idle":"2025-03-26T16:36:05.266590Z","shell.execute_reply.started":"2025-03-26T16:36:05.253787Z","shell.execute_reply":"2025-03-26T16:36:05.265815Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Load the dataset and set up the finetuning environment","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AdamW\n\n# Custom Dataset\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len=512):\n        self.texts = texts.tolist() if hasattr(texts, 'tolist') else texts\n        self.labels = labels.tolist() if hasattr(labels, 'tolist') else labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, index):\n        text = self.texts[index]\n        label = self.labels[index]\n        encoding = self.tokenizer(\n            text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_len,\n            return_tensors='pt'\n        )\n        return {\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n\n# Prepare Data\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\ntrain_dataset = TextDataset(X_train, y_train, tokenizer)\nval_dataset = TextDataset(X_val, y_val, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n\n# Initialize Model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = BERTClassifierWithAttention(num_classes=4).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = AdamW(model.parameters(), lr=2e-5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T16:36:05.267462Z","iopub.execute_input":"2025-03-26T16:36:05.267700Z","iopub.status.idle":"2025-03-26T16:36:08.999022Z","shell.execute_reply.started":"2025-03-26T16:36:05.267681Z","shell.execute_reply":"2025-03-26T16:36:08.997958Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2da0bb0d9a584f6c9dd8119108e0f603"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d01245ad6ee48dbb72a757ac8da9603"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cba4f13ab6c9475caf9f8cb5e7e34d44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a12a8b32e3646959919a4eb394a4169"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5490e04a84a741c8b0b92114cda117ef"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Train it\n> Run a few epochs on our dataset","metadata":{}},{"cell_type":"code","source":"# Training Loop\ndef train_model(epochs=3):\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        for batch in train_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n\n            optimizer.zero_grad()\n            logits, _, _ = model(input_ids, attention_mask)  # Ignore attentions and QKV during training\n            loss = criterion(logits, labels)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}\")\n\n# Run Training\ntrain_model(epochs=10)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Generate Embeddings from finetuned model","metadata":{}},{"cell_type":"code","source":"def generate_embeddings(texts, tokenizer, model, batch_size=16, max_len=512):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    model.eval()\n\n    embeddings = []\n\n    with torch.no_grad():\n        for i in range(0, len(texts), batch_size):\n            batch_texts = texts[i:i+batch_size]\n            inputs = tokenizer(batch_texts, padding=True, truncation=True, max_length=max_len, return_tensors='pt')\n            input_ids = inputs['input_ids'].to(device)\n            attention_mask = inputs['attention_mask'].to(device)\n            \n            outputs = model.bert(input_ids=input_ids, attention_mask=attention_mask)\n            batch_embeddings = outputs.pooler_output.cpu().numpy()  # Use pooler_output\n            embeddings.extend(batch_embeddings)\n    \n    return embeddings\n\n# Generate embeddings for train and test sets\nX_train_embeddings = generate_embeddings(X_train.tolist(), tokenizer, model)\nX_val_embeddings = generate_embeddings(X_val.tolist(), tokenizer, model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T16:50:07.193608Z","iopub.execute_input":"2025-03-26T16:50:07.193991Z","iopub.status.idle":"2025-03-26T16:50:10.100765Z","shell.execute_reply.started":"2025-03-26T16:50:07.193953Z","shell.execute_reply":"2025-03-26T16:50:10.100071Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Classify embeddings using SVM","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n\n# Train the SVM\nsvm_classifier = SVC(kernel='linear')\nsvm_classifier.fit(X_train_embeddings, y_train)\n\n# Predict and Evaluate\ny_pred = svm_classifier.predict(X_val_embeddings)\naccuracy = accuracy_score(y_val, y_pred)\n\nprint(f\"Validation Accuracy with SVM: {accuracy * 100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T16:51:29.405576Z","iopub.execute_input":"2025-03-26T16:51:29.405867Z","iopub.status.idle":"2025-03-26T16:51:29.423474Z","shell.execute_reply.started":"2025-03-26T16:51:29.405847Z","shell.execute_reply":"2025-03-26T16:51:29.422500Z"}},"outputs":[{"name":"stdout","text":"Validation Accuracy with SVM: 51.04%\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## Analyze misclassified","metadata":{}},{"cell_type":"code","source":"misclassified_indices = np.where(y_val != y_pred)[0]\nfor idx in misclassified_indices[:10]:  # Inspect first 10 misclassified samples\n    print(f\"Text: {X_val.iloc[idx]}\")\n    print(f\"True Label: {y_val.iloc[idx]}, Predicted Label: {y_pred[idx]}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T16:51:22.710143Z","iopub.execute_input":"2025-03-26T16:51:22.710504Z","iopub.status.idle":"2025-03-26T16:51:22.719728Z","shell.execute_reply.started":"2025-03-26T16:51:22.710478Z","shell.execute_reply":"2025-03-26T16:51:22.718832Z"}},"outputs":[{"name":"stdout","text":"Text: From symbol of peace to megaphone for war, #Putin rewrites history of #Luzniki stadium\nTrue Label: 3, Predicted Label: 2\n\nText: Countries supporting #Ukraine do not have the means to track the weapons they send to the Kiev army. And some have already gone to pro-Russian troops in the #Donbass, as some photos show 👇\n\nby @FuturaDaprile\n\n \nTrue Label: 1, Predicted Label: 2\n\nText: It's #11thApril! So what?\n\n* Day of war #47 in #Ukraine.\n* Austrian PM #Nehammer in Moscow from #Putin.\n* #Draghi to #Algiers for #gas deal.\n* National #sea day\n\nApril 11, 1961 Bob #Dylan's first performance.\nTrue Label: 2, Predicted Label: 3\n\nText: Russia and Ukraine between Culture and History\nFriday, March 18, 2022, 4 p.m. - Room 22, Pescara Campus and Teams Platform\nNotice on the Athenaeum website:\nTrue Label: 3, Predicted Label: 2\n\nText: LA #GUERRA GIUSTA NON ESISTE!\nINVOCARE #PACE INVIANDO #ARMI NON È PER L'EROE CHE COMBATTE, MA PER IL POPOLO CHE VIENE MACELLATO!\nSIAMO UN PAESE DI COGLIONI INVASATI!\nSveglia!!!\n#StopTheWar\n#DraghiVatteneSubito\n#NonInMioNome\nTrue Label: 1, Predicted Label: 2\n\nText: This #April25 I will go to the parade.\nI had the honor of knowing that generation of #Liberals who took up arms to defend the Fatherland, and I know they would have no problem today recognizing the #partisans of that time in the Ukrainians and Ukrainians of today.\n \nTrue Label: 2, Predicted Label: 3\n\nText: But how nice,with a war going on,with bombs falling,with civilians being killed we also have time to have a laugh. All scene!!! Fuck you shit actors #putin #actors #bugs #UkraineRussiaWar #Ukraine #aveterottoilcaxxo\nTrue Label: 1, Predicted Label: 2\n\nText: How do voters of German parties answer the question, whether weapons should be supplied to Ukraine:\nInteresting - liberals and conservatives are more skeptical than Greens and Social Democrats.\nObvious: AfD is against it.\n\nThe progressive word is: freedom and life for Ukraine.\nTrue Label: 2, Predicted Label: 1\n\nText: Moscow does not rule out Putin-Zelensky meeting, but after understanding #Ukraine war #UkraineRussianWar #russia #putin #bucha #onu #Zelensky\nTrue Label: 3, Predicted Label: 2\n\nText: \"The little girl with the candy\"-and the rifle!\nMore and more propaganda and less and less journalism.\nInstead of charades Ukrainians need free voices. A heartfelt appeal on my site:\n \n#RussiansAreOurBrothers #UkrainiansAreOurBrothers.\nTrue Label: 1, Predicted Label: 2\n\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Extract attentions and Q, K and V values","metadata":{}},{"cell_type":"code","source":"def extract_attention_and_qkv(model, tokenizer, text):\n    encoding = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n    input_ids = encoding['input_ids'].to(device)\n    attention_mask = encoding['attention_mask'].to(device)\n\n    with torch.no_grad():\n        _, attentions, qkv_values = model(input_ids, attention_mask)\n    \n    return attentions, qkv_values\n\n# Example usage\ntext = \"\"\"\n#RussiansAreOurBrothers #UkrainiansAreOurBrothers.\n\"\"\"\nattentions, qkv_values = extract_attention_and_qkv(model, tokenizer, text)\n\n# Inspect Q, K, V values for the first layer\nq, k, v = qkv_values[0]\nprint(\"Query Matrix:\", q)\nprint(\"Key Matrix:\", k)\nprint(\"Value Matrix:\", v)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T16:53:46.607330Z","iopub.execute_input":"2025-03-26T16:53:46.607650Z","iopub.status.idle":"2025-03-26T16:53:46.637835Z","shell.execute_reply.started":"2025-03-26T16:53:46.607625Z","shell.execute_reply":"2025-03-26T16:53:46.637099Z"}},"outputs":[{"name":"stdout","text":"Query Matrix: tensor([[[ 8.3724e-01, -7.7544e-01,  4.0249e-01,  ..., -4.6010e-01,\n           4.1874e-01,  6.6709e-01],\n         [ 8.2787e-01, -1.4159e+00, -2.5362e-01,  ...,  5.7746e-01,\n           1.0030e+00, -2.8594e-02],\n         [ 3.6384e-02,  7.5411e-01, -5.6148e-02,  ..., -2.5419e+00,\n           1.1673e+00, -1.0077e+00],\n         ...,\n         [-6.0397e-04,  3.4162e-01,  4.0235e-01,  ..., -8.9235e-01,\n           7.5787e-01, -5.4485e-01],\n         [ 1.9296e-01, -1.6582e+00, -1.4569e-01,  ..., -1.7868e+00,\n           8.6699e-01, -1.6676e-01],\n         [ 3.0208e-01, -9.1320e-01, -1.8782e-01,  ..., -8.2039e-01,\n           6.2926e-01,  1.4462e+00]]], device='cuda:0')\nKey Matrix: tensor([[[-0.2393, -0.1241, -0.7415,  ..., -0.5536,  0.6236,  0.6638],\n         [ 0.2241, -0.6511, -0.2357,  ..., -1.1656, -1.7942,  0.1933],\n         [ 0.5809, -0.2196,  0.1073,  ..., -0.2248, -0.1608, -0.2031],\n         ...,\n         [-0.6795, -0.7143, -0.3750,  ...,  0.0167, -0.2185, -0.4044],\n         [ 0.8291,  0.1017, -0.7217,  ...,  1.0924,  0.2582, -0.2602],\n         [ 0.7599,  0.7550, -0.7120,  ...,  0.5184, -0.3768,  0.7007]]],\n       device='cuda:0')\nValue Matrix: tensor([[[ 0.8569, -0.5185,  0.1406,  ...,  0.1022, -0.3519,  0.3250],\n         [-0.3569, -0.3511,  0.9311,  ...,  0.2329, -0.5208, -0.3081],\n         [-0.3929, -0.4367, -0.1237,  ...,  0.3728, -0.1443, -0.3978],\n         ...,\n         [-0.2491, -0.2026,  0.1245,  ...,  0.2377, -1.1331, -0.6687],\n         [-0.3620, -0.3440,  0.0320,  ...,  0.3859,  0.5183, -0.7004],\n         [-0.1002, -0.0502, -0.0848,  ..., -0.8253,  0.1841, -0.5860]]],\n       device='cuda:0')\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"print(attentions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T16:53:49.244742Z","iopub.execute_input":"2025-03-26T16:53:49.245030Z","iopub.status.idle":"2025-03-26T16:53:49.339884Z","shell.execute_reply.started":"2025-03-26T16:53:49.245009Z","shell.execute_reply":"2025-03-26T16:53:49.338858Z"}},"outputs":[{"name":"stdout","text":"(tensor([[[[0.0000e+00, 2.9059e-02, 1.3968e-02,  ..., 1.0241e-01,\n           1.2721e-01, 2.2019e-01],\n          [5.5664e-02, 4.9264e-02, 1.1167e-01,  ..., 0.0000e+00,\n           4.5037e-02, 4.3709e-02],\n          [3.2370e-02, 0.0000e+00, 1.0589e-01,  ..., 5.2223e-02,\n           4.2695e-02, 2.4076e-02],\n          ...,\n          [0.0000e+00, 5.9775e-02, 1.0283e-01,  ..., 2.1018e-02,\n           2.7905e-02, 5.9327e-02],\n          [4.4971e-02, 6.0651e-02, 3.4937e-02,  ..., 8.4852e-02,\n           1.2033e-01, 7.8589e-02],\n          [4.7866e-02, 2.6169e-02, 2.1659e-02,  ..., 1.2267e-01,\n           1.5580e-01, 1.1815e-01]],\n\n         [[7.0704e-01, 7.1352e-03, 2.3396e-03,  ..., 1.2198e-01,\n           5.1849e-02, 5.6926e-03],\n          [2.3098e-03, 3.7452e-02, 1.7760e-01,  ..., 7.5352e-03,\n           5.6336e-03, 1.1091e-02],\n          [1.8390e-02, 3.0137e-02, 1.1514e-01,  ..., 8.7339e-03,\n           0.0000e+00, 3.0796e-02],\n          ...,\n          [2.0492e-01, 4.8632e-02, 0.0000e+00,  ..., 1.6656e-02,\n           9.9902e-02, 5.5538e-02],\n          [2.5526e-03, 1.1826e-01, 2.5128e-02,  ..., 2.7382e-02,\n           4.7463e-01, 0.0000e+00],\n          [0.0000e+00, 7.8776e-02, 1.2211e-02,  ..., 6.6653e-02,\n           4.8989e-01, 2.2305e-02]],\n\n         [[8.6071e-01, 1.4687e-02, 0.0000e+00,  ..., 1.5987e-02,\n           2.6079e-02, 0.0000e+00],\n          [3.0059e-01, 1.1949e-02, 7.9440e-03,  ..., 2.3694e-02,\n           1.7025e-02, 6.7489e-03],\n          [8.8316e-01, 3.7003e-02, 0.0000e+00,  ..., 7.5926e-03,\n           1.2576e-02, 4.5241e-02],\n          ...,\n          [9.5247e-04, 1.4558e-04, 1.6426e-04,  ..., 5.0335e-03,\n           1.5594e-02, 1.3224e-02],\n          [8.4815e-02, 8.1611e-02, 1.2220e-03,  ..., 6.0807e-01,\n           0.0000e+00, 6.4863e-02],\n          [1.4961e-01, 9.1298e-04, 8.9942e-04,  ..., 2.7821e-02,\n           8.0435e-01, 8.6365e-02]],\n\n         ...,\n\n         [[3.8463e-02, 4.1729e-03, 4.6356e-03,  ..., 2.7197e-01,\n           0.0000e+00, 4.8423e-03],\n          [1.5388e-01, 1.7001e-01, 1.5617e-02,  ..., 7.5294e-03,\n           5.5846e-02, 2.1681e-01],\n          [2.7347e-01, 3.1667e-02, 1.1229e-01,  ..., 7.9179e-03,\n           3.4116e-02, 7.9336e-02],\n          ...,\n          [1.1912e-01, 1.4506e-02, 6.2922e-02,  ..., 3.5153e-02,\n           2.8543e-02, 4.1382e-02],\n          [0.0000e+00, 1.3299e-02, 4.8770e-03,  ..., 3.8209e-02,\n           2.7354e-02, 1.1214e-01],\n          [3.8975e-01, 1.7177e-02, 1.5212e-02,  ..., 7.5721e-02,\n           4.4188e-02, 1.8704e-02]],\n\n         [[8.9017e-01, 8.6399e-03, 6.9081e-03,  ..., 0.0000e+00,\n           1.2638e-02, 1.9046e-02],\n          [1.2018e-01, 3.4512e-02, 6.2770e-01,  ..., 3.5747e-03,\n           2.9138e-02, 1.8397e-02],\n          [8.1294e-01, 9.2020e-03, 5.1022e-03,  ..., 4.5297e-04,\n           0.0000e+00, 1.9554e-02],\n          ...,\n          [1.7503e-03, 1.1446e-03, 2.5341e-03,  ..., 5.0909e-02,\n           6.6263e-01, 1.6146e-01],\n          [1.8155e-03, 4.1708e-04, 7.4393e-04,  ..., 2.5517e-02,\n           3.0619e-02, 1.0278e+00],\n          [1.5273e-01, 4.5406e-04, 4.3704e-03,  ..., 1.4937e-01,\n           3.0212e-01, 4.1481e-01]],\n\n         [[1.0618e+00, 0.0000e+00, 5.9243e-04,  ..., 1.1976e-03,\n           1.0136e-07, 1.9718e-02],\n          [9.5270e-02, 1.9247e-02, 1.2097e-01,  ..., 1.0252e-02,\n           1.2056e-02, 4.3619e-02],\n          [6.8505e-01, 5.8040e-03, 2.5853e-02,  ..., 0.0000e+00,\n           2.0924e-03, 5.8542e-02],\n          ...,\n          [5.0034e-02, 2.9562e-03, 2.5472e-02,  ..., 5.1357e-03,\n           8.5233e-03, 1.3568e-01],\n          [4.8935e-01, 1.1339e-02, 8.5686e-03,  ..., 4.8226e-02,\n           1.4846e-03, 2.4442e-01],\n          [0.0000e+00, 9.6469e-04, 0.0000e+00,  ..., 1.6579e-02,\n           6.5722e-04, 1.6044e-01]]]], device='cuda:0'), tensor([[[[0.0000e+00, 2.6151e-02, 2.7282e-02,  ..., 7.5757e-02,\n           2.4884e-01, 0.0000e+00],\n          [8.5345e-02, 7.7594e-03, 3.6421e-02,  ..., 2.7054e-02,\n           1.7423e-01, 2.2818e-01],\n          [2.3215e-01, 2.4440e-02, 2.0888e-02,  ..., 6.2885e-02,\n           1.1627e-01, 6.8834e-02],\n          ...,\n          [2.1531e-01, 4.2842e-02, 7.0834e-02,  ..., 2.1426e-02,\n           1.5606e-01, 2.1740e-01],\n          [3.5747e-01, 6.6413e-02, 2.4922e-02,  ..., 3.7622e-02,\n           1.1877e-01, 5.2424e-02],\n          [3.4663e-01, 2.0791e-02, 4.3331e-02,  ..., 5.6702e-02,\n           1.2478e-01, 3.0044e-02]],\n\n         [[0.0000e+00, 2.1575e-02, 4.8084e-02,  ..., 2.6328e-02,\n           2.6595e-02, 3.6617e-02],\n          [6.7044e-01, 2.5827e-02, 5.9787e-02,  ..., 3.8611e-04,\n           0.0000e+00, 9.0689e-02],\n          [3.4705e-01, 5.4899e-03, 2.3655e-02,  ..., 2.6478e-03,\n           0.0000e+00, 0.0000e+00],\n          ...,\n          [3.4367e-01, 2.3186e-03, 8.2299e-04,  ..., 0.0000e+00,\n           4.7953e-01, 2.5630e-01],\n          [2.8556e-01, 0.0000e+00, 3.6979e-04,  ..., 1.5215e-03,\n           0.0000e+00, 8.0483e-01],\n          [4.6517e-01, 2.2077e-04, 3.4969e-03,  ..., 6.2879e-03,\n           1.4708e-02, 5.9784e-01]],\n\n         [[7.1596e-01, 2.3280e-02, 1.2375e-02,  ..., 2.5236e-02,\n           3.9649e-02, 4.4962e-02],\n          [2.5403e-01, 0.0000e+00, 8.7437e-03,  ..., 4.3062e-02,\n           1.3711e-01, 8.1997e-02],\n          [0.0000e+00, 3.4767e-02, 7.0482e-02,  ..., 3.5133e-02,\n           4.5310e-02, 1.0000e-02],\n          ...,\n          [4.6835e-01, 1.7871e-02, 3.6472e-02,  ..., 1.6792e-02,\n           1.3982e-01, 2.1493e-02],\n          [3.7438e-01, 0.0000e+00, 3.1862e-02,  ..., 2.3971e-02,\n           5.4544e-02, 0.0000e+00],\n          [6.5539e-01, 2.8240e-02, 1.4961e-02,  ..., 1.7970e-02,\n           3.6724e-02, 6.1034e-02]],\n\n         ...,\n\n         [[1.4695e-01, 0.0000e+00, 5.2536e-02,  ..., 5.3559e-02,\n           1.2204e-01, 1.3888e-01],\n          [2.0903e-01, 1.1401e-02, 2.4783e-02,  ..., 0.0000e+00,\n           1.3725e-01, 1.5762e-01],\n          [8.5689e-02, 3.7544e-02, 1.2260e-03,  ..., 5.4806e-02,\n           0.0000e+00, 0.0000e+00],\n          ...,\n          [4.6548e-01, 6.7697e-02, 3.6305e-02,  ..., 1.1219e-02,\n           6.9782e-02, 1.1918e-01],\n          [0.0000e+00, 1.0852e-01, 4.3504e-02,  ..., 2.5260e-02,\n           8.0105e-02, 1.2681e-01],\n          [8.1480e-02, 8.4098e-02, 4.0857e-02,  ..., 6.0509e-02,\n           9.6782e-02, 1.1257e-01]],\n\n         [[6.1471e-01, 4.2366e-02, 2.2769e-02,  ..., 1.3817e-02,\n           7.0878e-02, 1.0179e-01],\n          [8.5275e-01, 2.5657e-02, 2.0654e-02,  ..., 0.0000e+00,\n           1.5080e-02, 3.6832e-02],\n          [8.0149e-01, 3.8286e-02, 1.0728e-02,  ..., 1.7441e-02,\n           9.0860e-03, 2.1939e-02],\n          ...,\n          [5.4072e-02, 2.4035e-02, 7.2509e-03,  ..., 1.4634e-02,\n           1.2427e-01, 2.5836e-01],\n          [1.3683e-01, 2.8889e-02, 1.7394e-02,  ..., 5.8680e-02,\n           1.1962e-01, 1.9793e-01],\n          [7.5388e-01, 0.0000e+00, 1.0166e-03,  ..., 0.0000e+00,\n           6.8057e-02, 1.3958e-01]],\n\n         [[2.4107e-01, 3.3857e-02, 2.2195e-02,  ..., 5.9069e-02,\n           5.5987e-02, 2.1858e-01],\n          [6.8793e-02, 1.3609e-01, 4.6469e-04,  ..., 6.7910e-04,\n           4.3467e-02, 5.7879e-02],\n          [8.6348e-02, 3.2726e-03, 6.1231e-01,  ..., 7.8174e-03,\n           1.7655e-02, 5.9128e-02],\n          ...,\n          [4.4515e-02, 1.2644e-03, 3.6615e-03,  ..., 1.7019e-01,\n           6.1961e-03, 1.8023e-02],\n          [0.0000e+00, 8.4881e-02, 1.1725e-02,  ..., 3.0376e-02,\n           2.7372e-02, 1.4714e-01],\n          [1.9618e-01, 5.4193e-02, 4.6162e-02,  ..., 2.4650e-02,\n           4.2597e-02, 5.0026e-02]]]], device='cuda:0'), tensor([[[[1.0568e+00, 1.6658e-03, 2.2388e-04,  ..., 2.1438e-03,\n           1.0912e-03, 1.3623e-02],\n          [7.3233e-07, 2.1893e-05, 1.1111e+00,  ..., 6.7885e-10,\n           2.6668e-07, 3.1309e-06],\n          [7.6097e-05, 0.0000e+00, 1.3984e-03,  ..., 3.1437e-10,\n           7.2422e-07, 4.0368e-04],\n          ...,\n          [1.1580e-05, 8.0734e-07, 1.5965e-04,  ..., 1.2842e-04,\n           1.1106e+00, 1.4778e-04],\n          [0.0000e+00, 1.8123e-07, 2.8532e-06,  ..., 3.2130e-06,\n           8.4367e-04, 1.1101e+00],\n          [9.9252e-01, 2.9987e-06, 1.7422e-04,  ..., 6.8606e-05,\n           2.2479e-04, 7.0163e-02]],\n\n         [[7.5690e-01, 1.6165e-02, 1.2496e-02,  ..., 1.3000e-02,\n           3.2473e-02, 1.2968e-01],\n          [7.3788e-01, 6.1461e-03, 4.0369e-03,  ..., 1.1575e-02,\n           2.8086e-02, 2.8888e-01],\n          [8.2865e-01, 1.5599e-02, 2.3565e-03,  ..., 0.0000e+00,\n           0.0000e+00, 1.9221e-01],\n          ...,\n          [1.8211e-01, 4.0028e-02, 2.1272e-02,  ..., 4.3288e-02,\n           1.3862e-01, 2.3199e-01],\n          [7.6895e-01, 0.0000e+00, 1.2622e-03,  ..., 1.4669e-02,\n           6.7931e-02, 2.1202e-01],\n          [8.2704e-01, 8.8765e-03, 2.0253e-03,  ..., 1.3526e-02,\n           4.9937e-02, 1.7789e-01]],\n\n         [[8.4221e-01, 8.9846e-03, 8.5757e-03,  ..., 1.6228e-02,\n           4.7026e-02, 9.1753e-02],\n          [8.4534e-01, 3.1123e-03, 9.1802e-03,  ..., 8.7581e-03,\n           7.5119e-02, 1.4084e-01],\n          [1.2033e-01, 2.9472e-03, 5.8862e-03,  ..., 1.5884e-02,\n           1.0127e-01, 1.2003e-01],\n          ...,\n          [3.0774e-01, 0.0000e+00, 4.7216e-02,  ..., 2.0306e-02,\n           1.0744e-01, 2.3181e-01],\n          [5.7709e-01, 6.3023e-02, 1.2865e-02,  ..., 3.0301e-02,\n           6.2884e-02, 1.5522e-01],\n          [4.7107e-01, 1.8050e-02, 1.9260e-02,  ..., 4.0701e-02,\n           9.4477e-02, 1.8570e-01]],\n\n         ...,\n\n         [[7.1656e-01, 1.1269e-02, 2.7426e-03,  ..., 6.4621e-03,\n           8.3974e-03, 9.7262e-02],\n          [3.2355e-07, 5.3775e-05, 1.1110e+00,  ..., 0.0000e+00,\n           2.4364e-07, 6.9047e-06],\n          [6.4337e-05, 4.4597e-06, 3.2404e-03,  ..., 0.0000e+00,\n           3.5153e-07, 1.1060e-03],\n          ...,\n          [8.4459e-05, 3.8987e-07, 6.5964e-05,  ..., 1.1575e-04,\n           1.1105e+00, 3.1040e-04],\n          [3.6584e-04, 3.4719e-07, 3.1054e-06,  ..., 1.0410e-05,\n           7.2164e-04, 1.1100e+00],\n          [1.0989e+00, 7.7424e-06, 1.3293e-05,  ..., 4.2806e-05,\n           2.5546e-04, 5.8478e-03]],\n\n         [[7.1974e-01, 1.1155e-02, 0.0000e+00,  ..., 3.2373e-02,\n           3.8284e-02, 1.7937e-01],\n          [1.8459e-01, 3.2845e-02, 7.6014e-03,  ..., 0.0000e+00,\n           4.6919e-02, 5.8844e-01],\n          [1.0465e-02, 9.5576e-04, 6.2225e-01,  ..., 3.2683e-03,\n           6.2369e-03, 6.3263e-03],\n          ...,\n          [8.5494e-02, 1.6316e-02, 6.9883e-02,  ..., 0.0000e+00,\n           4.5686e-02, 1.3056e-01],\n          [7.0032e-01, 0.0000e+00, 2.5478e-02,  ..., 0.0000e+00,\n           0.0000e+00, 1.3095e-01],\n          [8.1576e-01, 1.1057e-02, 4.6338e-03,  ..., 3.0017e-02,\n           2.8664e-02, 9.7498e-02]],\n\n         [[0.0000e+00, 1.4503e-03, 6.2663e-04,  ..., 2.2547e-03,\n           5.0061e-03, 1.3625e-02],\n          [4.1683e-02, 5.6817e-02, 2.3088e-01,  ..., 7.1537e-02,\n           9.1507e-02, 3.1159e-02],\n          [1.2252e-01, 5.0907e-02, 2.1752e-02,  ..., 5.0210e-02,\n           6.0966e-02, 4.5902e-02],\n          ...,\n          [8.7288e-02, 3.5717e-02, 5.7849e-02,  ..., 4.5807e-02,\n           0.0000e+00, 1.1415e-01],\n          [4.9138e-01, 9.6653e-03, 4.6301e-03,  ..., 3.3486e-02,\n           7.5975e-02, 4.0089e-01],\n          [1.0767e+00, 1.2709e-03, 5.9675e-04,  ..., 2.2813e-03,\n           0.0000e+00, 9.7194e-03]]]], device='cuda:0'), tensor([[[[2.3790e-01, 1.6350e-03, 2.2872e-03,  ..., 5.5605e-03,\n           4.8685e-02, 7.9344e-01],\n          [1.9610e-01, 4.8274e-02, 2.9712e-05,  ..., 1.3101e-04,\n           9.1544e-03, 2.6701e-01],\n          [0.0000e+00, 1.0996e-04, 3.1702e-01,  ..., 0.0000e+00,\n           6.5313e-02, 4.8090e-01],\n          ...,\n          [1.9485e-02, 2.4130e-04, 8.8896e-04,  ..., 0.0000e+00,\n           2.9301e-03, 0.0000e+00],\n          [1.5364e-01, 9.4057e-04, 4.2036e-03,  ..., 2.1650e-03,\n           0.0000e+00, 7.2835e-01],\n          [2.4495e-01, 4.2350e-03, 1.2728e-03,  ..., 0.0000e+00,\n           2.5600e-02, 7.7925e-01]],\n\n         [[2.8925e-01, 2.8585e-02, 7.4699e-02,  ..., 7.1110e-03,\n           8.4264e-02, 4.1865e-01],\n          [4.9137e-02, 1.4856e-01, 5.0027e-02,  ..., 8.6216e-02,\n           7.0850e-02, 7.8001e-02],\n          [6.3299e-01, 5.6628e-02, 0.0000e+00,  ..., 4.4849e-03,\n           1.6502e-02, 2.4165e-01],\n          ...,\n          [3.8476e-02, 2.4993e-01, 3.6854e-02,  ..., 0.0000e+00,\n           1.1108e-01, 5.2356e-02],\n          [2.7414e-02, 6.8253e-02, 3.8384e-03,  ..., 1.6605e-02,\n           1.0918e-01, 6.9048e-01],\n          [0.0000e+00, 1.6153e-03, 6.6206e-03,  ..., 2.2090e-03,\n           2.7885e-02, 5.4261e-01]],\n\n         [[3.6396e-02, 4.3260e-02, 1.8802e-01,  ..., 3.9620e-02,\n           4.1101e-02, 7.6338e-02],\n          [2.7263e-01, 3.7131e-03, 8.4102e-03,  ..., 2.1096e-02,\n           2.3726e-01, 4.3226e-01],\n          [5.0570e-01, 6.0033e-03, 1.8177e-03,  ..., 0.0000e+00,\n           6.0085e-02, 2.8470e-01],\n          ...,\n          [1.9849e-01, 5.6631e-03, 1.2525e-02,  ..., 1.0576e-01,\n           3.2887e-01, 1.3696e-01],\n          [2.4444e-01, 1.1861e-02, 3.1479e-02,  ..., 0.0000e+00,\n           2.9791e-01, 1.9827e-01],\n          [1.5298e-01, 1.2729e-02, 8.2094e-03,  ..., 1.1940e-02,\n           7.3284e-03, 7.4680e-01]],\n\n         ...,\n\n         [[5.4127e-01, 7.6787e-03, 2.3090e-02,  ..., 3.3841e-02,\n           3.6769e-02, 1.4302e-01],\n          [2.8997e-01, 2.3649e-02, 1.9958e-01,  ..., 0.0000e+00,\n           3.5860e-03, 4.0669e-01],\n          [4.4559e-01, 1.5939e-03, 1.0023e-02,  ..., 8.0031e-04,\n           1.2963e-03, 4.5647e-01],\n          ...,\n          [3.1211e-01, 6.8305e-04, 4.3132e-03,  ..., 0.0000e+00,\n           6.6687e-01, 6.4307e-02],\n          [8.7376e-01, 4.4280e-04, 0.0000e+00,  ..., 0.0000e+00,\n           3.9911e-02, 1.7517e-01],\n          [0.0000e+00, 2.0409e-03, 7.1160e-03,  ..., 6.4804e-03,\n           0.0000e+00, 7.9486e-02]],\n\n         [[2.3870e-01, 1.3565e-02, 1.0191e-02,  ..., 8.8431e-03,\n           1.0314e-02, 6.7270e-01],\n          [0.0000e+00, 1.9270e-01, 0.0000e+00,  ..., 3.4590e-02,\n           3.8381e-02, 1.0308e-01],\n          [2.2961e-01, 7.0934e-03, 1.1638e-02,  ..., 8.8846e-03,\n           1.8200e-02, 0.0000e+00],\n          ...,\n          [1.8702e-03, 1.2237e-04, 2.6778e-03,  ..., 4.7103e-04,\n           3.2230e-04, 1.9436e-03],\n          [3.8955e-02, 1.6911e-01, 9.8186e-03,  ..., 7.8805e-02,\n           0.0000e+00, 7.3816e-02],\n          [0.0000e+00, 4.8489e-04, 1.1239e-03,  ..., 7.2276e-04,\n           4.3173e-03, 7.9942e-01]],\n\n         [[0.0000e+00, 2.2379e-02, 1.3932e-02,  ..., 1.2954e-02,\n           0.0000e+00, 2.2705e-01],\n          [7.6122e-01, 8.2481e-02, 1.2652e-02,  ..., 7.8890e-04,\n           8.7417e-03, 2.0096e-01],\n          [7.8077e-01, 0.0000e+00, 2.2134e-03,  ..., 2.3930e-04,\n           1.7405e-02, 2.1064e-01],\n          ...,\n          [7.3813e-03, 0.0000e+00, 1.6292e-02,  ..., 2.2185e-02,\n           1.0565e-02, 2.5264e-02],\n          [7.5368e-02, 2.2395e-02, 1.4172e-02,  ..., 1.0708e-01,\n           0.0000e+00, 2.0382e-01],\n          [7.8137e-01, 5.7506e-03, 2.1991e-03,  ..., 3.2616e-03,\n           3.4687e-02, 2.4851e-01]]]], device='cuda:0'), tensor([[[[5.6403e-02, 3.4783e-02, 5.7810e-03,  ..., 0.0000e+00,\n           1.0035e-02, 6.7595e-01],\n          [1.4890e-02, 2.3049e-01, 6.3408e-03,  ..., 7.5518e-02,\n           4.2676e-02, 1.7464e-01],\n          [4.1046e-02, 1.2277e-02, 6.7496e-03,  ..., 1.5041e-03,\n           3.2519e-03, 8.2220e-01],\n          ...,\n          [0.0000e+00, 1.2141e-02, 2.0006e-03,  ..., 9.9310e-04,\n           5.0218e-04, 0.0000e+00],\n          [3.0711e-02, 9.3854e-02, 2.2653e-03,  ..., 9.7420e-03,\n           1.0120e-01, 0.0000e+00],\n          [5.0134e-02, 6.3859e-03, 1.1618e-04,  ..., 1.7677e-03,\n           8.5885e-02, 9.3249e-01]],\n\n         [[1.5734e-02, 3.9332e-02, 0.0000e+00,  ..., 1.3956e-03,\n           7.1417e-03, 1.3639e-03],\n          [1.0855e-02, 3.5802e-02, 8.8440e-02,  ..., 3.4458e-02,\n           3.4802e-02, 2.0199e-01],\n          [1.7990e-02, 8.2540e-02, 8.4019e-02,  ..., 2.6154e-02,\n           4.2369e-02, 3.3240e-01],\n          ...,\n          [1.1805e-03, 3.5461e-02, 2.7142e-01,  ..., 4.7695e-02,\n           1.9797e-02, 2.2240e-01],\n          [0.0000e+00, 7.0717e-02, 3.3604e-02,  ..., 1.3691e-01,\n           6.3883e-02, 1.4197e-02],\n          [4.9463e-02, 2.6925e-02, 8.8744e-03,  ..., 1.9827e-02,\n           2.9033e-02, 0.0000e+00]],\n\n         [[3.2739e-02, 9.6726e-02, 6.3322e-02,  ..., 0.0000e+00,\n           3.2271e-02, 7.2666e-02],\n          [3.9784e-02, 7.4643e-03, 6.8733e-03,  ..., 5.6278e-03,\n           2.0109e-02, 9.6052e-01],\n          [6.8485e-02, 1.3828e-03, 0.0000e+00,  ..., 1.7191e-03,\n           7.0755e-02, 9.5936e-01],\n          ...,\n          [2.6305e-01, 1.0503e-02, 1.3861e-02,  ..., 5.2771e-03,\n           9.0491e-02, 6.1692e-01],\n          [1.0673e-01, 8.5262e-02, 4.1299e-02,  ..., 2.9871e-02,\n           8.6524e-02, 2.9579e-01],\n          [1.1279e-01, 7.0594e-03, 0.0000e+00,  ..., 8.5850e-03,\n           0.0000e+00, 8.5127e-01]],\n\n         ...,\n\n         [[3.4917e-02, 1.7725e-02, 1.6969e-02,  ..., 3.2617e-02,\n           1.4727e-02, 6.2763e-01],\n          [6.5160e-02, 8.3098e-03, 4.1880e-03,  ..., 2.0572e-02,\n           5.2591e-02, 8.1065e-01],\n          [7.9757e-02, 1.7425e-02, 0.0000e+00,  ..., 2.2592e-02,\n           1.3423e-02, 5.6366e-01],\n          ...,\n          [1.2893e-01, 5.4122e-02, 0.0000e+00,  ..., 1.2264e-01,\n           6.9010e-02, 3.5104e-01],\n          [9.8988e-02, 8.6816e-02, 1.0602e-02,  ..., 1.1380e-01,\n           6.3021e-02, 3.8231e-01],\n          [3.8176e-02, 6.5823e-03, 3.7127e-03,  ..., 3.4081e-03,\n           1.1013e-02, 0.0000e+00]],\n\n         [[1.5933e-02, 4.7629e-03, 1.7364e-02,  ..., 3.9391e-03,\n           2.6983e-02, 9.4205e-01],\n          [5.3960e-03, 4.1289e-03, 2.3272e-03,  ..., 5.3445e-04,\n           9.5588e-03, 1.0821e+00],\n          [2.1661e-02, 1.2658e-02, 7.1448e-03,  ..., 3.3343e-04,\n           8.9442e-03, 1.0464e+00],\n          ...,\n          [1.2600e-02, 2.8498e-02, 4.3488e-02,  ..., 1.7599e-03,\n           3.2041e-03, 2.0179e-02],\n          [4.7161e-02, 2.2710e-01, 5.7426e-02,  ..., 4.1269e-02,\n           1.8183e-02, 6.4799e-02],\n          [1.0110e-02, 0.0000e+00, 2.4591e-03,  ..., 4.8557e-04,\n           4.7645e-03, 1.0582e+00]],\n\n         [[5.1918e-02, 1.1842e-01, 3.0507e-02,  ..., 9.9250e-03,\n           1.5009e-02, 5.1607e-01],\n          [1.0732e-01, 4.2089e-02, 1.5285e-01,  ..., 1.7165e-04,\n           1.6572e-03, 7.6442e-01],\n          [6.6647e-02, 1.4449e-03, 2.4290e-02,  ..., 0.0000e+00,\n           2.7913e-03, 9.5098e-01],\n          ...,\n          [1.0642e-02, 0.0000e+00, 6.2988e-03,  ..., 8.5858e-02,\n           5.6718e-01, 4.0199e-01],\n          [6.2929e-03, 6.3506e-01, 6.2209e-02,  ..., 2.5255e-02,\n           5.1972e-02, 2.3694e-01],\n          [1.7176e-02, 3.5436e-03, 1.3636e-02,  ..., 8.9117e-03,\n           1.6626e-02, 9.9724e-01]]]], device='cuda:0'), tensor([[[[4.2997e-02, 0.0000e+00, 1.0053e-01,  ..., 2.6321e-02,\n           7.9428e-02, 4.3352e-01],\n          [0.0000e+00, 2.5723e-02, 8.1098e-02,  ..., 9.2260e-03,\n           2.9911e-02, 1.8593e-01],\n          [2.4165e-02, 1.1805e-02, 8.8947e-02,  ..., 0.0000e+00,\n           3.0882e-02, 2.1981e-01],\n          ...,\n          [2.2255e-02, 5.2855e-03, 4.3629e-03,  ..., 3.7998e-02,\n           4.8687e-02, 0.0000e+00],\n          [5.4422e-02, 9.1525e-02, 4.1301e-02,  ..., 5.0134e-02,\n           1.6419e-01, 0.0000e+00],\n          [0.0000e+00, 3.8742e-03, 7.3119e-03,  ..., 0.0000e+00,\n           1.2784e-02, 9.9979e-01]],\n\n         [[1.9289e-02, 0.0000e+00, 5.7788e-04,  ..., 3.3956e-02,\n           0.0000e+00, 6.7866e-01],\n          [1.0696e-02, 3.5859e-02, 2.2698e-03,  ..., 7.2690e-04,\n           1.9135e-01, 8.5544e-01],\n          [4.2869e-03, 6.6631e-02, 8.8655e-02,  ..., 2.3052e-04,\n           7.6053e-03, 8.8160e-01],\n          ...,\n          [1.4255e-02, 7.6667e-04, 6.0456e-04,  ..., 1.7294e-02,\n           1.2378e-02, 0.0000e+00],\n          [1.9140e-02, 4.5920e-02, 6.6836e-04,  ..., 3.4116e-01,\n           1.3477e-01, 0.0000e+00],\n          [8.3953e-03, 2.9238e-03, 4.1808e-04,  ..., 1.6857e-03,\n           1.7833e-02, 1.0546e+00]],\n\n         [[1.8228e-02, 1.9579e-02, 1.1852e-02,  ..., 3.0793e-03,\n           0.0000e+00, 9.0426e-01],\n          [1.3559e-02, 2.1021e-02, 0.0000e+00,  ..., 9.0693e-03,\n           4.4904e-02, 9.1944e-01],\n          [4.6220e-02, 4.2650e-02, 2.7362e-03,  ..., 2.5089e-02,\n           5.7632e-02, 6.7391e-01],\n          ...,\n          [1.5342e-02, 3.3725e-02, 6.3867e-03,  ..., 6.8273e-03,\n           7.1026e-02, 8.3063e-01],\n          [3.4187e-02, 7.0392e-02, 6.0895e-03,  ..., 5.8793e-03,\n           1.9484e-01, 6.4911e-01],\n          [3.2249e-02, 7.8706e-04, 1.2100e-03,  ..., 3.7878e-04,\n           1.0881e-03, 1.0601e+00]],\n\n         ...,\n\n         [[4.7922e-02, 6.2483e-02, 1.0997e-01,  ..., 7.1266e-03,\n           1.1592e-01, 5.0931e-01],\n          [9.5811e-04, 2.7322e-03, 0.0000e+00,  ..., 7.5471e-06,\n           4.4220e-04, 3.8975e-02],\n          [0.0000e+00, 0.0000e+00, 8.5592e-03,  ..., 3.1603e-06,\n           0.0000e+00, 7.0480e-04],\n          ...,\n          [4.0919e-03, 6.2378e-04, 2.4765e-05,  ..., 1.2813e-02,\n           1.0600e+00, 2.9339e-02],\n          [1.7554e-02, 9.6250e-02, 0.0000e+00,  ..., 7.1592e-03,\n           1.5265e-01, 6.0014e-01],\n          [2.0982e-01, 2.7054e-03, 7.3131e-03,  ..., 1.7651e-02,\n           6.9123e-02, 7.4573e-01]],\n\n         [[1.1846e-02, 1.0355e-01, 0.0000e+00,  ..., 1.8105e-02,\n           1.5499e-01, 5.3066e-01],\n          [1.7421e-02, 4.9346e-02, 1.9844e-03,  ..., 0.0000e+00,\n           3.4378e-02, 9.9697e-01],\n          [4.5143e-02, 3.4873e-01, 1.1256e-01,  ..., 0.0000e+00,\n           1.3112e-02, 0.0000e+00],\n          ...,\n          [0.0000e+00, 1.1805e-03, 1.8085e-03,  ..., 1.0404e-02,\n           2.3075e-03, 2.8761e-02],\n          [7.2367e-03, 0.0000e+00, 4.4821e-03,  ..., 7.0241e-02,\n           2.3181e-02, 1.3094e-01],\n          [1.3844e-02, 1.3738e-02, 5.7653e-03,  ..., 3.7604e-03,\n           1.2122e-02, 9.5426e-01]],\n\n         [[2.3637e-02, 1.0157e-02, 2.4030e-03,  ..., 1.3602e-02,\n           2.8384e-01, 7.1444e-01],\n          [7.5605e-02, 1.7957e-02, 0.0000e+00,  ..., 3.3488e-02,\n           2.3166e-01, 0.0000e+00],\n          [0.0000e+00, 1.6434e-03, 1.4592e-02,  ..., 9.0762e-03,\n           3.2631e-02, 6.1808e-01],\n          ...,\n          [3.2375e-02, 3.6227e-03, 3.8749e-04,  ..., 1.6623e-03,\n           2.8141e-02, 1.0289e+00],\n          [0.0000e+00, 2.7349e-02, 4.9629e-03,  ..., 1.2278e-02,\n           0.0000e+00, 0.0000e+00],\n          [1.2417e-02, 6.0687e-03, 0.0000e+00,  ..., 1.0169e-02,\n           1.3057e-02, 9.9602e-01]]]], device='cuda:0'), tensor([[[[6.9019e-03, 1.0991e-01, 1.4122e-02,  ..., 1.4289e-01,\n           2.3020e-01, 6.1854e-02],\n          [6.5154e-04, 6.4900e-01, 1.1079e-03,  ..., 3.3702e-03,\n           1.3874e-03, 3.3523e-01],\n          [1.7698e-03, 0.0000e+00, 5.6759e-02,  ..., 2.5925e-03,\n           1.8156e-04, 9.9885e-01],\n          ...,\n          [1.0755e-02, 0.0000e+00, 1.1659e-02,  ..., 2.6746e-02,\n           9.7014e-03, 4.5718e-01],\n          [5.2955e-03, 4.8599e-02, 2.9912e-03,  ..., 3.3891e-02,\n           2.3287e-01, 0.0000e+00],\n          [1.9658e-03, 1.6806e-03, 0.0000e+00,  ..., 1.6789e-03,\n           2.5362e-03, 1.0865e+00]],\n\n         [[3.7184e-02, 9.9966e-02, 1.7146e-02,  ..., 0.0000e+00,\n           2.7676e-01, 7.2290e-02],\n          [9.6074e-03, 2.9212e-03, 1.8653e-03,  ..., 2.2586e-03,\n           1.5717e-02, 1.0476e+00],\n          [2.6217e-03, 1.1253e-03, 1.6698e-03,  ..., 5.9818e-04,\n           4.0161e-03, 1.0865e+00],\n          ...,\n          [6.5627e-03, 1.3499e-02, 1.0359e-02,  ..., 2.9157e-03,\n           0.0000e+00, 1.0021e+00],\n          [3.4454e-02, 6.2435e-02, 1.7737e-02,  ..., 3.8801e-02,\n           1.1875e-01, 0.0000e+00],\n          [0.0000e+00, 0.0000e+00, 2.7237e-03,  ..., 2.9110e-03,\n           5.8388e-03, 1.0656e+00]],\n\n         [[7.2314e-03, 0.0000e+00, 2.0019e-03,  ..., 0.0000e+00,\n           9.0798e-01, 1.0730e-01],\n          [0.0000e+00, 1.2899e-02, 1.8035e-04,  ..., 7.3738e-04,\n           4.3875e-03, 1.0838e+00],\n          [7.8512e-03, 8.7653e-02, 0.0000e+00,  ..., 5.6631e-04,\n           9.1468e-03, 9.8432e-01],\n          ...,\n          [2.7900e-02, 2.6622e-01, 9.4194e-03,  ..., 5.0872e-03,\n           7.6920e-03, 3.7916e-01],\n          [1.3410e-01, 1.1978e-01, 1.2224e-02,  ..., 1.8710e-02,\n           1.0668e-01, 0.0000e+00],\n          [1.5132e-02, 1.5718e-02, 2.7722e-03,  ..., 3.4092e-03,\n           2.4583e-02, 0.0000e+00]],\n\n         ...,\n\n         [[4.3643e-02, 2.1577e-02, 2.6378e-03,  ..., 0.0000e+00,\n           0.0000e+00, 8.2241e-01],\n          [3.5926e-02, 2.6440e-02, 1.9678e-03,  ..., 5.0733e-03,\n           5.9435e-02, 9.7031e-01],\n          [1.6649e-01, 3.3995e-01, 0.0000e+00,  ..., 5.2797e-03,\n           7.9649e-02, 1.6739e-01],\n          ...,\n          [4.2031e-02, 3.3867e-02, 2.1474e-02,  ..., 3.8423e-03,\n           1.2598e-02, 5.5732e-02],\n          [0.0000e+00, 3.0821e-02, 1.0899e-02,  ..., 6.4513e-02,\n           1.0026e-01, 2.9406e-01],\n          [0.0000e+00, 3.4376e-02, 6.5998e-03,  ..., 6.1043e-03,\n           0.0000e+00, 9.1189e-01]],\n\n         [[7.4271e-03, 9.0854e-02, 2.1503e-03,  ..., 1.4010e-02,\n           2.8309e-02, 8.3215e-01],\n          [2.3741e-03, 9.4884e-02, 4.7532e-05,  ..., 3.7604e-04,\n           5.9759e-03, 9.8286e-01],\n          [7.0804e-03, 5.4518e-02, 6.6611e-03,  ..., 6.2517e-04,\n           2.6450e-03, 0.0000e+00],\n          ...,\n          [6.6026e-02, 1.1028e-01, 1.9818e-02,  ..., 0.0000e+00,\n           1.4045e-02, 6.6658e-01],\n          [0.0000e+00, 0.0000e+00, 6.8478e-04,  ..., 2.6999e-03,\n           9.8267e-02, 4.4528e-01],\n          [5.9468e-03, 1.9545e-02, 2.1824e-03,  ..., 4.5174e-03,\n           1.0398e-02, 1.0104e+00]],\n\n         [[0.0000e+00, 1.5867e-03, 0.0000e+00,  ..., 0.0000e+00,\n           1.0118e+00, 2.2338e-02],\n          [0.0000e+00, 3.6276e-02, 2.6123e-03,  ..., 6.0170e-03,\n           7.0263e-01, 3.2430e-01],\n          [8.4093e-03, 1.0028e+00, 2.7972e-02,  ..., 1.3356e-05,\n           9.6711e-03, 3.9289e-02],\n          ...,\n          [5.4433e-04, 3.6064e-04, 7.4202e-04,  ..., 3.6696e-03,\n           2.8227e-03, 9.5670e-02],\n          [1.2325e-04, 5.1180e-05, 3.4553e-05,  ..., 1.0024e+00,\n           4.3002e-02, 3.1822e-02],\n          [0.0000e+00, 5.5847e-03, 5.8377e-03,  ..., 6.9144e-03,\n           0.0000e+00, 9.7037e-01]]]], device='cuda:0'), tensor([[[[1.4460e-02, 7.7931e-03, 7.6393e-03,  ..., 0.0000e+00,\n           1.9852e-02, 9.5802e-01],\n          [1.3470e-02, 2.5245e-02, 2.2902e-03,  ..., 6.0783e-03,\n           1.3690e-02, 9.8484e-01],\n          [8.2874e-02, 1.1523e-01, 5.3350e-02,  ..., 8.5659e-03,\n           1.3764e-01, 5.5604e-01],\n          ...,\n          [4.9373e-02, 6.3900e-02, 7.9700e-03,  ..., 1.4766e-02,\n           2.7269e-01, 2.3298e-01],\n          [1.4513e-01, 1.0106e-02, 7.4890e-03,  ..., 1.4922e-02,\n           2.7598e-01, 4.6026e-01],\n          [1.2088e-02, 5.8624e-03, 3.7044e-04,  ..., 1.2628e-03,\n           0.0000e+00, 0.0000e+00]],\n\n         [[0.0000e+00, 1.7719e-02, 2.5286e-03,  ..., 2.1791e-02,\n           6.2301e-01, 2.1508e-01],\n          [4.9343e-03, 2.0069e-02, 3.1985e-03,  ..., 2.3463e-02,\n           7.0265e-02, 7.8045e-01],\n          [1.8450e-03, 1.3095e-03, 4.9438e-03,  ..., 1.7087e-03,\n           3.9569e-03, 1.0506e+00],\n          ...,\n          [2.0854e-02, 1.4367e-02, 0.0000e+00,  ..., 2.1487e-02,\n           0.0000e+00, 8.5055e-01],\n          [7.4883e-02, 1.4846e-02, 1.6173e-02,  ..., 4.6810e-02,\n           3.2515e-01, 3.3539e-01],\n          [1.1653e-02, 2.9220e-03, 8.5011e-04,  ..., 3.8914e-03,\n           1.1661e-02, 1.0595e+00]],\n\n         [[1.5905e-01, 2.9515e-01, 3.0535e-02,  ..., 1.7783e-02,\n           3.8656e-02, 4.1158e-01],\n          [1.3591e-02, 6.9880e-02, 3.4844e-02,  ..., 1.0011e-02,\n           2.0627e-03, 8.9226e-01],\n          [1.5016e-03, 3.1568e-02, 3.3846e-02,  ..., 1.6685e-03,\n           5.7415e-04, 3.4861e-01],\n          ...,\n          [0.0000e+00, 0.0000e+00, 4.2231e-03,  ..., 1.1125e-02,\n           4.2451e-01, 3.8324e-01],\n          [2.1067e-01, 4.9690e-01, 1.4732e-02,  ..., 2.6776e-03,\n           6.4553e-03, 3.1425e-01],\n          [1.8245e-02, 1.3564e-02, 4.7926e-03,  ..., 5.6132e-03,\n           1.0658e-02, 9.8297e-01]],\n\n         ...,\n\n         [[1.4671e-02, 1.4025e-02, 0.0000e+00,  ..., 0.0000e+00,\n           3.7337e-02, 9.4701e-01],\n          [2.2575e-02, 3.9706e-02, 1.5571e-03,  ..., 2.0268e-03,\n           2.6809e-02, 0.0000e+00],\n          [0.0000e+00, 2.5546e-02, 5.1075e-02,  ..., 3.7690e-03,\n           2.2153e-02, 9.1825e-01],\n          ...,\n          [9.6016e-02, 1.0419e-01, 2.2019e-03,  ..., 5.0018e-03,\n           3.1718e-01, 5.1403e-02],\n          [7.1585e-02, 4.7174e-02, 9.4844e-04,  ..., 1.4639e-02,\n           3.1209e-01, 4.6764e-01],\n          [1.1847e-02, 8.8042e-03, 3.7069e-03,  ..., 6.3637e-03,\n           8.4066e-03, 1.0126e+00]],\n\n         [[1.1415e-02, 2.2773e-02, 1.9007e-02,  ..., 9.2823e-02,\n           1.2454e-01, 2.7952e-01],\n          [2.9789e-02, 8.4330e-02, 2.3472e-03,  ..., 2.7805e-02,\n           4.3566e-02, 5.4780e-01],\n          [2.5692e-03, 7.2703e-04, 3.7170e-02,  ..., 0.0000e+00,\n           3.4413e-04, 9.0161e-01],\n          ...,\n          [6.8667e-04, 1.2014e-03, 5.5845e-02,  ..., 1.1688e-02,\n           8.2251e-04, 1.7584e-02],\n          [2.7676e-02, 5.5481e-02, 6.0536e-02,  ..., 8.2150e-02,\n           4.2458e-02, 1.7911e-01],\n          [1.6682e-02, 3.7957e-03, 2.7515e-03,  ..., 3.0253e-03,\n           1.5125e-02, 1.0409e+00]],\n\n         [[4.7022e-04, 3.4089e-04, 7.7065e-05,  ..., 2.3882e-03,\n           1.0904e+00, 4.6929e-03],\n          [0.0000e+00, 7.3467e-02, 5.2672e-03,  ..., 3.2420e-02,\n           1.8681e-01, 7.2938e-01],\n          [3.2706e-03, 2.5912e-03, 5.0355e-02,  ..., 9.6913e-04,\n           6.3527e-04, 1.0423e+00],\n          ...,\n          [1.1597e-02, 1.2387e-03, 2.3943e-03,  ..., 1.9300e-02,\n           3.1235e-02, 7.9996e-01],\n          [1.7032e-02, 1.2518e-03, 1.2697e-04,  ..., 1.4166e-01,\n           4.6012e-01, 0.0000e+00],\n          [1.2954e-02, 4.9945e-03, 0.0000e+00,  ..., 7.0025e-03,\n           2.2046e-02, 1.0128e+00]]]], device='cuda:0'), tensor([[[[9.4120e-02, 3.7012e-03, 2.9435e-03,  ..., 1.9360e-02,\n           1.7976e-01, 7.3756e-01],\n          [3.7758e-02, 1.1542e-02, 1.1360e-03,  ..., 1.6675e-02,\n           4.3447e-02, 8.6059e-01],\n          [4.9159e-02, 8.7592e-04, 2.4793e-03,  ..., 2.2598e-03,\n           3.9029e-03, 9.6356e-01],\n          ...,\n          [1.8643e-01, 8.5464e-03, 1.3152e-03,  ..., 4.2841e-02,\n           0.0000e+00, 7.2246e-01],\n          [2.6932e-01, 2.4512e-03, 8.1896e-04,  ..., 3.4896e-02,\n           1.8680e-01, 0.0000e+00],\n          [1.9351e-02, 4.7973e-03, 6.8161e-04,  ..., 4.7794e-03,\n           1.3359e-02, 1.0387e+00]],\n\n         [[0.0000e+00, 6.3455e-02, 2.7303e-02,  ..., 0.0000e+00,\n           1.8298e-02, 5.2210e-01],\n          [6.8099e-03, 5.4749e-02, 1.4828e-02,  ..., 8.8264e-03,\n           9.7385e-03, 8.7814e-01],\n          [2.6767e-02, 1.9334e-02, 8.2825e-02,  ..., 4.5444e-02,\n           1.6107e-02, 6.8352e-01],\n          ...,\n          [2.5144e-02, 8.6033e-02, 1.6954e-02,  ..., 6.1590e-02,\n           3.2165e-02, 5.2210e-01],\n          [7.8428e-02, 6.3996e-02, 6.4353e-03,  ..., 2.5689e-02,\n           0.0000e+00, 7.1196e-01],\n          [1.2138e-02, 3.9969e-02, 7.0577e-03,  ..., 8.4484e-03,\n           0.0000e+00, 8.6502e-01]],\n\n         [[5.3412e-02, 8.3009e-03, 4.5471e-03,  ..., 0.0000e+00,\n           3.0801e-02, 9.9401e-01],\n          [0.0000e+00, 5.1424e-03, 0.0000e+00,  ..., 1.2110e-04,\n           0.0000e+00, 1.0715e+00],\n          [4.1558e-03, 2.9369e-04, 6.5325e-02,  ..., 7.0104e-04,\n           6.0941e-04, 7.1556e-01],\n          ...,\n          [3.3987e-02, 6.7667e-03, 3.4332e-04,  ..., 0.0000e+00,\n           7.9426e-01, 2.4539e-01],\n          [5.4050e-02, 1.3174e-01, 1.0350e-02,  ..., 6.0246e-03,\n           1.3092e-02, 8.2863e-01],\n          [1.6259e-02, 3.6887e-03, 8.6761e-03,  ..., 3.3559e-03,\n           5.1197e-03, 1.0340e+00]],\n\n         ...,\n\n         [[0.0000e+00, 0.0000e+00, 4.4541e-03,  ..., 2.8493e-02,\n           0.0000e+00, 0.0000e+00],\n          [1.0226e-02, 3.3252e-01, 1.3994e-02,  ..., 2.4697e-02,\n           1.6817e-02, 2.9776e-01],\n          [0.0000e+00, 3.4445e-02, 1.6615e-01,  ..., 6.8722e-03,\n           1.4553e-02, 4.8543e-01],\n          ...,\n          [0.0000e+00, 0.0000e+00, 3.3873e-02,  ..., 5.4551e-02,\n           2.0679e-01, 1.4005e-01],\n          [1.1218e-01, 1.3653e-01, 4.4401e-03,  ..., 7.5425e-02,\n           1.3729e-01, 4.3787e-01],\n          [1.6589e-02, 2.0382e-02, 3.3030e-03,  ..., 9.4292e-03,\n           1.8389e-02, 9.6808e-01]],\n\n         [[1.3146e-02, 4.9652e-02, 1.9948e-02,  ..., 3.8380e-02,\n           4.0807e-02, 7.2156e-01],\n          [2.9743e-03, 2.9495e-02, 1.4754e-02,  ..., 3.2898e-03,\n           6.2253e-03, 9.7920e-01],\n          [3.7492e-04, 2.5793e-04, 2.9123e-01,  ..., 9.0506e-04,\n           0.0000e+00, 6.0163e-01],\n          ...,\n          [1.1876e-02, 5.4044e-02, 2.2819e-01,  ..., 4.6829e-02,\n           3.5403e-02, 2.9187e-02],\n          [4.1838e-02, 1.7552e-01, 6.8394e-03,  ..., 0.0000e+00,\n           1.0792e-01, 2.1981e-01],\n          [2.6655e-03, 4.9907e-03, 4.1042e-03,  ..., 3.7879e-03,\n           3.9236e-03, 1.0566e+00]],\n\n         [[3.8252e-02, 3.0340e-03, 1.2515e-04,  ..., 3.5236e-03,\n           7.4570e-02, 9.8134e-01],\n          [0.0000e+00, 2.3361e-01, 0.0000e+00,  ..., 4.3223e-02,\n           0.0000e+00, 5.0863e-01],\n          [1.5021e-02, 8.8420e-02, 6.2718e-02,  ..., 3.2917e-02,\n           8.9746e-03, 3.4254e-01],\n          ...,\n          [2.0796e-01, 1.0494e-01, 8.0175e-04,  ..., 0.0000e+00,\n           3.9751e-01, 2.3112e-01],\n          [3.0694e-01, 2.3043e-02, 1.0759e-04,  ..., 7.4768e-03,\n           1.8766e-01, 5.6321e-01],\n          [1.9242e-02, 1.7189e-02, 1.9384e-03,  ..., 7.5827e-03,\n           2.4722e-02, 9.6091e-01]]]], device='cuda:0'), tensor([[[[5.9249e-02, 4.6874e-03, 1.4202e-02,  ..., 1.2120e-02,\n           4.2109e-02, 0.0000e+00],\n          [0.0000e+00, 1.8270e-01, 0.0000e+00,  ..., 1.9912e-04,\n           6.2509e-03, 7.1247e-01],\n          [6.2598e-04, 3.6084e-05, 7.9733e-01,  ..., 8.7413e-07,\n           5.0492e-05, 3.2594e-02],\n          ...,\n          [1.8281e-02, 4.4235e-03, 1.6306e-02,  ..., 2.0684e-01,\n           2.0492e-01, 0.0000e+00],\n          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.4719e-02,\n           3.4894e-01, 5.9257e-01],\n          [1.2558e-02, 4.9126e-03, 2.2015e-02,  ..., 3.4584e-03,\n           4.2235e-03, 9.8008e-01]],\n\n         [[9.1143e-02, 4.7096e-04, 0.0000e+00,  ..., 1.5922e-01,\n           2.2845e-01, 1.6670e-01],\n          [3.8972e-02, 2.3679e-02, 6.5220e-02,  ..., 3.0057e-02,\n           3.1352e-02, 6.8724e-01],\n          [8.4429e-03, 1.4527e-04, 1.3254e-01,  ..., 8.7608e-05,\n           4.2565e-04, 0.0000e+00],\n          ...,\n          [0.0000e+00, 3.2487e-04, 2.5177e-01,  ..., 1.1019e-01,\n           6.9515e-02, 1.0211e-01],\n          [1.5231e-01, 6.6203e-03, 9.4008e-03,  ..., 4.2906e-01,\n           3.0274e-02, 9.2624e-02],\n          [1.1501e-01, 3.9978e-03, 6.2159e-02,  ..., 1.6241e-02,\n           0.0000e+00, 6.6337e-01]],\n\n         [[8.9909e-02, 2.2236e-03, 1.5981e-02,  ..., 0.0000e+00,\n           1.0820e-01, 8.5180e-01],\n          [2.3321e-02, 1.8193e-02, 8.0876e-04,  ..., 3.9263e-03,\n           3.9258e-02, 9.8558e-01],\n          [0.0000e+00, 6.1962e-03, 3.9944e-02,  ..., 2.6795e-03,\n           2.9744e-03, 9.5032e-01],\n          ...,\n          [4.7194e-02, 8.6153e-02, 7.9864e-02,  ..., 6.5757e-03,\n           9.4251e-02, 2.9966e-01],\n          [3.6598e-01, 2.6911e-02, 3.1826e-03,  ..., 2.1958e-02,\n           2.7380e-01, 2.9696e-01],\n          [3.5273e-02, 1.5575e-02, 1.0147e-01,  ..., 9.4082e-03,\n           0.0000e+00, 6.0432e-01]],\n\n         ...,\n\n         [[3.9787e-02, 9.1020e-04, 4.2840e-04,  ..., 1.3857e-02,\n           0.0000e+00, 1.0043e+00],\n          [1.7761e-02, 9.8662e-02, 8.4850e-04,  ..., 9.6474e-03,\n           2.0767e-02, 0.0000e+00],\n          [2.1364e-02, 6.0584e-02, 4.9498e-02,  ..., 0.0000e+00,\n           3.5977e-03, 8.8748e-01],\n          ...,\n          [0.0000e+00, 7.8614e-04, 1.4205e-04,  ..., 1.6521e-02,\n           5.2123e-03, 1.4711e-01],\n          [2.2157e-02, 3.3646e-03, 1.4845e-03,  ..., 9.2808e-02,\n           1.9614e-02, 6.6732e-01],\n          [5.8091e-02, 1.5041e-02, 2.3582e-02,  ..., 1.7694e-02,\n           2.4442e-02, 7.3206e-01]],\n\n         [[5.4129e-02, 7.0899e-02, 1.2226e-01,  ..., 2.1474e-02,\n           6.9667e-02, 3.5300e-01],\n          [2.1679e-02, 1.9149e-02, 4.6787e-03,  ..., 3.5547e-03,\n           0.0000e+00, 9.8020e-01],\n          [4.6650e-02, 1.8234e-03, 5.6280e-03,  ..., 6.9676e-04,\n           0.0000e+00, 1.0139e+00],\n          ...,\n          [1.3929e-01, 5.0735e-02, 3.3500e-02,  ..., 0.0000e+00,\n           2.7060e-02, 5.6665e-01],\n          [3.5539e-01, 1.9140e-02, 2.1990e-02,  ..., 2.3169e-02,\n           6.9172e-02, 5.0869e-01],\n          [1.0490e-01, 0.0000e+00, 5.8359e-02,  ..., 1.0294e-02,\n           1.3470e-02, 4.9981e-01]],\n\n         [[1.4116e-01, 2.3194e-02, 4.6356e-02,  ..., 6.6567e-02,\n           2.9736e-01, 1.5376e-01],\n          [1.8638e-02, 2.5702e-05, 0.0000e+00,  ..., 2.6767e-03,\n           2.0401e-02, 9.0384e-01],\n          [0.0000e+00, 1.7338e-02, 1.8770e-02,  ..., 9.6580e-03,\n           1.9837e-02, 3.0618e-01],\n          ...,\n          [1.1311e-01, 9.0272e-03, 1.2504e-01,  ..., 2.3810e-02,\n           0.0000e+00, 2.4583e-01],\n          [0.0000e+00, 3.9391e-02, 1.5477e-02,  ..., 6.9819e-02,\n           6.8606e-02, 4.2436e-01],\n          [0.0000e+00, 5.1901e-02, 6.5573e-02,  ..., 4.9942e-02,\n           0.0000e+00, 1.2937e-01]]]], device='cuda:0'), tensor([[[[0.0000e+00, 2.7249e-02, 4.8814e-03,  ..., 3.1750e-02,\n           1.0178e-01, 3.9102e-01],\n          [5.6097e-02, 5.2603e-03, 0.0000e+00,  ..., 1.2516e-03,\n           8.9115e-03, 9.9740e-01],\n          [2.7489e-02, 2.2541e-03, 3.2769e-02,  ..., 3.9235e-03,\n           4.1092e-03, 8.7338e-01],\n          ...,\n          [2.1104e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n           7.1425e-02, 6.8700e-01],\n          [3.0496e-01, 1.0757e-01, 8.0089e-04,  ..., 5.5916e-03,\n           1.2575e-01, 4.5665e-01],\n          [3.4645e-02, 1.0936e-02, 0.0000e+00,  ..., 1.9777e-02,\n           0.0000e+00, 6.2413e-01]],\n\n         [[9.1241e-02, 8.4064e-03, 1.7491e-03,  ..., 6.6398e-03,\n           5.2755e-02, 9.3027e-01],\n          [1.8735e-03, 1.1669e-01, 8.3234e-05,  ..., 6.0391e-05,\n           6.0817e-03, 8.6065e-01],\n          [2.1019e-03, 8.9125e-06, 3.4812e-01,  ..., 4.2528e-06,\n           6.5575e-05, 6.4128e-01],\n          ...,\n          [1.4599e-02, 6.2182e-04, 8.4401e-05,  ..., 7.2752e-02,\n           0.0000e+00, 0.0000e+00],\n          [9.6535e-03, 9.9642e-03, 1.1631e-04,  ..., 0.0000e+00,\n           0.0000e+00, 1.0288e+00],\n          [4.5256e-03, 1.8974e-04, 1.3127e-03,  ..., 1.8257e-04,\n           1.7345e-03, 1.0956e+00]],\n\n         [[2.0938e-01, 0.0000e+00, 0.0000e+00,  ..., 7.6252e-02,\n           1.9892e-01, 4.1812e-01],\n          [8.9864e-03, 2.6322e-03, 5.0332e-05,  ..., 2.8281e-04,\n           1.4477e-02, 1.0750e+00],\n          [1.9032e-03, 6.6137e-05, 1.8548e-02,  ..., 6.1666e-05,\n           5.1606e-04, 1.0661e+00],\n          ...,\n          [5.6874e-02, 1.1669e-02, 3.3058e-03,  ..., 1.7027e-01,\n           4.2268e-01, 1.6506e-01],\n          [2.5246e-02, 5.4466e-03, 4.3873e-04,  ..., 2.4331e-02,\n           5.1290e-01, 0.0000e+00],\n          [2.8855e-02, 1.6383e-03, 3.2121e-03,  ..., 4.8179e-03,\n           0.0000e+00, 9.8348e-01]],\n\n         ...,\n\n         [[2.5842e-02, 8.2189e-04, 6.7887e-03,  ..., 2.1165e-03,\n           5.1298e-03, 1.0396e+00],\n          [5.3720e-03, 2.3296e-02, 1.5974e-04,  ..., 3.3344e-04,\n           8.6273e-04, 1.0660e+00],\n          [3.9992e-03, 1.3459e-04, 0.0000e+00,  ..., 1.7296e-05,\n           6.9288e-04, 9.1512e-01],\n          ...,\n          [4.7115e-03, 5.7700e-05, 1.8148e-04,  ..., 2.9467e-02,\n           0.0000e+00, 1.0304e+00],\n          [2.0080e-02, 7.6362e-04, 2.7234e-04,  ..., 3.8498e-03,\n           3.1435e-02, 1.0350e+00],\n          [3.1794e-03, 3.3538e-04, 2.1891e-03,  ..., 1.6991e-04,\n           1.1038e-03, 1.0934e+00]],\n\n         [[0.0000e+00, 1.2182e-03, 2.9266e-04,  ..., 4.9384e-02,\n           0.0000e+00, 0.0000e+00],\n          [1.2392e-01, 4.3948e-01, 4.0881e-05,  ..., 1.2989e-03,\n           6.8363e-02, 2.9322e-02],\n          [6.2611e-03, 5.3902e-08, 1.0209e+00,  ..., 0.0000e+00,\n           0.0000e+00, 0.0000e+00],\n          ...,\n          [2.9097e-01, 8.7884e-04, 1.9859e-04,  ..., 2.0798e-01,\n           1.1151e-01, 8.9359e-03],\n          [2.7033e-01, 2.1230e-02, 6.3502e-04,  ..., 3.9507e-02,\n           0.0000e+00, 1.6258e-02],\n          [9.6793e-02, 5.2686e-02, 5.9169e-02,  ..., 5.8055e-02,\n           5.5144e-02, 1.4443e-01]],\n\n         [[7.9109e-02, 1.0757e-02, 8.9579e-02,  ..., 6.0488e-02,\n           2.0694e-01, 5.5222e-02],\n          [8.6525e-03, 1.6237e-03, 7.5599e-03,  ..., 3.8178e-03,\n           0.0000e+00, 1.0035e+00],\n          [1.3214e-01, 3.9629e-04, 1.2252e-02,  ..., 4.6899e-03,\n           2.3844e-02, 8.4409e-01],\n          ...,\n          [8.3274e-02, 1.3194e-02, 7.3934e-02,  ..., 3.0806e-02,\n           1.6422e-01, 2.1964e-01],\n          [4.5234e-02, 0.0000e+00, 2.0196e-02,  ..., 9.9465e-02,\n           1.3640e-01, 4.0417e-01],\n          [4.2625e-02, 2.1080e-03, 9.1845e-03,  ..., 5.7966e-03,\n           1.2828e-02, 0.0000e+00]]]], device='cuda:0'), tensor([[[[7.7239e-02, 1.3117e-02, 1.6626e-02,  ..., 4.6407e-02,\n           7.0128e-03, 2.5354e-02],\n          [4.6540e-02, 2.0068e-02, 2.6341e-02,  ..., 2.8261e-03,\n           1.1311e-02, 8.4105e-01],\n          [4.5444e-02, 0.0000e+00, 7.9281e-03,  ..., 5.1042e-04,\n           1.4280e-03, 1.0160e+00],\n          ...,\n          [8.8563e-02, 1.4243e-02, 7.8241e-03,  ..., 6.5684e-02,\n           1.8506e-02, 3.6437e-01],\n          [2.5825e-01, 1.2286e-02, 0.0000e+00,  ..., 6.2288e-02,\n           1.7808e-01, 2.5753e-01],\n          [2.4643e-02, 3.4142e-04, 0.0000e+00,  ..., 0.0000e+00,\n           1.2147e-03, 0.0000e+00]],\n\n         [[9.4494e-02, 2.1288e-03, 1.7525e-02,  ..., 6.1559e-02,\n           3.7338e-02, 7.6232e-02],\n          [1.5785e-02, 1.6340e-03, 3.7240e-04,  ..., 2.5027e-03,\n           4.1161e-03, 1.0691e+00],\n          [0.0000e+00, 1.8645e-03, 9.8828e-05,  ..., 1.1623e-03,\n           3.3858e-03, 1.0798e+00],\n          ...,\n          [3.0521e-02, 2.9971e-03, 1.0475e-03,  ..., 4.6975e-03,\n           4.9814e-02, 9.9105e-01],\n          [0.0000e+00, 2.0016e-03, 2.0013e-03,  ..., 1.7628e-02,\n           8.6222e-02, 6.2812e-01],\n          [2.1328e-02, 1.2799e-03, 5.6209e-04,  ..., 2.7387e-03,\n           4.6182e-03, 1.0619e+00]],\n\n         [[4.8270e-02, 1.8117e-03, 2.2474e-03,  ..., 2.1061e-01,\n           5.6404e-02, 0.0000e+00],\n          [9.9192e-02, 0.0000e+00, 3.2346e-02,  ..., 1.5724e-02,\n           2.1677e-02, 4.2359e-01],\n          [2.3567e-02, 4.3375e-03, 1.7482e-02,  ..., 1.1865e-03,\n           2.1589e-03, 0.0000e+00],\n          ...,\n          [3.4500e-02, 9.0683e-03, 3.8400e-02,  ..., 1.0532e-01,\n           2.6113e-02, 3.3751e-01],\n          [5.1177e-02, 7.6241e-03, 7.6865e-03,  ..., 1.1917e-01,\n           8.3713e-02, 0.0000e+00],\n          [3.2946e-02, 3.4380e-03, 4.9632e-03,  ..., 7.8339e-03,\n           1.4780e-02, 0.0000e+00]],\n\n         ...,\n\n         [[9.5607e-02, 1.6180e-03, 1.2784e-02,  ..., 0.0000e+00,\n           3.8346e-02, 2.0690e-03],\n          [3.4904e-02, 3.5948e-01, 4.8430e-03,  ..., 1.2501e-02,\n           3.5723e-02, 1.4079e-01],\n          [5.5479e-02, 6.3056e-03, 7.2976e-02,  ..., 0.0000e+00,\n           1.5633e-02, 7.4836e-01],\n          ...,\n          [1.3025e-01, 1.5078e-02, 0.0000e+00,  ..., 2.0128e-01,\n           9.1189e-02, 0.0000e+00],\n          [1.8534e-01, 1.1549e-02, 4.0112e-03,  ..., 1.6444e-02,\n           3.3444e-01, 4.5740e-01],\n          [5.3033e-02, 2.3720e-02, 3.3924e-03,  ..., 6.5297e-03,\n           3.3810e-02, 9.0499e-01]],\n\n         [[0.0000e+00, 1.3022e-03, 2.3583e-02,  ..., 1.4045e-01,\n           5.0950e-02, 6.7143e-02],\n          [0.0000e+00, 1.9992e-03, 1.8058e-03,  ..., 1.2342e-03,\n           2.6284e-03, 1.0666e+00],\n          [5.3993e-02, 2.8959e-04, 0.0000e+00,  ..., 2.2887e-03,\n           2.0499e-03, 1.0055e+00],\n          ...,\n          [2.4862e-01, 1.2249e-03, 1.4645e-03,  ..., 3.9395e-02,\n           3.6379e-02, 6.9242e-01],\n          [6.3212e-01, 1.1062e-03, 1.2765e-03,  ..., 3.2115e-02,\n           3.4523e-02, 3.4905e-01],\n          [1.8557e-02, 3.7913e-04, 0.0000e+00,  ..., 0.0000e+00,\n           3.3142e-03, 1.0749e+00]],\n\n         [[1.8568e-01, 1.0151e-03, 7.8672e-03,  ..., 2.1558e-01,\n           1.8748e-01, 3.9161e-02],\n          [0.0000e+00, 6.1118e-03, 1.3225e-04,  ..., 1.5237e-03,\n           5.9668e-03, 1.0634e+00],\n          [2.6995e-02, 4.6573e-04, 5.2906e-02,  ..., 0.0000e+00,\n           3.7110e-04, 8.7312e-01],\n          ...,\n          [9.9330e-02, 0.0000e+00, 9.7120e-05,  ..., 1.8255e-01,\n           0.0000e+00, 3.9008e-01],\n          [0.0000e+00, 5.0315e-03, 0.0000e+00,  ..., 1.3584e-01,\n           0.0000e+00, 5.7049e-01],\n          [8.7106e-03, 1.4069e-03, 0.0000e+00,  ..., 1.8539e-03,\n           1.2263e-03, 0.0000e+00]]]], device='cuda:0'))\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}