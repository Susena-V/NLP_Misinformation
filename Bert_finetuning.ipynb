{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10789676,"sourceType":"datasetVersion","datasetId":6695691}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom transformers import BertTokenizer, BertModel\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"820a0c9c-9169-4aff-a454-26eb58d5ecd8","_cell_guid":"4556d261-d0b0-4b8a-a180-ed6bea44ba55","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T16:35:30.755563Z","iopub.execute_input":"2025-03-26T16:35:30.755766Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/filtered-and-translated-nlp/filr.csv')","metadata":{"_uuid":"b12b1693-96de-438f-b5bd-0be80408e2de","_cell_guid":"8aea64a9-6ad6-43c8-833b-ab922fafa30c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = data['Translated']\ny = data['Label']","metadata":{"_uuid":"f324c517-6ce5-4951-9dca-a423158612f8","_cell_guid":"bc70eafa-cd6d-4fb7-be0c-4482f90eebb2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"X","metadata":{"_uuid":"f7ac0f97-3ada-4ef2-bbfb-c2fab978d7b4","_cell_guid":"c03d2dcb-94e4-4f34-a030-670e062b5b26","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T13:19:53.646209Z","iopub.execute_input":"2025-03-26T13:19:53.646416Z","iopub.status.idle":"2025-03-26T13:19:53.667971Z","shell.execute_reply.started":"2025-03-26T13:19:53.646398Z","shell.execute_reply":"2025-03-26T13:19:53.667020Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"0      After hacking state TV by replacing propaganda...\n1      #flowers #lovers\\nMake love not war.\\nMarc Cha...\n2      If only we all showed more love and understand...\n3      Who are the soldiers we see in the videos? Are...\n4      I didn't think #Salvini could make his positio...\n                             ...                        \n475    If I write that Ms. #Zelensky was allegedly sp...\n476    #Zelensky and his wife #OlenaZelenska bought a...\n477    ALL UNITED AGAINST DRAGONS\\nAGAINST WAR\\nAGAIN...\n478    ALL UNITED AGAINST DRAGONS\\nAGAINST WAR\\nAGAIN...\n479    ALL UNITED AGAINST DRAGONS\\nAGAINST WAR\\nAGAIN...\nName: Translated, Length: 480, dtype: object"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"y","metadata":{"_uuid":"fd473d00-ea62-4e6e-97dd-e5be0e554983","_cell_guid":"7dc367df-0e09-49ba-a522-0c957629e44e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T13:19:53.668905Z","iopub.execute_input":"2025-03-26T13:19:53.669217Z","iopub.status.idle":"2025-03-26T13:19:53.686321Z","shell.execute_reply.started":"2025-03-26T13:19:53.669186Z","shell.execute_reply":"2025-03-26T13:19:53.685506Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"0      1\n1      2\n2      2\n3      2\n4      1\n      ..\n475    1\n476    0\n477    2\n478    2\n479    2\nName: Label, Length: 480, dtype: int64"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')","metadata":{"_uuid":"c172be2d-0f81-4396-82ec-d3bb6c89ec92","_cell_guid":"ce50c640-2218-4b16-8d4e-99ab22e3bf5d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T13:19:53.688123Z","iopub.execute_input":"2025-03-26T13:19:53.688308Z","iopub.status.idle":"2025-03-26T13:19:57.076997Z","shell.execute_reply.started":"2025-03-26T13:19:53.688292Z","shell.execute_reply":"2025-03-26T13:19:57.076108Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2d5a99d755b418c89b7b839a57b2496"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13b4b91806974c78bdc6c56388a4cf69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bf8d79fd7544ed4b768fa52b10eb86b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0d9d400ace7406b803f3cf642f1e84a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"829d11fbb179419782a1ab7ad7f0e6be"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"def generate_embeddings(texts):\n    model.eval()\n    embeddings = []\n    with torch.no_grad():\n        for text in texts:\n            inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n            outputs = model(**inputs)\n            pooled_output = outputs.pooler_output.squeeze().numpy()\n            embeddings.append(pooled_output)\n    return np.array(embeddings)\n\n# Generate embeddings\nprint(\"Generating BERT embeddings...\")\nX_embeddings = generate_embeddings(X)","metadata":{"_uuid":"d89783d0-215b-4acb-b854-991b4bef4f25","_cell_guid":"c9f614b1-157e-4b5c-a6f6-d0f563de464c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T13:19:57.078214Z","iopub.execute_input":"2025-03-26T13:19:57.078536Z","iopub.status.idle":"2025-03-26T13:20:45.705011Z","shell.execute_reply.started":"2025-03-26T13:19:57.078506Z","shell.execute_reply":"2025-03-26T13:20:45.703896Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Generating BERT embeddings...\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X_embeddings, y, test_size=0.2, random_state=42)\n\n# Train an SVM classifier\nprint(\"Training SVM...\")\nclf = SVC(kernel='linear')\nclf.fit(X_train, y_train)\n\n# Predict and evaluate\ny_pred = clf.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy * 100:.2f}%\")","metadata":{"_uuid":"3ce84c7e-9bfa-4cf4-809d-91e668faffc0","_cell_guid":"21686bc9-715c-4054-827b-915e792dfee3","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T13:20:45.706074Z","iopub.execute_input":"2025-03-26T13:20:45.706483Z","iopub.status.idle":"2025-03-26T13:20:45.837889Z","shell.execute_reply.started":"2025-03-26T13:20:45.706445Z","shell.execute_reply":"2025-03-26T13:20:45.836849Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Training SVM...\nAccuracy: 41.67%\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"_uuid":"a275a7b8-2e9d-4393-9dc0-5ece45107d95","_cell_guid":"e9a07a01-fa9f-43fd-a3b0-81e73e5b41dc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Follow a repo \nFor finetuning","metadata":{"_uuid":"747f90a3-1ba4-4579-97b8-c0217ef96b83","_cell_guid":"3dcc202b-2f28-46b1-9ac8-8ce25bebecbb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"!pip install transformers","metadata":{"_uuid":"c8faea0e-f580-4287-9d64-039cfb8bc4c8","_cell_guid":"26863294-31a6-4121-a618-186dd0b9aa48","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T02:50:32.387609Z","iopub.execute_input":"2025-03-26T02:50:32.387906Z","iopub.status.idle":"2025-03-26T02:50:38.027333Z","shell.execute_reply.started":"2025-03-26T02:50:32.387879Z","shell.execute_reply":"2025-03-26T02:50:38.026133Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.12.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nimport transformers\nfrom transformers import AutoModel, BertTokenizerFast\n\n# specify GPU\ndevice = torch.device(\"cuda\")","metadata":{"_uuid":"3a86378d-0af3-4900-bf8b-50bf7745eca4","_cell_guid":"243292f3-0838-4162-bc86-dc96bffff454","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T02:50:38.028669Z","iopub.execute_input":"2025-03-26T02:50:38.029035Z","iopub.status.idle":"2025-03-26T02:50:38.062060Z","shell.execute_reply.started":"2025-03-26T02:50:38.028997Z","shell.execute_reply":"2025-03-26T02:50:38.061468Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/filtered-and-translated-nlp/filr.csv')","metadata":{"_uuid":"b01a6be1-06de-4075-96f2-74e1410308e3","_cell_guid":"0e477476-d020-46ed-bc4f-08325cc77a81","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T02:50:38.062842Z","iopub.execute_input":"2025-03-26T02:50:38.063059Z","iopub.status.idle":"2025-03-26T02:50:38.096223Z","shell.execute_reply.started":"2025-03-26T02:50:38.063039Z","shell.execute_reply":"2025-03-26T02:50:38.095161Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"X = data['Translated']\ny = data['Label']","metadata":{"_uuid":"4f417171-73cd-44b9-b235-1728ce9deda0","_cell_guid":"146c98c0-99f1-4386-8217-03c6aa60587e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T02:50:38.097180Z","iopub.execute_input":"2025-03-26T02:50:38.097499Z","iopub.status.idle":"2025-03-26T02:50:38.101177Z","shell.execute_reply.started":"2025-03-26T02:50:38.097469Z","shell.execute_reply":"2025-03-26T02:50:38.100534Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"y.value_counts(normalize = True)","metadata":{"_uuid":"b084b2e3-645a-4785-91fd-0669c3830dae","_cell_guid":"96863ecc-335a-4d1e-b53a-f0d3ca9998bd","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T02:50:38.102062Z","iopub.execute_input":"2025-03-26T02:50:38.102294Z","iopub.status.idle":"2025-03-26T02:50:38.132968Z","shell.execute_reply.started":"2025-03-26T02:50:38.102274Z","shell.execute_reply":"2025-03-26T02:50:38.132338Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"Label\n2    0.431250\n1    0.235417\n3    0.172917\n0    0.160417\nName: proportion, dtype: float64"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"train_text, temp_text, train_labels, temp_labels = train_test_split(X, y, \n                                                                    random_state=2018, \n                                                                    test_size=0.3, \n                                                                    stratify=y)\n\n# we will use temp_text and temp_labels to create validation and test set\nval_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n                                                                random_state=2018, \n                                                                test_size=0.5, \n                                                                stratify=temp_labels)","metadata":{"_uuid":"70d68f67-0c46-49be-946c-e157623a89f8","_cell_guid":"ed7a0d9d-697c-477a-a5a0-68eed7a7b5ea","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T02:50:38.133877Z","iopub.execute_input":"2025-03-26T02:50:38.134145Z","iopub.status.idle":"2025-03-26T02:50:38.159054Z","shell.execute_reply.started":"2025-03-26T02:50:38.134117Z","shell.execute_reply":"2025-03-26T02:50:38.158375Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"bert = AutoModel.from_pretrained('bert-base-uncased')\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')","metadata":{"_uuid":"6f71cb1e-bbef-4dc6-bff7-f5dc6421f0a6","_cell_guid":"fab5b8a2-0757-41e2-ad42-168d324074b8","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T02:50:38.160014Z","iopub.execute_input":"2025-03-26T02:50:38.160301Z","iopub.status.idle":"2025-03-26T02:50:39.180125Z","shell.execute_reply.started":"2025-03-26T02:50:38.160272Z","shell.execute_reply":"2025-03-26T02:50:39.179095Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# sample data\ntext = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\n\n# encode text\nsent_id = tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False)","metadata":{"_uuid":"9102ce92-0c4d-4112-bd84-bf978c41fc3a","_cell_guid":"5f73af2d-5b96-4334-821e-638759dbc5f9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T02:50:39.181131Z","iopub.execute_input":"2025-03-26T02:50:39.181507Z","iopub.status.idle":"2025-03-26T02:50:39.194673Z","shell.execute_reply.started":"2025-03-26T02:50:39.181472Z","shell.execute_reply":"2025-03-26T02:50:39.193809Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"print(sent_id)","metadata":{"_uuid":"74e693e5-1f6e-4c4b-b685-d2da776dade9","_cell_guid":"bd5ffb2f-071f-42b2-940e-c01d96994ea5","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T02:50:39.195692Z","iopub.execute_input":"2025-03-26T02:50:39.195944Z","iopub.status.idle":"2025-03-26T02:50:39.215019Z","shell.execute_reply.started":"2025-03-26T02:50:39.195921Z","shell.execute_reply":"2025-03-26T02:50:39.214026Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"{'input_ids': [[101, 2023, 2003, 1037, 14324, 2944, 14924, 4818, 102, 0], [101, 2057, 2097, 2986, 1011, 8694, 1037, 14324, 2944, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"_uuid":"4d31201f-22bb-4bd2-8d14-db9fc13393ff","_cell_guid":"ea5e5d9f-8a5e-4720-bec4-df6d51e1afc4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tokenization","metadata":{"_uuid":"b91d4331-f26f-407c-ad2b-350fdbd1dffd","_cell_guid":"da966a1d-9f21-4161-b929-723f78b01ffb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"seq_len = [len(i.split()) for i in train_text]\n\npd.Series(seq_len).hist(bins = 30)","metadata":{"_uuid":"7a47ed2d-59cd-4688-9fe6-4a97c26483b4","_cell_guid":"88bbc7f3-13ae-477d-ac41-e8b4374bfb21","trusted":true,"collapsed":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:39.216125Z","iopub.execute_input":"2025-03-26T02:50:39.216476Z","iopub.status.idle":"2025-03-26T02:50:39.601109Z","shell.execute_reply.started":"2025-03-26T02:50:39.216437Z","shell.execute_reply":"2025-03-26T02:50:39.600386Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"<Axes: >"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"max_seq_len = max(seq_len)","metadata":{"_uuid":"1d5c036a-5119-4c70-843f-39d1f4180a84","_cell_guid":"3ef2a485-2291-4258-b6c1-97e7ded974b9","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:39.601878Z","iopub.execute_input":"2025-03-26T02:50:39.602142Z","iopub.status.idle":"2025-03-26T02:50:39.606033Z","shell.execute_reply.started":"2025-03-26T02:50:39.602119Z","shell.execute_reply":"2025-03-26T02:50:39.605081Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"max_seq_len","metadata":{"_uuid":"31fa9e1b-46ea-4748-a534-41614c67a1e2","_cell_guid":"1e36a491-8104-42ad-9477-2c4e337322b6","trusted":true,"collapsed":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:39.610061Z","iopub.execute_input":"2025-03-26T02:50:39.610291Z","iopub.status.idle":"2025-03-26T02:50:39.627931Z","shell.execute_reply.started":"2025-03-26T02:50:39.610270Z","shell.execute_reply":"2025-03-26T02:50:39.627267Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"58"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"# tokenize and encode sequences in the training set\ntokens_train = tokenizer.batch_encode_plus(\n    train_text.tolist(),\n    max_length = max_seq_len,\n    pad_to_max_length=True,\n    truncation=True,\n    return_token_type_ids=False\n)\n\n# tokenize and encode sequences in the validation set\ntokens_val = tokenizer.batch_encode_plus(\n    val_text.tolist(),\n    max_length = max_seq_len,\n    pad_to_max_length=True,\n    truncation=True,\n    return_token_type_ids=False\n)\n\n# tokenize and encode sequences in the test set\ntokens_test = tokenizer.batch_encode_plus(\n    test_text.tolist(),\n    max_length = max_seq_len,\n    pad_to_max_length=True,\n    truncation=True,\n    return_token_type_ids=False\n)","metadata":{"_uuid":"1150ca83-91fd-4a36-af81-d8c45e7f5b07","_cell_guid":"cd5d472c-30d7-4f91-acd2-2ef48907ae00","trusted":true,"collapsed":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:39.629689Z","iopub.execute_input":"2025-03-26T02:50:39.629883Z","iopub.status.idle":"2025-03-26T02:50:39.689739Z","shell.execute_reply.started":"2025-03-26T02:50:39.629866Z","shell.execute_reply":"2025-03-26T02:50:39.688789Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2673: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"\n# for train set\ntrain_seq = torch.tensor(tokens_train['input_ids'])\ntrain_mask = torch.tensor(tokens_train['attention_mask'])\ntrain_y = torch.tensor(train_labels.tolist())\n\n# for validation set\nval_seq = torch.tensor(tokens_val['input_ids'])\nval_mask = torch.tensor(tokens_val['attention_mask'])\nval_y = torch.tensor(val_labels.tolist())\n\n# for test set\ntest_seq = torch.tensor(tokens_test['input_ids'])\ntest_mask = torch.tensor(tokens_test['attention_mask'])\ntest_y = torch.tensor(test_labels.tolist())","metadata":{"_uuid":"ff701258-6d64-4002-bad9-415b147ef0b9","_cell_guid":"31c9a293-a1dd-44b4-89b3-0484f379e058","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:39.690292Z","iopub.execute_input":"2025-03-26T02:50:39.690534Z","iopub.status.idle":"2025-03-26T02:50:39.704164Z","shell.execute_reply.started":"2025-03-26T02:50:39.690515Z","shell.execute_reply":"2025-03-26T02:50:39.703323Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n#define a batch size\nbatch_size = 32\n\n# wrap tensors\ntrain_data = TensorDataset(train_seq, train_mask, train_y)\n\n# sampler for sampling the data during training\ntrain_sampler = RandomSampler(train_data)\n\n# dataLoader for train set\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# wrap tensors\nval_data = TensorDataset(val_seq, val_mask, val_y)\n\n# sampler for sampling the data during training\nval_sampler = SequentialSampler(val_data)\n\n# dataLoader for validation set\nval_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)","metadata":{"_uuid":"d7a932f3-26c8-4528-aafd-4f1f54365554","_cell_guid":"ebc4e20b-2b60-4d44-998b-96f7d1cdcd40","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:39.705091Z","iopub.execute_input":"2025-03-26T02:50:39.705393Z","iopub.status.idle":"2025-03-26T02:50:39.730961Z","shell.execute_reply.started":"2025-03-26T02:50:39.705367Z","shell.execute_reply":"2025-03-26T02:50:39.730094Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"\n\n# freeze all the parameters\nfor param in bert.parameters():\n    param.requires_grad = False","metadata":{"_uuid":"dbe27428-dd88-4121-a104-76e4ec16070a","_cell_guid":"87712a31-e7fb-4a18-991c-23e6529cf232","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:39.731796Z","iopub.execute_input":"2025-03-26T02:50:39.732114Z","iopub.status.idle":"2025-03-26T02:50:39.750541Z","shell.execute_reply.started":"2025-03-26T02:50:39.732091Z","shell.execute_reply":"2025-03-26T02:50:39.749727Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"class BERT_Arch(nn.Module):\n\n    def __init__(self, bert):\n      \n      super(BERT_Arch, self).__init__()\n\n      self.bert = bert \n      \n      # dropout layer\n      self.dropout = nn.Dropout(0.1)\n      \n      # relu activation function\n      self.relu =  nn.ReLU()\n\n      # dense layer 1\n      self.fc1 = nn.Linear(768,512)\n      \n      # dense layer 2 (Output layer)\n      self.fc2 = nn.Linear(512,4)\n\n      #softmax activation function\n      self.softmax = nn.LogSoftmax(dim=1)\n\n    #define the forward pass\n    def forward(self, sent_id, mask):\n    # Pass the inputs to the model\n        outputs = self.bert(sent_id, attention_mask=mask)\n        cls_hs = outputs.pooler_output\n        \n        # Pass through fully connected layers\n        x = self.fc1(cls_hs)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.softmax(x)\n    \n        return x","metadata":{"_uuid":"572f1a89-03f0-4377-a7b4-3f3e8b42be5d","_cell_guid":"f7a49a59-88c9-41fd-84fb-9a3977f402c5","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:39.751272Z","iopub.execute_input":"2025-03-26T02:50:39.751524Z","iopub.status.idle":"2025-03-26T02:50:39.772998Z","shell.execute_reply.started":"2025-03-26T02:50:39.751505Z","shell.execute_reply":"2025-03-26T02:50:39.772387Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"print(device)","metadata":{"_uuid":"678ef220-2a2e-4cf2-ba29-5681397e3432","_cell_guid":"463de2d8-2351-468d-b853-010208da3b34","trusted":true,"collapsed":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:39.773633Z","iopub.execute_input":"2025-03-26T02:50:39.773874Z","iopub.status.idle":"2025-03-26T02:50:39.801485Z","shell.execute_reply.started":"2025-03-26T02:50:39.773855Z","shell.execute_reply":"2025-03-26T02:50:39.800701Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# pass the pre-trained BERT to our define architecture\nmodel = BERT_Arch(bert)\n\n# push the model to GPU\nmodel = model.to(device)","metadata":{"_uuid":"ada4338a-cead-42a3-8561-b127e0ab9f86","_cell_guid":"9ab44a81-852e-4c6f-951c-cb3a0f061307","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:39.802221Z","iopub.execute_input":"2025-03-26T02:50:39.802473Z","iopub.status.idle":"2025-03-26T02:50:40.382539Z","shell.execute_reply.started":"2025-03-26T02:50:39.802448Z","shell.execute_reply":"2025-03-26T02:50:40.381586Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"\n# optimizer from hugging face transformers\nfrom transformers import AdamW\n\n# define the optimizer\noptimizer = AdamW(model.parameters(), lr = 1e-3)","metadata":{"_uuid":"7ad419b7-9bd7-4b53-9f4c-c0330a3dbc6c","_cell_guid":"4f27c952-e7fa-41b5-ac5d-3f06ed84847c","trusted":true,"collapsed":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:40.385693Z","iopub.execute_input":"2025-03-26T02:50:40.385904Z","iopub.status.idle":"2025-03-26T02:50:40.419028Z","shell.execute_reply.started":"2025-03-26T02:50:40.385885Z","shell.execute_reply":"2025-03-26T02:50:40.418384Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"\nfrom sklearn.utils.class_weight import compute_class_weight\n\n#compute the class weights\nclass_wts = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n\nprint(class_wts)","metadata":{"_uuid":"2c481e8a-0b84-4b9e-9076-da4c3c5b968e","_cell_guid":"b90934cb-f593-42ad-85a9-dafc6acde987","trusted":true,"collapsed":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:40.419797Z","iopub.execute_input":"2025-03-26T02:50:40.420046Z","iopub.status.idle":"2025-03-26T02:50:40.426465Z","shell.execute_reply.started":"2025-03-26T02:50:40.420014Z","shell.execute_reply":"2025-03-26T02:50:40.425542Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"[1.55555556 1.06329114 0.57931034 1.44827586]\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"\n# convert class weights to tensor\nweights= torch.tensor(class_wts,dtype=torch.float)\nweights = weights.to(device)\n\n# loss function\ncross_entropy  = nn.NLLLoss(weight=weights) \n\n# number of training epochs\nepochs = 10","metadata":{"_uuid":"962890b1-e917-4763-aaaa-3917bd924423","_cell_guid":"4d62881d-dca7-4c04-b4a6-e07c6ad39ead","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:40.427242Z","iopub.execute_input":"2025-03-26T02:50:40.427547Z","iopub.status.idle":"2025-03-26T02:50:40.451741Z","shell.execute_reply.started":"2025-03-26T02:50:40.427525Z","shell.execute_reply":"2025-03-26T02:50:40.450925Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# function to train the model\ndef train():\n  \n  model.train()\n\n  total_loss, total_accuracy = 0, 0\n  \n  # empty list to save model predictions\n  total_preds=[]\n  \n  # iterate over batches\n  for step,batch in enumerate(train_dataloader):\n    \n    # progress update after every 50 batches.\n    if step % 50 == 0 and not step == 0:\n      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n\n    # push the batch to gpu\n    batch = [r.to(device) for r in batch]\n \n    sent_id, mask, labels = batch\n\n    # clear previously calculated gradients \n    model.zero_grad()        \n\n    # get model predictions for the current batch\n    preds = model(sent_id, mask)\n\n    # compute the loss between actual and predicted values\n    loss = cross_entropy(preds, labels)\n\n    # add on to the total loss\n    total_loss = total_loss + loss.item()\n\n    # backward pass to calculate the gradients\n    loss.backward()\n\n    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n    # update parameters\n    optimizer.step()\n\n    # model predictions are stored on GPU. So, push it to CPU\n    preds=preds.detach().cpu().numpy()\n\n    # append the model predictions\n    total_preds.append(preds)\n\n  # compute the training loss of the epoch\n  avg_loss = total_loss / len(train_dataloader)\n  \n  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n  # reshape the predictions in form of (number of samples, no. of classes)\n  total_preds  = np.concatenate(total_preds, axis=0)\n\n  #returns the loss and predictions\n  return avg_loss, total_preds","metadata":{"_uuid":"e93dc8bf-76a0-4a4a-b1b1-21114e4b563d","_cell_guid":"8a0cb4f2-9db5-4055-a1cd-c889414405e8","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:40.452735Z","iopub.execute_input":"2025-03-26T02:50:40.453036Z","iopub.status.idle":"2025-03-26T02:50:40.468984Z","shell.execute_reply.started":"2025-03-26T02:50:40.453006Z","shell.execute_reply":"2025-03-26T02:50:40.468387Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# function for evaluating the model\ndef evaluate():\n  \n  print(\"\\nEvaluating...\")\n  \n  # deactivate dropout layers\n  model.eval()\n\n  total_loss, total_accuracy = 0, 0\n  \n  # empty list to save the model predictions\n  total_preds = []\n\n  # iterate over batches\n  for step,batch in enumerate(val_dataloader):\n    \n    # Progress update every 50 batches.\n    if step % 50 == 0 and not step == 0:\n      \n      # Calculate elapsed time in minutes.\n      elapsed = format_time(time.time() - t0)\n            \n      # Report progress.\n      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n\n    # push the batch to gpu\n    batch = [t.to(device) for t in batch]\n\n    sent_id, mask, labels = batch\n\n    # deactivate autograd\n    with torch.no_grad():\n      \n      # model predictions\n      preds = model(sent_id, mask)\n\n      # compute the validation loss between actual and predicted values\n      loss = cross_entropy(preds,labels)\n\n      total_loss = total_loss + loss.item()\n\n      preds = preds.detach().cpu().numpy()\n\n      total_preds.append(preds)\n\n  # compute the validation loss of the epoch\n  avg_loss = total_loss / len(val_dataloader) \n\n  # reshape the predictions in form of (number of samples, no. of classes)\n  total_preds  = np.concatenate(total_preds, axis=0)\n\n  return avg_loss, total_preds","metadata":{"_uuid":"fa7facde-ed9b-4840-9aff-8f9f5fe1eefe","_cell_guid":"d3903f69-6be0-4fda-8a5d-9a30e25e33e7","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:40.469658Z","iopub.execute_input":"2025-03-26T02:50:40.469896Z","iopub.status.idle":"2025-03-26T02:50:40.496353Z","shell.execute_reply.started":"2025-03-26T02:50:40.469876Z","shell.execute_reply":"2025-03-26T02:50:40.495680Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"\n# set initial loss to infinite\nbest_valid_loss = float('inf')\n\n# empty lists to store training and validation loss of each epoch\ntrain_losses=[]\nvalid_losses=[]\n\n#for each epoch\nfor epoch in range(epochs):\n     \n    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n    \n    #train model\n    train_loss, _ = train()\n    \n    #evaluate model\n    valid_loss, _ = evaluate()\n    \n    #save the best model\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'saved_weights.pt')\n    \n    # append training and validation loss\n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n    \n    print(f'\\nTraining Loss: {train_loss:.3f}')\n    print(f'Validation Loss: {valid_loss:.3f}')","metadata":{"_uuid":"d1d0ba8c-fafb-40b2-a022-a8ed323828b3","_cell_guid":"d997cba5-a241-444d-a038-b6b3bb7a21ac","trusted":true,"collapsed":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:40.497235Z","iopub.execute_input":"2025-03-26T02:50:40.497554Z","iopub.status.idle":"2025-03-26T02:50:56.604411Z","shell.execute_reply.started":"2025-03-26T02:50:40.497524Z","shell.execute_reply":"2025-03-26T02:50:56.603377Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"\n Epoch 1 / 10\n\nEvaluating...\n\nTraining Loss: 1.582\nValidation Loss: 1.349\n\n Epoch 2 / 10\n\nEvaluating...\n\nTraining Loss: 1.437\nValidation Loss: 1.392\n\n Epoch 3 / 10\n\nEvaluating...\n\nTraining Loss: 1.372\nValidation Loss: 1.420\n\n Epoch 4 / 10\n\nEvaluating...\n\nTraining Loss: 1.391\nValidation Loss: 1.344\n\n Epoch 5 / 10\n\nEvaluating...\n\nTraining Loss: 1.418\nValidation Loss: 1.371\n\n Epoch 6 / 10\n\nEvaluating...\n\nTraining Loss: 1.376\nValidation Loss: 1.349\n\n Epoch 7 / 10\n\nEvaluating...\n\nTraining Loss: 1.326\nValidation Loss: 1.327\n\n Epoch 8 / 10\n\nEvaluating...\n\nTraining Loss: 1.310\nValidation Loss: 1.373\n\n Epoch 9 / 10\n\nEvaluating...\n\nTraining Loss: 1.376\nValidation Loss: 1.383\n\n Epoch 10 / 10\n\nEvaluating...\n\nTraining Loss: 1.345\nValidation Loss: 1.368\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"\n\n# get predictions for test data\nwith torch.no_grad():\n  preds = model(test_seq.to(device), test_mask.to(device))\n  preds = preds.detach().cpu().numpy()\n     \n\n# model's performance\npreds = np.argmax(preds, axis = 1)\nprint(classification_report(test_y, preds))","metadata":{"_uuid":"399cba0d-5899-491a-85a3-a3f987960f3e","_cell_guid":"c6e87c55-0574-46fc-afba-247f319cb470","trusted":true,"collapsed":true,"jupyter":{"source_hidden":true,"outputs_hidden":true},"execution":{"iopub.status.busy":"2025-03-26T02:50:56.605532Z","iopub.execute_input":"2025-03-26T02:50:56.605873Z","iopub.status.idle":"2025-03-26T02:50:56.820150Z","shell.execute_reply.started":"2025-03-26T02:50:56.605843Z","shell.execute_reply":"2025-03-26T02:50:56.819254Z"}},"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.26      0.75      0.38        12\n           1       0.00      0.00      0.00        17\n           2       0.50      0.45      0.47        31\n           3       0.44      0.33      0.38        12\n\n    accuracy                           0.38        72\n   macro avg       0.30      0.38      0.31        72\nweighted avg       0.33      0.38      0.33        72\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ChatGPT","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer, BertModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Custom Dataset\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len=512):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, index):\n        text = self.texts.iloc[index]  # Using .iloc for safe indexing\n        label = self.labels.iloc[index]\n        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt')\n\n        return {\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n\n\n# Define Model\nclass BERTClassifier(nn.Module):\n    def __init__(self, num_classes=4):\n        super(BERTClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.fc = nn.Linear(768, num_classes)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        logits = self.fc(outputs.pooler_output)\n        return logits\n\n# Prepare Data\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\ntrain_dataset = TextDataset(X_train, y_train, tokenizer)\ntest_dataset = TextDataset(X_test, y_test, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n\n# Initialize Model and Parameters\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = BERTClassifier(num_classes=4).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=2e-5)\n\n# Training Loop\ndef train_model(epochs=3):\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        for batch in train_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n\n            optimizer.zero_grad()\n            outputs = model(input_ids, attention_mask)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}\")\n\n# Evaluation\ndef evaluate_model():\n    model.eval()\n    all_preds = []\n    all_labels = []\n    with torch.no_grad():\n        for batch in test_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n\n            outputs = model(input_ids, attention_mask)\n            predictions = torch.argmax(outputs, dim=1)\n            all_preds.extend(predictions.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n\n# Run Training and Evaluation\ntrain_model(25)\nevaluate_model()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T13:58:40.699341Z","iopub.execute_input":"2025-03-26T13:58:40.699685Z","iopub.status.idle":"2025-03-26T14:14:52.772456Z","shell.execute_reply.started":"2025-03-26T13:58:40.699656Z","shell.execute_reply":"2025-03-26T14:14:52.771550Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 1.3202871928612392\nEpoch 2, Loss: 1.1803770437836647\nEpoch 3, Loss: 1.0171967794497807\nEpoch 4, Loss: 0.776843490699927\nEpoch 5, Loss: 0.5988089914123217\nEpoch 6, Loss: 0.5111452552179495\nEpoch 7, Loss: 0.3892643637955189\nEpoch 8, Loss: 0.2932945806533098\nEpoch 9, Loss: 0.22148984453330436\nEpoch 10, Loss: 0.14605277481799325\nEpoch 11, Loss: 0.11839799738178651\nEpoch 12, Loss: 0.08262243463347356\nEpoch 13, Loss: 0.06385140710820754\nEpoch 14, Loss: 0.051120028753454484\nEpoch 15, Loss: 0.05385881934004525\nEpoch 16, Loss: 0.04409429019627472\nEpoch 17, Loss: 0.032616524530264236\nEpoch 18, Loss: 0.02854304830543697\nEpoch 19, Loss: 0.03043265885207802\nEpoch 20, Loss: 0.025352997045653563\nEpoch 21, Loss: 0.025301651699313272\nEpoch 22, Loss: 0.02350107211774836\nEpoch 23, Loss: 0.018318536012278248\nEpoch 24, Loss: 0.02259548307241251\nEpoch 25, Loss: 0.02261593899068733\nTest Accuracy: 46.88%\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import os\n\ndef save_model(model, path='/kaggle/working/model_state_dict.pt'):\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n    torch.save(model.state_dict(), path)\n    print(f\"Model state dict saved to {path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:14:52.773883Z","iopub.execute_input":"2025-03-26T14:14:52.774267Z","iopub.status.idle":"2025-03-26T14:14:52.778361Z","shell.execute_reply.started":"2025-03-26T14:14:52.774232Z","shell.execute_reply":"2025-03-26T14:14:52.777437Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"save_model(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:14:52.779793Z","iopub.execute_input":"2025-03-26T14:14:52.779987Z","iopub.status.idle":"2025-03-26T14:14:53.884112Z","shell.execute_reply.started":"2025-03-26T14:14:52.779971Z","shell.execute_reply":"2025-03-26T14:14:53.883037Z"}},"outputs":[{"name":"stdout","text":"Model state dict saved to /kaggle/working/model_state_dict.pt\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"def save_tokenizer(tokenizer, path='/kaggle/working/tokenizer'):\n    os.makedirs(path, exist_ok=True)\n    tokenizer.save_pretrained(path)\n    print(f\"Tokenizer saved to {path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:14:53.885821Z","iopub.execute_input":"2025-03-26T14:14:53.886133Z","iopub.status.idle":"2025-03-26T14:14:53.890346Z","shell.execute_reply.started":"2025-03-26T14:14:53.886111Z","shell.execute_reply":"2025-03-26T14:14:53.889483Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"save_tokenizer(tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:14:53.891264Z","iopub.execute_input":"2025-03-26T14:14:53.891559Z","iopub.status.idle":"2025-03-26T14:14:53.930982Z","shell.execute_reply.started":"2025-03-26T14:14:53.891531Z","shell.execute_reply":"2025-03-26T14:14:53.930271Z"}},"outputs":[{"name":"stdout","text":"Tokenizer saved to /kaggle/working/tokenizer\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import numpy as np\n\ndef generate_embeddings(texts, tokenizer, model, batch_size=16, max_len=512):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    model.eval()\n\n    embeddings = []\n\n    with torch.no_grad():\n        for i in range(0, len(texts), batch_size):\n            batch_texts = texts[i:i+batch_size]\n\n            inputs = tokenizer(batch_texts, padding=True, truncation=True, max_length=max_len, return_tensors='pt')\n            input_ids = inputs['input_ids'].to(device)\n            attention_mask = inputs['attention_mask'].to(device)\n            \n            outputs = model.bert(input_ids=input_ids, attention_mask=attention_mask)\n\n            # If pooler_output is available, use it; otherwise, use mean of last hidden state\n            if hasattr(outputs, 'pooler_output'):\n                batch_embeddings = outputs.pooler_output.cpu().numpy()\n            else:\n                batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n\n            embeddings.extend(batch_embeddings)\n    \n    return np.array(embeddings)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:14:53.931878Z","iopub.execute_input":"2025-03-26T14:14:53.932198Z","iopub.status.idle":"2025-03-26T14:14:53.938597Z","shell.execute_reply.started":"2025-03-26T14:14:53.932169Z","shell.execute_reply":"2025-03-26T14:14:53.937503Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n\n# Assuming X_train, X_test, y_train, y_test are available\nX_train_embeddings = generate_embeddings(X_train.tolist(), tokenizer, model)\nX_test_embeddings = generate_embeddings(X_test.tolist(), tokenizer, model)\n\n# Train the SVM\nsvm_classifier = SVC(kernel='linear', class_weight = 'balanced')\nsvm_classifier.fit(X_train_embeddings, y_train)\n\n# Predict and Evaluate\ny_pred = svm_classifier.predict(X_test_embeddings)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(f\"Test Accuracy with SVM: {accuracy * 100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:14:53.939316Z","iopub.execute_input":"2025-03-26T14:14:53.939559Z","iopub.status.idle":"2025-03-26T14:14:56.645812Z","shell.execute_reply.started":"2025-03-26T14:14:53.939540Z","shell.execute_reply":"2025-03-26T14:14:56.644821Z"}},"outputs":[{"name":"stdout","text":"Test Accuracy with SVM: 43.75%\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve\n\n# Predict using SVM\ny_pred = svm_classifier.predict(X_test_embeddings)\n\n# Confusion Matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(8,6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()\n\n# Classification Report\nprint(\"Classification Report:\\n\")\nprint(classification_report(y_test, y_pred))\n\n# AUC-ROC Score\nif len(set(y_test)) <= 2:\n    auc_score = roc_auc_score(y_test, svm_classifier.decision_function(X_test_embeddings))\n    print(f\"AUC-ROC Score: {auc_score:.2f}\")\n\n    # Plot ROC Curve\n    fpr, tpr, _ = roc_curve(y_test, svm_classifier.decision_function(X_test_embeddings))\n    plt.figure(figsize=(8,6))\n    plt.plot(fpr, tpr, color='blue', label=f\"AUC = {auc_score:.2f}\")\n    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.title(\"ROC Curve\")\n    plt.legend(loc=\"lower right\")\n    plt.show()\nelse:\n    print(\"AUC-ROC is not applicable for multi-class classification.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:14:56.647690Z","iopub.execute_input":"2025-03-26T14:14:56.647951Z","iopub.status.idle":"2025-03-26T14:14:56.875687Z","shell.execute_reply.started":"2025-03-26T14:14:56.647929Z","shell.execute_reply":"2025-03-26T14:14:56.875051Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 2 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"Classification Report:\n\n              precision    recall  f1-score   support\n\n           0       0.50      0.06      0.11        16\n           1       0.28      0.35      0.31        20\n           2       0.50      0.67      0.57        43\n           3       0.45      0.29      0.36        17\n\n    accuracy                           0.44        96\n   macro avg       0.43      0.35      0.34        96\nweighted avg       0.45      0.44      0.40        96\n\nAUC-ROC is not applicable for multi-class classification.\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"print('hi')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:14:56.876469Z","iopub.execute_input":"2025-03-26T14:14:56.876713Z","iopub.status.idle":"2025-03-26T14:14:56.880659Z","shell.execute_reply.started":"2025-03-26T14:14:56.876688Z","shell.execute_reply":"2025-03-26T14:14:56.879957Z"}},"outputs":[{"name":"stdout","text":"hi\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"misclassified_indices = np.where(y_test != y_pred)[0]\nfor idx in misclassified_indices[:10]:  # Inspect first 10 misclassified samples\n    print(f\"Text: {X_test.iloc[idx]}\")\n    print(f\"True Label: {y_test.iloc[idx]}, Predicted Label: {y_pred[idx]}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:14:56.881523Z","iopub.execute_input":"2025-03-26T14:14:56.881857Z","iopub.status.idle":"2025-03-26T14:14:56.899878Z","shell.execute_reply.started":"2025-03-26T14:14:56.881835Z","shell.execute_reply":"2025-03-26T14:14:56.899236Z"}},"outputs":[{"name":"stdout","text":"Text: From symbol of peace to megaphone for war, #Putin rewrites history of #Luzniki stadium\nTrue Label: 3, Predicted Label: 2\n\nText: It's #11thApril! So what?\n\n* Day of war #47 in #Ukraine.\n* Austrian PM #Nehammer in Moscow from #Putin.\n* #Draghi to #Algiers for #gas deal.\n* National #sea day\n\nApril 11, 1961 Bob #Dylan's first performance.\nTrue Label: 2, Predicted Label: 3\n\nText: LA #GUERRA GIUSTA NON ESISTE!\nINVOCARE #PACE INVIANDO #ARMI NON È PER L'EROE CHE COMBATTE, MA PER IL POPOLO CHE VIENE MACELLATO!\nSIAMO UN PAESE DI COGLIONI INVASATI!\nSveglia!!!\n#StopTheWar\n#DraghiVatteneSubito\n#NonInMioNome\nTrue Label: 1, Predicted Label: 2\n\nText: This #April25 I will go to the parade.\nI had the honor of knowing that generation of #Liberals who took up arms to defend the Fatherland, and I know they would have no problem today recognizing the #partisans of that time in the Ukrainians and Ukrainians of today.\n \nTrue Label: 2, Predicted Label: 3\n\nText: But how nice,with a war going on,with bombs falling,with civilians being killed we also have time to have a laugh. All scene!!! Fuck you shit actors #putin #actors #bugs #UkraineRussiaWar #Ukraine #aveterottoilcaxxo\nTrue Label: 1, Predicted Label: 2\n\nText: How do voters of German parties answer the question, whether weapons should be supplied to Ukraine:\nInteresting - liberals and conservatives are more skeptical than Greens and Social Democrats.\nObvious: AfD is against it.\n\nThe progressive word is: freedom and life for Ukraine.\nTrue Label: 2, Predicted Label: 1\n\nText: Moscow does not rule out Putin-Zelensky meeting, but after understanding #Ukraine war #UkraineRussianWar #russia #putin #bucha #onu #Zelensky\nTrue Label: 3, Predicted Label: 2\n\nText: YOU POINT THE FINGER AT THOSE IN WAR WHO ACCIDENTALLY KILL A CHILD\n\nI POINT THE FINGER AT EUROPE AND YOU PARENTS WHO MAKE THEM RISK THEIR LIVES WITHOUT VALID MEDICAL REASONS\nTrue Label: 0, Predicted Label: 1\n\nText: \"The little girl with the candy\"-and the rifle!\nMore and more propaganda and less and less journalism.\nInstead of charades Ukrainians need free voices. A heartfelt appeal on my site:\n \n#RussiansAreOurBrothers #UkrainiansAreOurBrothers.\nTrue Label: 1, Predicted Label: 3\n\nText: Far away I will go; over sea and land, to say no to war | to those I will see. If there is blood to spill Spill only your own; I no longer follow you. And if you find me, with me I carry no weapons: courage, up, gendarmes, fire upon me\n\n🖊️Boris Vian\n🎨from the web\n\n#WarPeace\nTrue Label: 3, Predicted Label: 2\n\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"# Early Stopping","metadata":{}},{"cell_type":"code","source":"class BERTClassifierWithAttention(nn.Module):\n    def __init__(self, num_classes=4):\n        super(BERTClassifierWithAttention, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)\n        self.fc = nn.Linear(768, num_classes)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        logits = self.fc(outputs.pooler_output)\n        \n        # Extract attention weights (already normalized)\n        attentions = outputs.attentions  # Tuple of attention weights for each layer\n        \n        # Compute attention scores (unnormalized)\n        attention_scores = []\n        for layer in self.bert.encoder.layer:\n            attn = layer.attention.self\n            q = attn.query(input_ids)\n            k = attn.key(input_ids)\n            scores = torch.matmul(q, k.transpose(-2, -1)) / (q.size(-1) ** 0.5)\n            attention_scores.append(scores.detach())  # Store unnormalized scores\n        \n        return logits, attentions, attention_scores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:35:21.888874Z","iopub.execute_input":"2025-03-26T14:35:21.889189Z","iopub.status.idle":"2025-03-26T14:35:21.895265Z","shell.execute_reply.started":"2025-03-26T14:35:21.889166Z","shell.execute_reply":"2025-03-26T14:35:21.894446Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef plot_attention_scores(scores, title=\"Attention Scores\"):\n    # Normalize scores for visualization\n    scores = scores[0].detach().numpy()  # Shape: (num_heads, seq_len, seq_len)\n    \n    plt.figure(figsize=(10, 8))\n    sns.heatmap(scores[0], annot=True, fmt=\".2f\", cmap=\"viridis\")  # First head\n    plt.title(title)\n    plt.xlabel(\"Keys\")\n    plt.ylabel(\"Queries\")\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:35:31.939554Z","iopub.execute_input":"2025-03-26T14:35:31.939909Z","iopub.status.idle":"2025-03-26T14:35:31.944892Z","shell.execute_reply.started":"2025-03-26T14:35:31.939884Z","shell.execute_reply":"2025-03-26T14:35:31.943965Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Attention","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import BertModel, BertTokenizer\n\nclass BERTClassifierWithAttention(nn.Module):\n    def __init__(self, num_classes=4):\n        super(BERTClassifierWithAttention, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)\n        self.fc = nn.Linear(768, num_classes)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        logits = self.fc(outputs.pooler_output)\n        \n        # Extract attention weights (normalized scores)\n        attentions = outputs.attentions  # Tuple of attention weights for each layer\n        \n        # Extract Q, K, V matrices manually\n        qkv_values = []\n        for layer in self.bert.encoder.layer:\n            attn = layer.attention.self\n            q = attn.query(outputs.last_hidden_state)  # Query projection\n            k = attn.key(outputs.last_hidden_state)    # Key projection\n            v = attn.value(outputs.last_hidden_state)  # Value projection\n            qkv_values.append((q.detach(), k.detach(), v.detach()))\n        \n        return logits, attentions, qkv_values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T16:34:25.740527Z","iopub.execute_input":"2025-03-26T16:34:25.740854Z","iopub.status.idle":"2025-03-26T16:34:25.746858Z","shell.execute_reply.started":"2025-03-26T16:34:25.740824Z","shell.execute_reply":"2025-03-26T16:34:25.745869Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AdamW\n\n# Custom Dataset\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len=512):\n        self.texts = texts.tolist() if hasattr(texts, 'tolist') else texts\n        self.labels = labels.tolist() if hasattr(labels, 'tolist') else labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, index):\n        text = self.texts[index]\n        label = self.labels[index]\n        encoding = self.tokenizer(\n            text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_len,\n            return_tensors='pt'\n        )\n        return {\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n\n# Prepare Data\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\ntrain_dataset = TextDataset(X_train, y_train, tokenizer)\nval_dataset = TextDataset(X_val, y_val, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n\n# Initialize Model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = BERTClassifierWithAttention(num_classes=4).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = AdamW(model.parameters(), lr=2e-5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T16:34:44.080892Z","iopub.execute_input":"2025-03-26T16:34:44.081274Z","iopub.status.idle":"2025-03-26T16:34:44.718571Z","shell.execute_reply.started":"2025-03-26T16:34:44.081239Z","shell.execute_reply":"2025-03-26T16:34:44.717706Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# Training Loop\ndef train_model(epochs=3):\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        for batch in train_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n\n            optimizer.zero_grad()\n            logits, _, _ = model(input_ids, attention_mask)  # Ignore attentions and QKV during training\n            loss = criterion(logits, labels)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}\")\n\n# Run Training\ntrain_model(epochs=20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T16:34:44.844743Z","iopub.execute_input":"2025-03-26T16:34:44.845008Z","iopub.status.idle":"2025-03-26T16:34:46.635502Z","shell.execute_reply.started":"2025-03-26T16:34:44.844986Z","shell.execute_reply":"2025-03-26T16:34:46.634216Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-f6ea32fc55d0>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Run Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-19-f6ea32fc55d0>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Ignore attentions and QKV during training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-1537f501e2c2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Query projection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# Key projection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Value projection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 7783 has 14.74 GiB memory in use. Of the allocated memory 14.41 GiB is allocated by PyTorch, and 202.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 7783 has 14.74 GiB memory in use. Of the allocated memory 14.41 GiB is allocated by PyTorch, and 202.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":19},{"cell_type":"code","source":"def generate_embeddings(texts, tokenizer, model, batch_size=16, max_len=512):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    model.eval()\n\n    embeddings = []\n\n    with torch.no_grad():\n        for i in range(0, len(texts), batch_size):\n            batch_texts = texts[i:i+batch_size]\n            inputs = tokenizer(batch_texts, padding=True, truncation=True, max_length=max_len, return_tensors='pt')\n            input_ids = inputs['input_ids'].to(device)\n            attention_mask = inputs['attention_mask'].to(device)\n            \n            outputs = model.bert(input_ids=input_ids, attention_mask=attention_mask)\n            batch_embeddings = outputs.pooler_output.cpu().numpy()  # Use pooler_output\n            embeddings.extend(batch_embeddings)\n    \n    return embeddings\n\n# Generate embeddings for train and test sets\nX_train_embeddings = generate_embeddings(X_train.tolist(), tokenizer, model)\nX_val_embeddings = generate_embeddings(X_val.tolist(), tokenizer, model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T16:34:32.279666Z","iopub.execute_input":"2025-03-26T16:34:32.279979Z","iopub.status.idle":"2025-03-26T16:34:34.985531Z","shell.execute_reply.started":"2025-03-26T16:34:32.279955Z","shell.execute_reply":"2025-03-26T16:34:34.984492Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n\n# Train the SVM\nsvm_classifier = SVC(kernel='linear')\nsvm_classifier.fit(X_train_embeddings, y_train)\n\n# Predict and Evaluate\ny_pred = svm_classifier.predict(X_val_embeddings)\naccuracy = accuracy_score(y_val, y_pred)\n\nprint(f\"Validation Accuracy with SVM: {accuracy * 100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T16:34:35.063686Z","iopub.execute_input":"2025-03-26T16:34:35.063984Z","iopub.status.idle":"2025-03-26T16:34:35.143659Z","shell.execute_reply.started":"2025-03-26T16:34:35.063956Z","shell.execute_reply":"2025-03-26T16:34:35.142911Z"}},"outputs":[{"name":"stdout","text":"Validation Accuracy with SVM: 41.67%\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"def extract_attention_and_qkv(model, tokenizer, text):\n    encoding = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n    input_ids = encoding['input_ids'].to(device)\n    attention_mask = encoding['attention_mask'].to(device)\n\n    with torch.no_grad():\n        _, attentions, qkv_values = model(input_ids, attention_mask)\n    \n    return attentions, qkv_values\n\n# Example usage\ntext = \"\"\"\nLA #GUERRA GIUSTA NON ESISTE!\nINVOCARE #PACE INVIANDO #ARMI NON È PER L'EROE CHE COMBATTE, MA PER IL POPOLO CHE VIENE MACELLATO!\nSIAMO UN PAESE DI COGLIONI INVASATI!\nSveglia!!!\n#StopTheWar\n#DraghiVatteneSubito\n#NonInMioNome\n\"\"\"\nattentions, qkv_values = extract_attention_and_qkv(model, tokenizer, text)\n\n# Inspect Q, K, V values for the first layer\nq, k, v = qkv_values[0]\nprint(\"Query Matrix:\", q)\nprint(\"Key Matrix:\", k)\nprint(\"Value Matrix:\", v)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T16:34:06.632130Z","iopub.execute_input":"2025-03-26T16:34:06.632500Z","iopub.status.idle":"2025-03-26T16:34:06.663940Z","shell.execute_reply.started":"2025-03-26T16:34:06.632471Z","shell.execute_reply":"2025-03-26T16:34:06.663059Z"}},"outputs":[{"name":"stdout","text":"Query Matrix: tensor([[[-0.1241, -0.5980,  1.1845,  ..., -0.8275, -1.1383,  0.6859],\n         [ 0.5763,  0.4325,  0.1491,  ..., -0.7481,  0.3936, -0.3614],\n         [-0.2064, -0.2172,  0.6020,  ..., -0.2539,  1.0593,  0.4779],\n         ...,\n         [ 0.1392, -0.1917,  0.9082,  ..., -0.5871,  0.7609, -0.5840],\n         [ 0.1853,  0.3536,  0.6060,  ..., -1.9021,  1.4134, -0.0185],\n         [-1.3535,  0.3570,  0.4382,  ...,  0.7874, -1.7051,  0.9216]]],\n       device='cuda:0')\nKey Matrix: tensor([[[ 1.2305,  0.5396,  1.1245,  ..., -0.5718, -0.1862,  1.7110],\n         [ 0.1592, -0.3806, -0.7967,  ..., -0.3110, -0.4183, -0.5395],\n         [ 0.4690, -0.3307,  0.2995,  ..., -0.5576,  0.3642,  0.4907],\n         ...,\n         [-0.2848, -0.0308, -0.5729,  ...,  1.3623,  0.2671,  0.0990],\n         [ 1.2696, -0.9617, -0.2788,  ...,  0.5497, -0.4457,  1.4647],\n         [ 1.5125,  0.1453, -0.0771,  ...,  0.6673,  0.8039,  1.0630]]],\n       device='cuda:0')\nValue Matrix: tensor([[[ 0.2834, -0.7550,  1.1573,  ...,  0.3550, -0.0870,  0.9501],\n         [-0.0308, -1.1588,  0.8012,  ...,  0.7110,  0.2408, -0.1394],\n         [-1.1151, -0.3977,  0.1651,  ..., -0.1654, -0.4030,  0.1543],\n         ...,\n         [-0.3764, -0.9357, -0.0230,  ...,  0.2031, -0.1373, -0.4006],\n         [-0.1756, -0.2583,  0.2263,  ...,  1.0077, -0.1237,  0.8131],\n         [-0.3642, -1.1336,  1.5440,  ...,  0.1718,  0.3081,  0.7497]]],\n       device='cuda:0')\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}