{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10789676,"sourceType":"datasetVersion","datasetId":6695691}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom transformers import BertTokenizer, BertModel\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"820a0c9c-9169-4aff-a454-26eb58d5ecd8","_cell_guid":"4556d261-d0b0-4b8a-a180-ed6bea44ba55","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-25T16:25:34.525385Z","iopub.execute_input":"2025-03-25T16:25:34.525706Z","iopub.status.idle":"2025-03-25T16:25:34.530472Z","shell.execute_reply.started":"2025-03-25T16:25:34.525671Z","shell.execute_reply":"2025-03-25T16:25:34.529393Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":112},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/filtered-and-translated-nlp/filr.csv')","metadata":{"_uuid":"b12b1693-96de-438f-b5bd-0be80408e2de","_cell_guid":"8aea64a9-6ad6-43c8-833b-ab922fafa30c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-25T16:25:34.705278Z","iopub.execute_input":"2025-03-25T16:25:34.705607Z","iopub.status.idle":"2025-03-25T16:25:34.734188Z","shell.execute_reply.started":"2025-03-25T16:25:34.705575Z","shell.execute_reply":"2025-03-25T16:25:34.733176Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":113},{"cell_type":"code","source":"X = data['Translated']\ny = data['Label']","metadata":{"_uuid":"f324c517-6ce5-4951-9dca-a423158612f8","_cell_guid":"bc70eafa-cd6d-4fb7-be0c-4482f90eebb2","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-25T16:25:34.900349Z","iopub.execute_input":"2025-03-25T16:25:34.900666Z","iopub.status.idle":"2025-03-25T16:25:34.904791Z","shell.execute_reply.started":"2025-03-25T16:25:34.900637Z","shell.execute_reply":"2025-03-25T16:25:34.903856Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":114},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"X","metadata":{"_uuid":"f7ac0f97-3ada-4ef2-bbfb-c2fab978d7b4","_cell_guid":"c03d2dcb-94e4-4f34-a030-670e062b5b26","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-25T16:25:35.225069Z","iopub.execute_input":"2025-03-25T16:25:35.225338Z","iopub.status.idle":"2025-03-25T16:25:35.231310Z","shell.execute_reply.started":"2025-03-25T16:25:35.225316Z","shell.execute_reply":"2025-03-25T16:25:35.230470Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"execution_count":115,"output_type":"execute_result","data":{"text/plain":"0      After hacking state TV by replacing propaganda...\n1      #flowers #lovers\\nMake love not war.\\nMarc Cha...\n2      If only we all showed more love and understand...\n3      Who are the soldiers we see in the videos? Are...\n4      I didn't think #Salvini could make his positio...\n                             ...                        \n475    If I write that Ms. #Zelensky was allegedly sp...\n476    #Zelensky and his wife #OlenaZelenska bought a...\n477    ALL UNITED AGAINST DRAGONS\\nAGAINST WAR\\nAGAIN...\n478    ALL UNITED AGAINST DRAGONS\\nAGAINST WAR\\nAGAIN...\n479    ALL UNITED AGAINST DRAGONS\\nAGAINST WAR\\nAGAIN...\nName: Translated, Length: 480, dtype: object"},"metadata":{}}],"execution_count":115},{"cell_type":"code","source":"y","metadata":{"_uuid":"fd473d00-ea62-4e6e-97dd-e5be0e554983","_cell_guid":"7dc367df-0e09-49ba-a522-0c957629e44e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-25T16:25:35.393829Z","iopub.execute_input":"2025-03-25T16:25:35.394024Z","iopub.status.idle":"2025-03-25T16:25:35.399621Z","shell.execute_reply.started":"2025-03-25T16:25:35.394007Z","shell.execute_reply":"2025-03-25T16:25:35.398851Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"execution_count":116,"output_type":"execute_result","data":{"text/plain":"0      1\n1      2\n2      2\n3      2\n4      1\n      ..\n475    1\n476    0\n477    2\n478    2\n479    2\nName: Label, Length: 480, dtype: int64"},"metadata":{}}],"execution_count":116},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')","metadata":{"_uuid":"c172be2d-0f81-4396-82ec-d3bb6c89ec92","_cell_guid":"ce50c640-2218-4b16-8d4e-99ab22e3bf5d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-25T16:25:35.893016Z","iopub.execute_input":"2025-03-25T16:25:35.893265Z","iopub.status.idle":"2025-03-25T16:25:36.223932Z","shell.execute_reply.started":"2025-03-25T16:25:35.893243Z","shell.execute_reply":"2025-03-25T16:25:36.223322Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":117},{"cell_type":"code","source":"def generate_embeddings(texts):\n    model.eval()\n    embeddings = []\n    with torch.no_grad():\n        for text in texts:\n            inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n            outputs = model(**inputs)\n            pooled_output = outputs.pooler_output.squeeze().numpy()\n            embeddings.append(pooled_output)\n    return np.array(embeddings)\n\n# Generate embeddings\nprint(\"Generating BERT embeddings...\")\nX_embeddings = generate_embeddings(X)","metadata":{"_uuid":"d89783d0-215b-4acb-b854-991b4bef4f25","_cell_guid":"c9f614b1-157e-4b5c-a6f6-d0f563de464c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-25T16:25:36.224847Z","iopub.execute_input":"2025-03-25T16:25:36.225110Z","iopub.status.idle":"2025-03-25T16:26:23.364577Z","shell.execute_reply.started":"2025-03-25T16:25:36.225079Z","shell.execute_reply":"2025-03-25T16:26:23.363592Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Generating BERT embeddings...\n","output_type":"stream"}],"execution_count":118},{"cell_type":"code","source":"# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X_embeddings, y, test_size=0.2, random_state=42)\n\n# Train an SVM classifier\nprint(\"Training SVM...\")\nclf = SVC(kernel='linear')\nclf.fit(X_train, y_train)\n\n# Predict and evaluate\ny_pred = clf.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy * 100:.2f}%\")","metadata":{"_uuid":"3ce84c7e-9bfa-4cf4-809d-91e668faffc0","_cell_guid":"21686bc9-715c-4054-827b-915e792dfee3","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-25T16:26:23.366602Z","iopub.execute_input":"2025-03-25T16:26:23.367018Z","iopub.status.idle":"2025-03-25T16:26:23.451785Z","shell.execute_reply.started":"2025-03-25T16:26:23.366972Z","shell.execute_reply":"2025-03-25T16:26:23.450766Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Training SVM...\nAccuracy: 41.67%\n","output_type":"stream"}],"execution_count":119},{"cell_type":"code","source":"","metadata":{"_uuid":"a275a7b8-2e9d-4393-9dc0-5ece45107d95","_cell_guid":"e9a07a01-fa9f-43fd-a3b0-81e73e5b41dc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Follow a repo \nFor finetuning","metadata":{"_uuid":"747f90a3-1ba4-4579-97b8-c0217ef96b83","_cell_guid":"3dcc202b-2f28-46b1-9ac8-8ce25bebecbb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"!pip install transformers","metadata":{"_uuid":"c8faea0e-f580-4287-9d64-039cfb8bc4c8","_cell_guid":"26863294-31a6-4121-a618-186dd0b9aa48","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-25T16:26:23.453037Z","iopub.execute_input":"2025-03-25T16:26:23.453325Z","iopub.status.idle":"2025-03-25T16:26:26.970694Z","shell.execute_reply.started":"2025-03-25T16:26:23.453302Z","shell.execute_reply":"2025-03-25T16:26:26.969824Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.12.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":120},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nimport transformers\nfrom transformers import AutoModel, BertTokenizerFast\n\n# specify GPU\ndevice = torch.device(\"cuda\")","metadata":{"_uuid":"3a86378d-0af3-4900-bf8b-50bf7745eca4","_cell_guid":"243292f3-0838-4162-bc86-dc96bffff454","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-25T16:26:26.971924Z","iopub.execute_input":"2025-03-25T16:26:26.972290Z","iopub.status.idle":"2025-03-25T16:26:26.977506Z","shell.execute_reply.started":"2025-03-25T16:26:26.972252Z","shell.execute_reply":"2025-03-25T16:26:26.976700Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":121},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/filtered-and-translated-nlp/filr.csv')","metadata":{"_uuid":"b01a6be1-06de-4075-96f2-74e1410308e3","_cell_guid":"0e477476-d020-46ed-bc4f-08325cc77a81","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-25T16:26:26.978428Z","iopub.execute_input":"2025-03-25T16:26:26.978722Z","iopub.status.idle":"2025-03-25T16:26:27.007381Z","shell.execute_reply.started":"2025-03-25T16:26:26.978692Z","shell.execute_reply":"2025-03-25T16:26:27.006592Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":122},{"cell_type":"code","source":"X = data['Translated']\ny = data['Label']","metadata":{"_uuid":"4f417171-73cd-44b9-b235-1728ce9deda0","_cell_guid":"146c98c0-99f1-4386-8217-03c6aa60587e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-25T16:26:27.008256Z","iopub.execute_input":"2025-03-25T16:26:27.008567Z","iopub.status.idle":"2025-03-25T16:26:27.012765Z","shell.execute_reply.started":"2025-03-25T16:26:27.008538Z","shell.execute_reply":"2025-03-25T16:26:27.011913Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":123},{"cell_type":"code","source":"y.value_counts(normalize = True)","metadata":{"_uuid":"b084b2e3-645a-4785-91fd-0669c3830dae","_cell_guid":"96863ecc-335a-4d1e-b53a-f0d3ca9998bd","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-25T16:26:27.015180Z","iopub.execute_input":"2025-03-25T16:26:27.015396Z","iopub.status.idle":"2025-03-25T16:26:27.028773Z","shell.execute_reply.started":"2025-03-25T16:26:27.015377Z","shell.execute_reply":"2025-03-25T16:26:27.028006Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"execution_count":124,"output_type":"execute_result","data":{"text/plain":"Label\n2    0.431250\n1    0.235417\n3    0.172917\n0    0.160417\nName: proportion, dtype: float64"},"metadata":{}}],"execution_count":124},{"cell_type":"code","source":"train_text, temp_text, train_labels, temp_labels = train_test_split(X, y, \n                                                                    random_state=2018, \n                                                                    test_size=0.3, \n                                                                    stratify=y)\n\n# we will use temp_text and temp_labels to create validation and test set\nval_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n                                                                random_state=2018, \n                                                                test_size=0.5, \n                                                                stratify=temp_labels)","metadata":{"_uuid":"70d68f67-0c46-49be-946c-e157623a89f8","_cell_guid":"ed7a0d9d-697c-477a-a5a0-68eed7a7b5ea","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-25T16:26:27.030430Z","iopub.execute_input":"2025-03-25T16:26:27.030681Z","iopub.status.idle":"2025-03-25T16:26:27.041477Z","shell.execute_reply.started":"2025-03-25T16:26:27.030661Z","shell.execute_reply":"2025-03-25T16:26:27.040665Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":125},{"cell_type":"code","source":"bert = AutoModel.from_pretrained('bert-base-uncased')\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')","metadata":{"_uuid":"6f71cb1e-bbef-4dc6-bff7-f5dc6421f0a6","_cell_guid":"fab5b8a2-0757-41e2-ad42-168d324074b8","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-25T16:26:27.042319Z","iopub.execute_input":"2025-03-25T16:26:27.042532Z","iopub.status.idle":"2025-03-25T16:26:27.261774Z","shell.execute_reply.started":"2025-03-25T16:26:27.042514Z","shell.execute_reply":"2025-03-25T16:26:27.260887Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":126},{"cell_type":"code","source":"# sample data\ntext = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\n\n# encode text\nsent_id = tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False)","metadata":{"_uuid":"9102ce92-0c4d-4112-bd84-bf978c41fc3a","_cell_guid":"5f73af2d-5b96-4334-821e-638759dbc5f9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-25T16:26:27.262680Z","iopub.execute_input":"2025-03-25T16:26:27.262953Z","iopub.status.idle":"2025-03-25T16:26:27.267339Z","shell.execute_reply.started":"2025-03-25T16:26:27.262922Z","shell.execute_reply":"2025-03-25T16:26:27.266616Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":127},{"cell_type":"code","source":"print(sent_id)","metadata":{"_uuid":"74e693e5-1f6e-4c4b-b685-d2da776dade9","_cell_guid":"bd5ffb2f-071f-42b2-940e-c01d96994ea5","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-25T16:26:27.268106Z","iopub.execute_input":"2025-03-25T16:26:27.268406Z","iopub.status.idle":"2025-03-25T16:26:27.280361Z","shell.execute_reply.started":"2025-03-25T16:26:27.268376Z","shell.execute_reply":"2025-03-25T16:26:27.279640Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"{'input_ids': [[101, 2023, 2003, 1037, 14324, 2944, 14924, 4818, 102, 0], [101, 2057, 2097, 2986, 1011, 8694, 1037, 14324, 2944, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n","output_type":"stream"}],"execution_count":128},{"cell_type":"code","source":"","metadata":{"_uuid":"4d31201f-22bb-4bd2-8d14-db9fc13393ff","_cell_guid":"ea5e5d9f-8a5e-4720-bec4-df6d51e1afc4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tokenization","metadata":{"_uuid":"b91d4331-f26f-407c-ad2b-350fdbd1dffd","_cell_guid":"da966a1d-9f21-4161-b929-723f78b01ffb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"seq_len = [len(i.split()) for i in train_text]\n\npd.Series(seq_len).hist(bins = 30)","metadata":{"_uuid":"7a47ed2d-59cd-4688-9fe6-4a97c26483b4","_cell_guid":"88bbc7f3-13ae-477d-ac41-e8b4374bfb21","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-25T16:26:27.281081Z","iopub.execute_input":"2025-03-25T16:26:27.281309Z","iopub.status.idle":"2025-03-25T16:26:27.482108Z","shell.execute_reply.started":"2025-03-25T16:26:27.281290Z","shell.execute_reply":"2025-03-25T16:26:27.481228Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"execution_count":129,"output_type":"execute_result","data":{"text/plain":"<Axes: >"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":129},{"cell_type":"code","source":"max_seq_len = max(seq_len)","metadata":{"_uuid":"1d5c036a-5119-4c70-843f-39d1f4180a84","_cell_guid":"3ef2a485-2291-4258-b6c1-97e7ded974b9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-25T16:26:27.482961Z","iopub.execute_input":"2025-03-25T16:26:27.483291Z","iopub.status.idle":"2025-03-25T16:26:27.487010Z","shell.execute_reply.started":"2025-03-25T16:26:27.483259Z","shell.execute_reply":"2025-03-25T16:26:27.486224Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":130},{"cell_type":"code","source":"max_seq_len","metadata":{"_uuid":"31fa9e1b-46ea-4748-a534-41614c67a1e2","_cell_guid":"1e36a491-8104-42ad-9477-2c4e337322b6","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-25T16:26:27.487756Z","iopub.execute_input":"2025-03-25T16:26:27.488030Z","iopub.status.idle":"2025-03-25T16:26:27.501962Z","shell.execute_reply.started":"2025-03-25T16:26:27.488009Z","shell.execute_reply":"2025-03-25T16:26:27.501216Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"execution_count":131,"output_type":"execute_result","data":{"text/plain":"58"},"metadata":{}}],"execution_count":131},{"cell_type":"code","source":"# tokenize and encode sequences in the training set\ntokens_train = tokenizer.batch_encode_plus(\n    train_text.tolist(),\n    max_length = max_seq_len,\n    pad_to_max_length=True,\n    truncation=True,\n    return_token_type_ids=False\n)\n\n# tokenize and encode sequences in the validation set\ntokens_val = tokenizer.batch_encode_plus(\n    val_text.tolist(),\n    max_length = max_seq_len,\n    pad_to_max_length=True,\n    truncation=True,\n    return_token_type_ids=False\n)\n\n# tokenize and encode sequences in the test set\ntokens_test = tokenizer.batch_encode_plus(\n    test_text.tolist(),\n    max_length = max_seq_len,\n    pad_to_max_length=True,\n    truncation=True,\n    return_token_type_ids=False\n)","metadata":{"_uuid":"1150ca83-91fd-4a36-af81-d8c45e7f5b07","_cell_guid":"cd5d472c-30d7-4f91-acd2-2ef48907ae00","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-25T16:26:27.502767Z","iopub.execute_input":"2025-03-25T16:26:27.503064Z","iopub.status.idle":"2025-03-25T16:26:27.558821Z","shell.execute_reply.started":"2025-03-25T16:26:27.503034Z","shell.execute_reply":"2025-03-25T16:26:27.558075Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2673: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"}],"execution_count":132},{"cell_type":"code","source":"\n# for train set\ntrain_seq = torch.tensor(tokens_train['input_ids'])\ntrain_mask = torch.tensor(tokens_train['attention_mask'])\ntrain_y = torch.tensor(train_labels.tolist())\n\n# for validation set\nval_seq = torch.tensor(tokens_val['input_ids'])\nval_mask = torch.tensor(tokens_val['attention_mask'])\nval_y = torch.tensor(val_labels.tolist())\n\n# for test set\ntest_seq = torch.tensor(tokens_test['input_ids'])\ntest_mask = torch.tensor(tokens_test['attention_mask'])\ntest_y = torch.tensor(test_labels.tolist())","metadata":{"_uuid":"ff701258-6d64-4002-bad9-415b147ef0b9","_cell_guid":"31c9a293-a1dd-44b4-89b3-0484f379e058","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-25T16:26:27.559481Z","iopub.execute_input":"2025-03-25T16:26:27.559860Z","iopub.status.idle":"2025-03-25T16:26:27.574691Z","shell.execute_reply.started":"2025-03-25T16:26:27.559830Z","shell.execute_reply":"2025-03-25T16:26:27.573930Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":133},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n#define a batch size\nbatch_size = 32\n\n# wrap tensors\ntrain_data = TensorDataset(train_seq, train_mask, train_y)\n\n# sampler for sampling the data during training\ntrain_sampler = RandomSampler(train_data)\n\n# dataLoader for train set\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# wrap tensors\nval_data = TensorDataset(val_seq, val_mask, val_y)\n\n# sampler for sampling the data during training\nval_sampler = SequentialSampler(val_data)\n\n# dataLoader for validation set\nval_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)","metadata":{"_uuid":"d7a932f3-26c8-4528-aafd-4f1f54365554","_cell_guid":"ebc4e20b-2b60-4d44-998b-96f7d1cdcd40","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-25T16:26:27.575735Z","iopub.execute_input":"2025-03-25T16:26:27.576003Z","iopub.status.idle":"2025-03-25T16:26:27.587342Z","shell.execute_reply.started":"2025-03-25T16:26:27.575985Z","shell.execute_reply":"2025-03-25T16:26:27.586493Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":134},{"cell_type":"code","source":"\n\n# freeze all the parameters\nfor param in bert.parameters():\n    param.requires_grad = False","metadata":{"_uuid":"dbe27428-dd88-4121-a104-76e4ec16070a","_cell_guid":"87712a31-e7fb-4a18-991c-23e6529cf232","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-25T16:26:27.588068Z","iopub.execute_input":"2025-03-25T16:26:27.588359Z","iopub.status.idle":"2025-03-25T16:26:27.599104Z","shell.execute_reply.started":"2025-03-25T16:26:27.588338Z","shell.execute_reply":"2025-03-25T16:26:27.598415Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":135},{"cell_type":"code","source":"class BERT_Arch(nn.Module):\n\n    def __init__(self, bert):\n      \n      super(BERT_Arch, self).__init__()\n\n      self.bert = bert \n      \n      # dropout layer\n      self.dropout = nn.Dropout(0.1)\n      \n      # relu activation function\n      self.relu =  nn.ReLU()\n\n      # dense layer 1\n      self.fc1 = nn.Linear(768,512)\n      \n      # dense layer 2 (Output layer)\n      self.fc2 = nn.Linear(512,4)\n\n      #softmax activation function\n      self.softmax = nn.LogSoftmax(dim=1)\n\n    #define the forward pass\n    def forward(self, sent_id, mask):\n    # Pass the inputs to the model\n        outputs = self.bert(sent_id, attention_mask=mask)\n        cls_hs = outputs.pooler_output\n        \n        # Pass through fully connected layers\n        x = self.fc1(cls_hs)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.softmax(x)\n    \n        return x","metadata":{"_uuid":"572f1a89-03f0-4377-a7b4-3f3e8b42be5d","_cell_guid":"f7a49a59-88c9-41fd-84fb-9a3977f402c5","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-25T16:26:27.599860Z","iopub.execute_input":"2025-03-25T16:26:27.600088Z","iopub.status.idle":"2025-03-25T16:26:27.612938Z","shell.execute_reply.started":"2025-03-25T16:26:27.600058Z","shell.execute_reply":"2025-03-25T16:26:27.612295Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":136},{"cell_type":"code","source":"print(device)","metadata":{"_uuid":"678ef220-2a2e-4cf2-ba29-5681397e3432","_cell_guid":"463de2d8-2351-468d-b853-010208da3b34","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-25T16:26:27.613768Z","iopub.execute_input":"2025-03-25T16:26:27.614028Z","iopub.status.idle":"2025-03-25T16:26:27.628376Z","shell.execute_reply.started":"2025-03-25T16:26:27.613999Z","shell.execute_reply":"2025-03-25T16:26:27.627589Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":137},{"cell_type":"code","source":"# pass the pre-trained BERT to our define architecture\nmodel = BERT_Arch(bert)\n\n# push the model to GPU\nmodel = model.to(device)","metadata":{"_uuid":"ada4338a-cead-42a3-8561-b127e0ab9f86","_cell_guid":"9ab44a81-852e-4c6f-951c-cb3a0f061307","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-25T16:26:27.629066Z","iopub.execute_input":"2025-03-25T16:26:27.629402Z","iopub.status.idle":"2025-03-25T16:26:27.817464Z","shell.execute_reply.started":"2025-03-25T16:26:27.629372Z","shell.execute_reply":"2025-03-25T16:26:27.816723Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":138},{"cell_type":"code","source":"\n# optimizer from hugging face transformers\nfrom transformers import AdamW\n\n# define the optimizer\noptimizer = AdamW(model.parameters(), lr = 1e-3)","metadata":{"_uuid":"7ad419b7-9bd7-4b53-9f4c-c0330a3dbc6c","_cell_guid":"4f27c952-e7fa-41b5-ac5d-3f06ed84847c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-25T16:26:27.820470Z","iopub.execute_input":"2025-03-25T16:26:27.820681Z","iopub.status.idle":"2025-03-25T16:26:27.829074Z","shell.execute_reply.started":"2025-03-25T16:26:27.820663Z","shell.execute_reply":"2025-03-25T16:26:27.828197Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}],"execution_count":139},{"cell_type":"code","source":"\nfrom sklearn.utils.class_weight import compute_class_weight\n\n#compute the class weights\nclass_wts = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n\nprint(class_wts)","metadata":{"_uuid":"2c481e8a-0b84-4b9e-9076-da4c3c5b968e","_cell_guid":"b90934cb-f593-42ad-85a9-dafc6acde987","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-25T16:26:27.830698Z","iopub.execute_input":"2025-03-25T16:26:27.830987Z","iopub.status.idle":"2025-03-25T16:26:27.849356Z","shell.execute_reply.started":"2025-03-25T16:26:27.830960Z","shell.execute_reply":"2025-03-25T16:26:27.848623Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"[1.55555556 1.06329114 0.57931034 1.44827586]\n","output_type":"stream"}],"execution_count":140},{"cell_type":"code","source":"\n# convert class weights to tensor\nweights= torch.tensor(class_wts,dtype=torch.float)\nweights = weights.to(device)\n\n# loss function\ncross_entropy  = nn.NLLLoss(weight=weights) \n\n# number of training epochs\nepochs = 10","metadata":{"_uuid":"962890b1-e917-4763-aaaa-3917bd924423","_cell_guid":"4d62881d-dca7-4c04-b4a6-e07c6ad39ead","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-25T16:26:27.850274Z","iopub.execute_input":"2025-03-25T16:26:27.850571Z","iopub.status.idle":"2025-03-25T16:26:27.860497Z","shell.execute_reply.started":"2025-03-25T16:26:27.850539Z","shell.execute_reply":"2025-03-25T16:26:27.859580Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":141},{"cell_type":"code","source":"# function to train the model\ndef train():\n  \n  model.train()\n\n  total_loss, total_accuracy = 0, 0\n  \n  # empty list to save model predictions\n  total_preds=[]\n  \n  # iterate over batches\n  for step,batch in enumerate(train_dataloader):\n    \n    # progress update after every 50 batches.\n    if step % 50 == 0 and not step == 0:\n      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n\n    # push the batch to gpu\n    batch = [r.to(device) for r in batch]\n \n    sent_id, mask, labels = batch\n\n    # clear previously calculated gradients \n    model.zero_grad()        \n\n    # get model predictions for the current batch\n    preds = model(sent_id, mask)\n\n    # compute the loss between actual and predicted values\n    loss = cross_entropy(preds, labels)\n\n    # add on to the total loss\n    total_loss = total_loss + loss.item()\n\n    # backward pass to calculate the gradients\n    loss.backward()\n\n    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n    # update parameters\n    optimizer.step()\n\n    # model predictions are stored on GPU. So, push it to CPU\n    preds=preds.detach().cpu().numpy()\n\n    # append the model predictions\n    total_preds.append(preds)\n\n  # compute the training loss of the epoch\n  avg_loss = total_loss / len(train_dataloader)\n  \n  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n  # reshape the predictions in form of (number of samples, no. of classes)\n  total_preds  = np.concatenate(total_preds, axis=0)\n\n  #returns the loss and predictions\n  return avg_loss, total_preds","metadata":{"_uuid":"e93dc8bf-76a0-4a4a-b1b1-21114e4b563d","_cell_guid":"8a0cb4f2-9db5-4055-a1cd-c889414405e8","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-25T16:26:27.861395Z","iopub.execute_input":"2025-03-25T16:26:27.861684Z","iopub.status.idle":"2025-03-25T16:26:27.873421Z","shell.execute_reply.started":"2025-03-25T16:26:27.861656Z","shell.execute_reply":"2025-03-25T16:26:27.872725Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":142},{"cell_type":"code","source":"# function for evaluating the model\ndef evaluate():\n  \n  print(\"\\nEvaluating...\")\n  \n  # deactivate dropout layers\n  model.eval()\n\n  total_loss, total_accuracy = 0, 0\n  \n  # empty list to save the model predictions\n  total_preds = []\n\n  # iterate over batches\n  for step,batch in enumerate(val_dataloader):\n    \n    # Progress update every 50 batches.\n    if step % 50 == 0 and not step == 0:\n      \n      # Calculate elapsed time in minutes.\n      elapsed = format_time(time.time() - t0)\n            \n      # Report progress.\n      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n\n    # push the batch to gpu\n    batch = [t.to(device) for t in batch]\n\n    sent_id, mask, labels = batch\n\n    # deactivate autograd\n    with torch.no_grad():\n      \n      # model predictions\n      preds = model(sent_id, mask)\n\n      # compute the validation loss between actual and predicted values\n      loss = cross_entropy(preds,labels)\n\n      total_loss = total_loss + loss.item()\n\n      preds = preds.detach().cpu().numpy()\n\n      total_preds.append(preds)\n\n  # compute the validation loss of the epoch\n  avg_loss = total_loss / len(val_dataloader) \n\n  # reshape the predictions in form of (number of samples, no. of classes)\n  total_preds  = np.concatenate(total_preds, axis=0)\n\n  return avg_loss, total_preds","metadata":{"_uuid":"fa7facde-ed9b-4840-9aff-8f9f5fe1eefe","_cell_guid":"d3903f69-6be0-4fda-8a5d-9a30e25e33e7","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-25T16:26:27.874170Z","iopub.execute_input":"2025-03-25T16:26:27.874356Z","iopub.status.idle":"2025-03-25T16:26:27.890815Z","shell.execute_reply.started":"2025-03-25T16:26:27.874340Z","shell.execute_reply":"2025-03-25T16:26:27.890097Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":143},{"cell_type":"code","source":"\n# set initial loss to infinite\nbest_valid_loss = float('inf')\n\n# empty lists to store training and validation loss of each epoch\ntrain_losses=[]\nvalid_losses=[]\n\n#for each epoch\nfor epoch in range(epochs):\n     \n    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n    \n    #train model\n    train_loss, _ = train()\n    \n    #evaluate model\n    valid_loss, _ = evaluate()\n    \n    #save the best model\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'saved_weights.pt')\n    \n    # append training and validation loss\n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n    \n    print(f'\\nTraining Loss: {train_loss:.3f}')\n    print(f'Validation Loss: {valid_loss:.3f}')","metadata":{"_uuid":"d1d0ba8c-fafb-40b2-a022-a8ed323828b3","_cell_guid":"d997cba5-a241-444d-a038-b6b3bb7a21ac","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-25T16:26:27.891648Z","iopub.execute_input":"2025-03-25T16:26:27.891915Z","iopub.status.idle":"2025-03-25T16:26:46.913127Z","shell.execute_reply.started":"2025-03-25T16:26:27.891887Z","shell.execute_reply":"2025-03-25T16:26:46.912263Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"\n Epoch 1 / 10\n\nEvaluating...\n\nTraining Loss: 1.539\nValidation Loss: 1.481\n\n Epoch 2 / 10\n\nEvaluating...\n\nTraining Loss: 1.422\nValidation Loss: 1.398\n\n Epoch 3 / 10\n\nEvaluating...\n\nTraining Loss: 1.375\nValidation Loss: 1.386\n\n Epoch 4 / 10\n\nEvaluating...\n\nTraining Loss: 1.351\nValidation Loss: 1.339\n\n Epoch 5 / 10\n\nEvaluating...\n\nTraining Loss: 1.408\nValidation Loss: 1.350\n\n Epoch 6 / 10\n\nEvaluating...\n\nTraining Loss: 1.358\nValidation Loss: 1.331\n\n Epoch 7 / 10\n\nEvaluating...\n\nTraining Loss: 1.362\nValidation Loss: 1.406\n\n Epoch 8 / 10\n\nEvaluating...\n\nTraining Loss: 1.368\nValidation Loss: 1.331\n\n Epoch 9 / 10\n\nEvaluating...\n\nTraining Loss: 1.364\nValidation Loss: 1.328\n\n Epoch 10 / 10\n\nEvaluating...\n\nTraining Loss: 1.313\nValidation Loss: 1.333\n","output_type":"stream"}],"execution_count":144},{"cell_type":"code","source":"\n\n# get predictions for test data\nwith torch.no_grad():\n  preds = model(test_seq.to(device), test_mask.to(device))\n  preds = preds.detach().cpu().numpy()\n     \n\n# model's performance\npreds = np.argmax(preds, axis = 1)\nprint(classification_report(test_y, preds))","metadata":{"_uuid":"399cba0d-5899-491a-85a3-a3f987960f3e","_cell_guid":"c6e87c55-0574-46fc-afba-247f319cb470","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-25T16:26:46.913880Z","iopub.execute_input":"2025-03-25T16:26:46.914108Z","iopub.status.idle":"2025-03-25T16:26:47.115020Z","shell.execute_reply.started":"2025-03-25T16:26:46.914087Z","shell.execute_reply":"2025-03-25T16:26:47.114193Z"}},"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00        12\n           1       0.25      0.18      0.21        17\n           2       0.43      0.48      0.45        31\n           3       0.29      0.58      0.39        12\n\n    accuracy                           0.35        72\n   macro avg       0.24      0.31      0.26        72\nweighted avg       0.29      0.35      0.31        72\n\n","output_type":"stream"}],"execution_count":145},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ChatGPT","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer, BertModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Custom Dataset\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len=512):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, index):\n        text = self.texts.iloc[index]  # Using .iloc for safe indexing\n        label = self.labels.iloc[index]\n        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt')\n\n        return {\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n\n\n# Define Model\nclass BERTClassifier(nn.Module):\n    def __init__(self, num_classes=4):\n        super(BERTClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.fc = nn.Linear(768, num_classes)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        logits = self.fc(outputs.pooler_output)\n        return logits\n\n# Prepare Data\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\ntrain_dataset = TextDataset(X_train, y_train, tokenizer)\ntest_dataset = TextDataset(X_test, y_test, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n\n# Initialize Model and Parameters\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = BERTClassifier(num_classes=4).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=2e-5)\n\n# Training Loop\ndef train_model(epochs=3):\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        for batch in train_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n\n            optimizer.zero_grad()\n            outputs = model(input_ids, attention_mask)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}\")\n\n# Evaluation\ndef evaluate_model():\n    model.eval()\n    all_preds = []\n    all_labels = []\n    with torch.no_grad():\n        for batch in test_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n\n            outputs = model(input_ids, attention_mask)\n            predictions = torch.argmax(outputs, dim=1)\n            all_preds.extend(predictions.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n\n# Run Training and Evaluation\ntrain_model(10)\nevaluate_model()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T16:26:47.115866Z","iopub.execute_input":"2025-03-25T16:26:47.116108Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\ndef save_model(model, tokenizer, path='/kaggle/working/'):\n    # Save model state dict\n    torch.save(model.state_dict(), f'{path}/model_state_dict.pt')\n    \n    # Save tokenizer using Hugging Face method\n    tokenizer.save_pretrained(path)\n    \n    print(f\"Model state dict and tokenizer saved to {path}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_model(model, tokenizer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# /kaggle/working/saved_weights.pt\n# the above is the model path\n\nfrom transformers import BertTokenizer, BertModel\n\ndef load_model(path='/kaggle/working/saved_weights.pt'):\n    model = BertModel.from_pretrained(path)\n    tokenizer = BertTokenizer.from_pretrained(path)\n    print(\"Model and tokenizer loaded from\", path)\n    return model, tokenizer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T16:24:28.842377Z","iopub.execute_input":"2025-03-25T16:24:28.842689Z","iopub.status.idle":"2025-03-25T16:24:28.847383Z","shell.execute_reply.started":"2025-03-25T16:24:28.842665Z","shell.execute_reply":"2025-03-25T16:24:28.846503Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}