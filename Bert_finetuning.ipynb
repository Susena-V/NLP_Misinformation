{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10789676,"sourceType":"datasetVersion","datasetId":6695691}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom transformers import BertTokenizer, BertModel\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"820a0c9c-9169-4aff-a454-26eb58d5ecd8","_cell_guid":"4556d261-d0b0-4b8a-a180-ed6bea44ba55","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-27T02:47:31.920499Z","iopub.execute_input":"2025-03-27T02:47:31.920808Z","iopub.status.idle":"2025-03-27T02:47:32.011407Z","shell.execute_reply.started":"2025-03-27T02:47:31.920783Z","shell.execute_reply":"2025-03-27T02:47:32.010522Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/filtered-and-translated-nlp/filr.csv')","metadata":{"_uuid":"b12b1693-96de-438f-b5bd-0be80408e2de","_cell_guid":"8aea64a9-6ad6-43c8-833b-ab922fafa30c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-27T02:47:32.131050Z","iopub.execute_input":"2025-03-27T02:47:32.131336Z","iopub.status.idle":"2025-03-27T02:47:32.173977Z","shell.execute_reply.started":"2025-03-27T02:47:32.131315Z","shell.execute_reply":"2025-03-27T02:47:32.173050Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = data['Translated']\ny = data['Label']","metadata":{"_uuid":"f324c517-6ce5-4951-9dca-a423158612f8","_cell_guid":"bc70eafa-cd6d-4fb7-be0c-4482f90eebb2","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-27T02:47:32.432611Z","iopub.execute_input":"2025-03-27T02:47:32.432919Z","iopub.status.idle":"2025-03-27T02:47:32.440699Z","shell.execute_reply.started":"2025-03-27T02:47:32.432897Z","shell.execute_reply":"2025-03-27T02:47:32.439631Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"X","metadata":{"_uuid":"f7ac0f97-3ada-4ef2-bbfb-c2fab978d7b4","_cell_guid":"c03d2dcb-94e4-4f34-a030-670e062b5b26","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T13:19:53.646209Z","iopub.execute_input":"2025-03-26T13:19:53.646416Z","iopub.status.idle":"2025-03-26T13:19:53.667971Z","shell.execute_reply.started":"2025-03-26T13:19:53.646398Z","shell.execute_reply":"2025-03-26T13:19:53.667020Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y","metadata":{"_uuid":"fd473d00-ea62-4e6e-97dd-e5be0e554983","_cell_guid":"7dc367df-0e09-49ba-a522-0c957629e44e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T13:19:53.668905Z","iopub.execute_input":"2025-03-26T13:19:53.669217Z","iopub.status.idle":"2025-03-26T13:19:53.686321Z","shell.execute_reply.started":"2025-03-26T13:19:53.669186Z","shell.execute_reply":"2025-03-26T13:19:53.685506Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')","metadata":{"_uuid":"c172be2d-0f81-4396-82ec-d3bb6c89ec92","_cell_guid":"ce50c640-2218-4b16-8d4e-99ab22e3bf5d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T13:19:53.688123Z","iopub.execute_input":"2025-03-26T13:19:53.688308Z","iopub.status.idle":"2025-03-26T13:19:57.076997Z","shell.execute_reply.started":"2025-03-26T13:19:53.688292Z","shell.execute_reply":"2025-03-26T13:19:57.076108Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_embeddings(texts):\n    model.eval()\n    embeddings = []\n    with torch.no_grad():\n        for text in texts:\n            inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n            outputs = model(**inputs)\n            pooled_output = outputs.pooler_output.squeeze().numpy()\n            embeddings.append(pooled_output)\n    return np.array(embeddings)\n\n# Generate embeddings\nprint(\"Generating BERT embeddings...\")\nX_embeddings = generate_embeddings(X)","metadata":{"_uuid":"d89783d0-215b-4acb-b854-991b4bef4f25","_cell_guid":"c9f614b1-157e-4b5c-a6f6-d0f563de464c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T13:19:57.078214Z","iopub.execute_input":"2025-03-26T13:19:57.078536Z","iopub.status.idle":"2025-03-26T13:20:45.705011Z","shell.execute_reply.started":"2025-03-26T13:19:57.078506Z","shell.execute_reply":"2025-03-26T13:20:45.703896Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X_embeddings, y, test_size=0.2, random_state=42)\n\n# Train an SVM classifier\nprint(\"Training SVM...\")\nclf = SVC(kernel='linear')\nclf.fit(X_train, y_train)\n\n# Predict and evaluate\ny_pred = clf.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy * 100:.2f}%\")","metadata":{"_uuid":"3ce84c7e-9bfa-4cf4-809d-91e668faffc0","_cell_guid":"21686bc9-715c-4054-827b-915e792dfee3","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T13:20:45.706074Z","iopub.execute_input":"2025-03-26T13:20:45.706483Z","iopub.status.idle":"2025-03-26T13:20:45.837889Z","shell.execute_reply.started":"2025-03-26T13:20:45.706445Z","shell.execute_reply":"2025-03-26T13:20:45.836849Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"a275a7b8-2e9d-4393-9dc0-5ece45107d95","_cell_guid":"e9a07a01-fa9f-43fd-a3b0-81e73e5b41dc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Follow a repo \nFor finetuning","metadata":{"_uuid":"747f90a3-1ba4-4579-97b8-c0217ef96b83","_cell_guid":"3dcc202b-2f28-46b1-9ac8-8ce25bebecbb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"!pip install transformers","metadata":{"_uuid":"c8faea0e-f580-4287-9d64-039cfb8bc4c8","_cell_guid":"26863294-31a6-4121-a618-186dd0b9aa48","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T02:50:32.387609Z","iopub.execute_input":"2025-03-26T02:50:32.387906Z","iopub.status.idle":"2025-03-26T02:50:38.027333Z","shell.execute_reply.started":"2025-03-26T02:50:32.387879Z","shell.execute_reply":"2025-03-26T02:50:38.026133Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nimport transformers\nfrom transformers import AutoModel, BertTokenizerFast\n\n# specify GPU\ndevice = torch.device(\"cuda\")","metadata":{"_uuid":"3a86378d-0af3-4900-bf8b-50bf7745eca4","_cell_guid":"243292f3-0838-4162-bc86-dc96bffff454","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T02:50:38.028669Z","iopub.execute_input":"2025-03-26T02:50:38.029035Z","iopub.status.idle":"2025-03-26T02:50:38.062060Z","shell.execute_reply.started":"2025-03-26T02:50:38.028997Z","shell.execute_reply":"2025-03-26T02:50:38.061468Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/filtered-and-translated-nlp/filr.csv')","metadata":{"_uuid":"b01a6be1-06de-4075-96f2-74e1410308e3","_cell_guid":"0e477476-d020-46ed-bc4f-08325cc77a81","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T02:50:38.062842Z","iopub.execute_input":"2025-03-26T02:50:38.063059Z","iopub.status.idle":"2025-03-26T02:50:38.096223Z","shell.execute_reply.started":"2025-03-26T02:50:38.063039Z","shell.execute_reply":"2025-03-26T02:50:38.095161Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = data['Translated']\ny = data['Label']","metadata":{"_uuid":"4f417171-73cd-44b9-b235-1728ce9deda0","_cell_guid":"146c98c0-99f1-4386-8217-03c6aa60587e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T02:50:38.097180Z","iopub.execute_input":"2025-03-26T02:50:38.097499Z","iopub.status.idle":"2025-03-26T02:50:38.101177Z","shell.execute_reply.started":"2025-03-26T02:50:38.097469Z","shell.execute_reply":"2025-03-26T02:50:38.100534Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y.value_counts(normalize = True)","metadata":{"_uuid":"b084b2e3-645a-4785-91fd-0669c3830dae","_cell_guid":"96863ecc-335a-4d1e-b53a-f0d3ca9998bd","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T02:50:38.102062Z","iopub.execute_input":"2025-03-26T02:50:38.102294Z","iopub.status.idle":"2025-03-26T02:50:38.132968Z","shell.execute_reply.started":"2025-03-26T02:50:38.102274Z","shell.execute_reply":"2025-03-26T02:50:38.132338Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_text, temp_text, train_labels, temp_labels = train_test_split(X, y, \n                                                                    random_state=2018, \n                                                                    test_size=0.3, \n                                                                    stratify=y)\n\n# we will use temp_text and temp_labels to create validation and test set\nval_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n                                                                random_state=2018, \n                                                                test_size=0.5, \n                                                                stratify=temp_labels)","metadata":{"_uuid":"70d68f67-0c46-49be-946c-e157623a89f8","_cell_guid":"ed7a0d9d-697c-477a-a5a0-68eed7a7b5ea","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T02:50:38.133877Z","iopub.execute_input":"2025-03-26T02:50:38.134145Z","iopub.status.idle":"2025-03-26T02:50:38.159054Z","shell.execute_reply.started":"2025-03-26T02:50:38.134117Z","shell.execute_reply":"2025-03-26T02:50:38.158375Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bert = AutoModel.from_pretrained('bert-base-uncased')\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')","metadata":{"_uuid":"6f71cb1e-bbef-4dc6-bff7-f5dc6421f0a6","_cell_guid":"fab5b8a2-0757-41e2-ad42-168d324074b8","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T02:50:38.160014Z","iopub.execute_input":"2025-03-26T02:50:38.160301Z","iopub.status.idle":"2025-03-26T02:50:39.180125Z","shell.execute_reply.started":"2025-03-26T02:50:38.160272Z","shell.execute_reply":"2025-03-26T02:50:39.179095Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# sample data\ntext = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\n\n# encode text\nsent_id = tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False)","metadata":{"_uuid":"9102ce92-0c4d-4112-bd84-bf978c41fc3a","_cell_guid":"5f73af2d-5b96-4334-821e-638759dbc5f9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T02:50:39.181131Z","iopub.execute_input":"2025-03-26T02:50:39.181507Z","iopub.status.idle":"2025-03-26T02:50:39.194673Z","shell.execute_reply.started":"2025-03-26T02:50:39.181472Z","shell.execute_reply":"2025-03-26T02:50:39.193809Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(sent_id)","metadata":{"_uuid":"74e693e5-1f6e-4c4b-b685-d2da776dade9","_cell_guid":"bd5ffb2f-071f-42b2-940e-c01d96994ea5","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-26T02:50:39.195692Z","iopub.execute_input":"2025-03-26T02:50:39.195944Z","iopub.status.idle":"2025-03-26T02:50:39.215019Z","shell.execute_reply.started":"2025-03-26T02:50:39.195921Z","shell.execute_reply":"2025-03-26T02:50:39.214026Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"4d31201f-22bb-4bd2-8d14-db9fc13393ff","_cell_guid":"ea5e5d9f-8a5e-4720-bec4-df6d51e1afc4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tokenization","metadata":{"_uuid":"b91d4331-f26f-407c-ad2b-350fdbd1dffd","_cell_guid":"da966a1d-9f21-4161-b929-723f78b01ffb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"seq_len = [len(i.split()) for i in train_text]\n\npd.Series(seq_len).hist(bins = 30)","metadata":{"_uuid":"7a47ed2d-59cd-4688-9fe6-4a97c26483b4","_cell_guid":"88bbc7f3-13ae-477d-ac41-e8b4374bfb21","trusted":true,"collapsed":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:39.216125Z","iopub.execute_input":"2025-03-26T02:50:39.216476Z","iopub.status.idle":"2025-03-26T02:50:39.601109Z","shell.execute_reply.started":"2025-03-26T02:50:39.216437Z","shell.execute_reply":"2025-03-26T02:50:39.600386Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"max_seq_len = max(seq_len)","metadata":{"_uuid":"1d5c036a-5119-4c70-843f-39d1f4180a84","_cell_guid":"3ef2a485-2291-4258-b6c1-97e7ded974b9","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:39.601878Z","iopub.execute_input":"2025-03-26T02:50:39.602142Z","iopub.status.idle":"2025-03-26T02:50:39.606033Z","shell.execute_reply.started":"2025-03-26T02:50:39.602119Z","shell.execute_reply":"2025-03-26T02:50:39.605081Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"max_seq_len","metadata":{"_uuid":"31fa9e1b-46ea-4748-a534-41614c67a1e2","_cell_guid":"1e36a491-8104-42ad-9477-2c4e337322b6","trusted":true,"collapsed":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:39.610061Z","iopub.execute_input":"2025-03-26T02:50:39.610291Z","iopub.status.idle":"2025-03-26T02:50:39.627931Z","shell.execute_reply.started":"2025-03-26T02:50:39.610270Z","shell.execute_reply":"2025-03-26T02:50:39.627267Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# tokenize and encode sequences in the training set\ntokens_train = tokenizer.batch_encode_plus(\n    train_text.tolist(),\n    max_length = max_seq_len,\n    pad_to_max_length=True,\n    truncation=True,\n    return_token_type_ids=False\n)\n\n# tokenize and encode sequences in the validation set\ntokens_val = tokenizer.batch_encode_plus(\n    val_text.tolist(),\n    max_length = max_seq_len,\n    pad_to_max_length=True,\n    truncation=True,\n    return_token_type_ids=False\n)\n\n# tokenize and encode sequences in the test set\ntokens_test = tokenizer.batch_encode_plus(\n    test_text.tolist(),\n    max_length = max_seq_len,\n    pad_to_max_length=True,\n    truncation=True,\n    return_token_type_ids=False\n)","metadata":{"_uuid":"1150ca83-91fd-4a36-af81-d8c45e7f5b07","_cell_guid":"cd5d472c-30d7-4f91-acd2-2ef48907ae00","trusted":true,"collapsed":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:39.629689Z","iopub.execute_input":"2025-03-26T02:50:39.629883Z","iopub.status.idle":"2025-03-26T02:50:39.689739Z","shell.execute_reply.started":"2025-03-26T02:50:39.629866Z","shell.execute_reply":"2025-03-26T02:50:39.688789Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# for train set\ntrain_seq = torch.tensor(tokens_train['input_ids'])\ntrain_mask = torch.tensor(tokens_train['attention_mask'])\ntrain_y = torch.tensor(train_labels.tolist())\n\n# for validation set\nval_seq = torch.tensor(tokens_val['input_ids'])\nval_mask = torch.tensor(tokens_val['attention_mask'])\nval_y = torch.tensor(val_labels.tolist())\n\n# for test set\ntest_seq = torch.tensor(tokens_test['input_ids'])\ntest_mask = torch.tensor(tokens_test['attention_mask'])\ntest_y = torch.tensor(test_labels.tolist())","metadata":{"_uuid":"ff701258-6d64-4002-bad9-415b147ef0b9","_cell_guid":"31c9a293-a1dd-44b4-89b3-0484f379e058","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:39.690292Z","iopub.execute_input":"2025-03-26T02:50:39.690534Z","iopub.status.idle":"2025-03-26T02:50:39.704164Z","shell.execute_reply.started":"2025-03-26T02:50:39.690515Z","shell.execute_reply":"2025-03-26T02:50:39.703323Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n#define a batch size\nbatch_size = 32\n\n# wrap tensors\ntrain_data = TensorDataset(train_seq, train_mask, train_y)\n\n# sampler for sampling the data during training\ntrain_sampler = RandomSampler(train_data)\n\n# dataLoader for train set\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# wrap tensors\nval_data = TensorDataset(val_seq, val_mask, val_y)\n\n# sampler for sampling the data during training\nval_sampler = SequentialSampler(val_data)\n\n# dataLoader for validation set\nval_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)","metadata":{"_uuid":"d7a932f3-26c8-4528-aafd-4f1f54365554","_cell_guid":"ebc4e20b-2b60-4d44-998b-96f7d1cdcd40","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:39.705091Z","iopub.execute_input":"2025-03-26T02:50:39.705393Z","iopub.status.idle":"2025-03-26T02:50:39.730961Z","shell.execute_reply.started":"2025-03-26T02:50:39.705367Z","shell.execute_reply":"2025-03-26T02:50:39.730094Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# freeze all the parameters\nfor param in bert.parameters():\n    param.requires_grad = False","metadata":{"_uuid":"dbe27428-dd88-4121-a104-76e4ec16070a","_cell_guid":"87712a31-e7fb-4a18-991c-23e6529cf232","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:39.731796Z","iopub.execute_input":"2025-03-26T02:50:39.732114Z","iopub.status.idle":"2025-03-26T02:50:39.750541Z","shell.execute_reply.started":"2025-03-26T02:50:39.732091Z","shell.execute_reply":"2025-03-26T02:50:39.749727Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class BERT_Arch(nn.Module):\n\n    def __init__(self, bert):\n      \n      super(BERT_Arch, self).__init__()\n\n      self.bert = bert \n      \n      # dropout layer\n      self.dropout = nn.Dropout(0.1)\n      \n      # relu activation function\n      self.relu =  nn.ReLU()\n\n      # dense layer 1\n      self.fc1 = nn.Linear(768,512)\n      \n      # dense layer 2 (Output layer)\n      self.fc2 = nn.Linear(512,4)\n\n      #softmax activation function\n      self.softmax = nn.LogSoftmax(dim=1)\n\n    #define the forward pass\n    def forward(self, sent_id, mask):\n    # Pass the inputs to the model\n        outputs = self.bert(sent_id, attention_mask=mask)\n        cls_hs = outputs.pooler_output\n        \n        # Pass through fully connected layers\n        x = self.fc1(cls_hs)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.softmax(x)\n    \n        return x","metadata":{"_uuid":"572f1a89-03f0-4377-a7b4-3f3e8b42be5d","_cell_guid":"f7a49a59-88c9-41fd-84fb-9a3977f402c5","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:39.751272Z","iopub.execute_input":"2025-03-26T02:50:39.751524Z","iopub.status.idle":"2025-03-26T02:50:39.772998Z","shell.execute_reply.started":"2025-03-26T02:50:39.751505Z","shell.execute_reply":"2025-03-26T02:50:39.772387Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(device)","metadata":{"_uuid":"678ef220-2a2e-4cf2-ba29-5681397e3432","_cell_guid":"463de2d8-2351-468d-b853-010208da3b34","trusted":true,"collapsed":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:39.773633Z","iopub.execute_input":"2025-03-26T02:50:39.773874Z","iopub.status.idle":"2025-03-26T02:50:39.801485Z","shell.execute_reply.started":"2025-03-26T02:50:39.773855Z","shell.execute_reply":"2025-03-26T02:50:39.800701Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pass the pre-trained BERT to our define architecture\nmodel = BERT_Arch(bert)\n\n# push the model to GPU\nmodel = model.to(device)","metadata":{"_uuid":"ada4338a-cead-42a3-8561-b127e0ab9f86","_cell_guid":"9ab44a81-852e-4c6f-951c-cb3a0f061307","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:39.802221Z","iopub.execute_input":"2025-03-26T02:50:39.802473Z","iopub.status.idle":"2025-03-26T02:50:40.382539Z","shell.execute_reply.started":"2025-03-26T02:50:39.802448Z","shell.execute_reply":"2025-03-26T02:50:40.381586Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# optimizer from hugging face transformers\nfrom transformers import AdamW\n\n# define the optimizer\noptimizer = AdamW(model.parameters(), lr = 1e-3)","metadata":{"_uuid":"7ad419b7-9bd7-4b53-9f4c-c0330a3dbc6c","_cell_guid":"4f27c952-e7fa-41b5-ac5d-3f06ed84847c","trusted":true,"collapsed":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:40.385693Z","iopub.execute_input":"2025-03-26T02:50:40.385904Z","iopub.status.idle":"2025-03-26T02:50:40.419028Z","shell.execute_reply.started":"2025-03-26T02:50:40.385885Z","shell.execute_reply":"2025-03-26T02:50:40.418384Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfrom sklearn.utils.class_weight import compute_class_weight\n\n#compute the class weights\nclass_wts = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n\nprint(class_wts)","metadata":{"_uuid":"2c481e8a-0b84-4b9e-9076-da4c3c5b968e","_cell_guid":"b90934cb-f593-42ad-85a9-dafc6acde987","trusted":true,"collapsed":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:40.419797Z","iopub.execute_input":"2025-03-26T02:50:40.420046Z","iopub.status.idle":"2025-03-26T02:50:40.426465Z","shell.execute_reply.started":"2025-03-26T02:50:40.420014Z","shell.execute_reply":"2025-03-26T02:50:40.425542Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# convert class weights to tensor\nweights= torch.tensor(class_wts,dtype=torch.float)\nweights = weights.to(device)\n\n# loss function\ncross_entropy  = nn.NLLLoss(weight=weights) \n\n# number of training epochs\nepochs = 10","metadata":{"_uuid":"962890b1-e917-4763-aaaa-3917bd924423","_cell_guid":"4d62881d-dca7-4c04-b4a6-e07c6ad39ead","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:40.427242Z","iopub.execute_input":"2025-03-26T02:50:40.427547Z","iopub.status.idle":"2025-03-26T02:50:40.451741Z","shell.execute_reply.started":"2025-03-26T02:50:40.427525Z","shell.execute_reply":"2025-03-26T02:50:40.450925Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# function to train the model\ndef train():\n  \n  model.train()\n\n  total_loss, total_accuracy = 0, 0\n  \n  # empty list to save model predictions\n  total_preds=[]\n  \n  # iterate over batches\n  for step,batch in enumerate(train_dataloader):\n    \n    # progress update after every 50 batches.\n    if step % 50 == 0 and not step == 0:\n      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n\n    # push the batch to gpu\n    batch = [r.to(device) for r in batch]\n \n    sent_id, mask, labels = batch\n\n    # clear previously calculated gradients \n    model.zero_grad()        \n\n    # get model predictions for the current batch\n    preds = model(sent_id, mask)\n\n    # compute the loss between actual and predicted values\n    loss = cross_entropy(preds, labels)\n\n    # add on to the total loss\n    total_loss = total_loss + loss.item()\n\n    # backward pass to calculate the gradients\n    loss.backward()\n\n    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n    # update parameters\n    optimizer.step()\n\n    # model predictions are stored on GPU. So, push it to CPU\n    preds=preds.detach().cpu().numpy()\n\n    # append the model predictions\n    total_preds.append(preds)\n\n  # compute the training loss of the epoch\n  avg_loss = total_loss / len(train_dataloader)\n  \n  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n  # reshape the predictions in form of (number of samples, no. of classes)\n  total_preds  = np.concatenate(total_preds, axis=0)\n\n  #returns the loss and predictions\n  return avg_loss, total_preds","metadata":{"_uuid":"e93dc8bf-76a0-4a4a-b1b1-21114e4b563d","_cell_guid":"8a0cb4f2-9db5-4055-a1cd-c889414405e8","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:40.452735Z","iopub.execute_input":"2025-03-26T02:50:40.453036Z","iopub.status.idle":"2025-03-26T02:50:40.468984Z","shell.execute_reply.started":"2025-03-26T02:50:40.453006Z","shell.execute_reply":"2025-03-26T02:50:40.468387Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# function for evaluating the model\ndef evaluate():\n  \n  print(\"\\nEvaluating...\")\n  \n  # deactivate dropout layers\n  model.eval()\n\n  total_loss, total_accuracy = 0, 0\n  \n  # empty list to save the model predictions\n  total_preds = []\n\n  # iterate over batches\n  for step,batch in enumerate(val_dataloader):\n    \n    # Progress update every 50 batches.\n    if step % 50 == 0 and not step == 0:\n      \n      # Calculate elapsed time in minutes.\n      elapsed = format_time(time.time() - t0)\n            \n      # Report progress.\n      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n\n    # push the batch to gpu\n    batch = [t.to(device) for t in batch]\n\n    sent_id, mask, labels = batch\n\n    # deactivate autograd\n    with torch.no_grad():\n      \n      # model predictions\n      preds = model(sent_id, mask)\n\n      # compute the validation loss between actual and predicted values\n      loss = cross_entropy(preds,labels)\n\n      total_loss = total_loss + loss.item()\n\n      preds = preds.detach().cpu().numpy()\n\n      total_preds.append(preds)\n\n  # compute the validation loss of the epoch\n  avg_loss = total_loss / len(val_dataloader) \n\n  # reshape the predictions in form of (number of samples, no. of classes)\n  total_preds  = np.concatenate(total_preds, axis=0)\n\n  return avg_loss, total_preds","metadata":{"_uuid":"fa7facde-ed9b-4840-9aff-8f9f5fe1eefe","_cell_guid":"d3903f69-6be0-4fda-8a5d-9a30e25e33e7","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:40.469658Z","iopub.execute_input":"2025-03-26T02:50:40.469896Z","iopub.status.idle":"2025-03-26T02:50:40.496353Z","shell.execute_reply.started":"2025-03-26T02:50:40.469876Z","shell.execute_reply":"2025-03-26T02:50:40.495680Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# set initial loss to infinite\nbest_valid_loss = float('inf')\n\n# empty lists to store training and validation loss of each epoch\ntrain_losses=[]\nvalid_losses=[]\n\n#for each epoch\nfor epoch in range(epochs):\n     \n    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n    \n    #train model\n    train_loss, _ = train()\n    \n    #evaluate model\n    valid_loss, _ = evaluate()\n    \n    #save the best model\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'saved_weights.pt')\n    \n    # append training and validation loss\n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n    \n    print(f'\\nTraining Loss: {train_loss:.3f}')\n    print(f'Validation Loss: {valid_loss:.3f}')","metadata":{"_uuid":"d1d0ba8c-fafb-40b2-a022-a8ed323828b3","_cell_guid":"d997cba5-a241-444d-a038-b6b3bb7a21ac","trusted":true,"collapsed":true,"execution":{"iopub.status.busy":"2025-03-26T02:50:40.497235Z","iopub.execute_input":"2025-03-26T02:50:40.497554Z","iopub.status.idle":"2025-03-26T02:50:56.604411Z","shell.execute_reply.started":"2025-03-26T02:50:40.497524Z","shell.execute_reply":"2025-03-26T02:50:56.603377Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# get predictions for test data\nwith torch.no_grad():\n  preds = model(test_seq.to(device), test_mask.to(device))\n  preds = preds.detach().cpu().numpy()\n     \n\n# model's performance\npreds = np.argmax(preds, axis = 1)\nprint(classification_report(test_y, preds))","metadata":{"_uuid":"399cba0d-5899-491a-85a3-a3f987960f3e","_cell_guid":"c6e87c55-0574-46fc-afba-247f319cb470","trusted":true,"collapsed":true,"jupyter":{"source_hidden":true,"outputs_hidden":true},"execution":{"iopub.status.busy":"2025-03-26T02:50:56.605532Z","iopub.execute_input":"2025-03-26T02:50:56.605873Z","iopub.status.idle":"2025-03-26T02:50:56.820150Z","shell.execute_reply.started":"2025-03-26T02:50:56.605843Z","shell.execute_reply":"2025-03-26T02:50:56.819254Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ChatGPT","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer, BertModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Custom Dataset\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len=512):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, index):\n        text = self.texts.iloc[index]  # Using .iloc for safe indexing\n        label = self.labels.iloc[index]\n        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt')\n\n        return {\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n\n\n# Define Model\nclass BERTClassifier(nn.Module):\n    def __init__(self, num_classes=4):\n        super(BERTClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.fc = nn.Linear(768, num_classes)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        logits = self.fc(outputs.pooler_output)\n        return logits\n\n# Prepare Data\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\ntrain_dataset = TextDataset(X_train, y_train, tokenizer)\ntest_dataset = TextDataset(X_test, y_test, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n\n# Initialize Model and Parameters\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = BERTClassifier(num_classes=4).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=2e-5)\n\n# Training Loop\ndef train_model(epochs=3):\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        for batch in train_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n\n            optimizer.zero_grad()\n            outputs = model(input_ids, attention_mask)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}\")\n\n# Evaluation\ndef evaluate_model():\n    model.eval()\n    all_preds = []\n    all_labels = []\n    with torch.no_grad():\n        for batch in test_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n\n            outputs = model(input_ids, attention_mask)\n            predictions = torch.argmax(outputs, dim=1)\n            all_preds.extend(predictions.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n\n# Run Training and Evaluation\ntrain_model(25)\nevaluate_model()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T13:58:40.699341Z","iopub.execute_input":"2025-03-26T13:58:40.699685Z","iopub.status.idle":"2025-03-26T14:14:52.772456Z","shell.execute_reply.started":"2025-03-26T13:58:40.699656Z","shell.execute_reply":"2025-03-26T14:14:52.771550Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\ndef save_model(model, path='/kaggle/working/model_state_dict.pt'):\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n    torch.save(model.state_dict(), path)\n    print(f\"Model state dict saved to {path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:14:52.773883Z","iopub.execute_input":"2025-03-26T14:14:52.774267Z","iopub.status.idle":"2025-03-26T14:14:52.778361Z","shell.execute_reply.started":"2025-03-26T14:14:52.774232Z","shell.execute_reply":"2025-03-26T14:14:52.777437Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_model(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:14:52.779793Z","iopub.execute_input":"2025-03-26T14:14:52.779987Z","iopub.status.idle":"2025-03-26T14:14:53.884112Z","shell.execute_reply.started":"2025-03-26T14:14:52.779971Z","shell.execute_reply":"2025-03-26T14:14:53.883037Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_tokenizer(tokenizer, path='/kaggle/working/tokenizer'):\n    os.makedirs(path, exist_ok=True)\n    tokenizer.save_pretrained(path)\n    print(f\"Tokenizer saved to {path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:14:53.885821Z","iopub.execute_input":"2025-03-26T14:14:53.886133Z","iopub.status.idle":"2025-03-26T14:14:53.890346Z","shell.execute_reply.started":"2025-03-26T14:14:53.886111Z","shell.execute_reply":"2025-03-26T14:14:53.889483Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_tokenizer(tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:14:53.891264Z","iopub.execute_input":"2025-03-26T14:14:53.891559Z","iopub.status.idle":"2025-03-26T14:14:53.930982Z","shell.execute_reply.started":"2025-03-26T14:14:53.891531Z","shell.execute_reply":"2025-03-26T14:14:53.930271Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\ndef generate_embeddings(texts, tokenizer, model, batch_size=16, max_len=512):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    model.eval()\n\n    embeddings = []\n\n    with torch.no_grad():\n        for i in range(0, len(texts), batch_size):\n            batch_texts = texts[i:i+batch_size]\n\n            inputs = tokenizer(batch_texts, padding=True, truncation=True, max_length=max_len, return_tensors='pt')\n            input_ids = inputs['input_ids'].to(device)\n            attention_mask = inputs['attention_mask'].to(device)\n            \n            outputs = model.bert(input_ids=input_ids, attention_mask=attention_mask)\n\n            # If pooler_output is available, use it; otherwise, use mean of last hidden state\n            if hasattr(outputs, 'pooler_output'):\n                batch_embeddings = outputs.pooler_output.cpu().numpy()\n            else:\n                batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n\n            embeddings.extend(batch_embeddings)\n    \n    return np.array(embeddings)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:14:53.931878Z","iopub.execute_input":"2025-03-26T14:14:53.932198Z","iopub.status.idle":"2025-03-26T14:14:53.938597Z","shell.execute_reply.started":"2025-03-26T14:14:53.932169Z","shell.execute_reply":"2025-03-26T14:14:53.937503Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n\n# Assuming X_train, X_test, y_train, y_test are available\nX_train_embeddings = generate_embeddings(X_train.tolist(), tokenizer, model)\nX_test_embeddings = generate_embeddings(X_test.tolist(), tokenizer, model)\n\n# Train the SVM\nsvm_classifier = SVC(kernel='linear', class_weight = 'balanced')\nsvm_classifier.fit(X_train_embeddings, y_train)\n\n# Predict and Evaluate\ny_pred = svm_classifier.predict(X_test_embeddings)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(f\"Test Accuracy with SVM: {accuracy * 100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:14:53.939316Z","iopub.execute_input":"2025-03-26T14:14:53.939559Z","iopub.status.idle":"2025-03-26T14:14:56.645812Z","shell.execute_reply.started":"2025-03-26T14:14:53.939540Z","shell.execute_reply":"2025-03-26T14:14:56.644821Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve\n\n# Predict using SVM\ny_pred = svm_classifier.predict(X_test_embeddings)\n\n# Confusion Matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(8,6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()\n\n# Classification Report\nprint(\"Classification Report:\\n\")\nprint(classification_report(y_test, y_pred))\n\n# AUC-ROC Score\nif len(set(y_test)) <= 2:\n    auc_score = roc_auc_score(y_test, svm_classifier.decision_function(X_test_embeddings))\n    print(f\"AUC-ROC Score: {auc_score:.2f}\")\n\n    # Plot ROC Curve\n    fpr, tpr, _ = roc_curve(y_test, svm_classifier.decision_function(X_test_embeddings))\n    plt.figure(figsize=(8,6))\n    plt.plot(fpr, tpr, color='blue', label=f\"AUC = {auc_score:.2f}\")\n    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.title(\"ROC Curve\")\n    plt.legend(loc=\"lower right\")\n    plt.show()\nelse:\n    print(\"AUC-ROC is not applicable for multi-class classification.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:14:56.647690Z","iopub.execute_input":"2025-03-26T14:14:56.647951Z","iopub.status.idle":"2025-03-26T14:14:56.875687Z","shell.execute_reply.started":"2025-03-26T14:14:56.647929Z","shell.execute_reply":"2025-03-26T14:14:56.875051Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('hi')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:14:56.876469Z","iopub.execute_input":"2025-03-26T14:14:56.876713Z","iopub.status.idle":"2025-03-26T14:14:56.880659Z","shell.execute_reply.started":"2025-03-26T14:14:56.876688Z","shell.execute_reply":"2025-03-26T14:14:56.879957Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"misclassified_indices = np.where(y_test != y_pred)[0]\nfor idx in misclassified_indices[:10]:  # Inspect first 10 misclassified samples\n    print(f\"Text: {X_test.iloc[idx]}\")\n    print(f\"True Label: {y_test.iloc[idx]}, Predicted Label: {y_pred[idx]}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:14:56.881523Z","iopub.execute_input":"2025-03-26T14:14:56.881857Z","iopub.status.idle":"2025-03-26T14:14:56.899878Z","shell.execute_reply.started":"2025-03-26T14:14:56.881835Z","shell.execute_reply":"2025-03-26T14:14:56.899236Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Early Stopping","metadata":{}},{"cell_type":"code","source":"class BERTClassifierWithAttention(nn.Module):\n    def __init__(self, num_classes=4):\n        super(BERTClassifierWithAttention, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)\n        self.fc = nn.Linear(768, num_classes)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        logits = self.fc(outputs.pooler_output)\n        \n        # Extract attention weights (already normalized)\n        attentions = outputs.attentions  # Tuple of attention weights for each layer\n        \n        # Compute attention scores (unnormalized)\n        attention_scores = []\n        for layer in self.bert.encoder.layer:\n            attn = layer.attention.self\n            q = attn.query(input_ids)\n            k = attn.key(input_ids)\n            scores = torch.matmul(q, k.transpose(-2, -1)) / (q.size(-1) ** 0.5)\n            attention_scores.append(scores.detach())  # Store unnormalized scores\n        \n        return logits, attentions, attention_scores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:35:21.888874Z","iopub.execute_input":"2025-03-26T14:35:21.889189Z","iopub.status.idle":"2025-03-26T14:35:21.895265Z","shell.execute_reply.started":"2025-03-26T14:35:21.889166Z","shell.execute_reply":"2025-03-26T14:35:21.894446Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef plot_attention_scores(scores, title=\"Attention Scores\"):\n    # Normalize scores for visualization\n    scores = scores[0].detach().numpy()  # Shape: (num_heads, seq_len, seq_len)\n    \n    plt.figure(figsize=(10, 8))\n    sns.heatmap(scores[0], annot=True, fmt=\".2f\", cmap=\"viridis\")  # First head\n    plt.title(title)\n    plt.xlabel(\"Keys\")\n    plt.ylabel(\"Queries\")\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T14:35:31.939554Z","iopub.execute_input":"2025-03-26T14:35:31.939909Z","iopub.status.idle":"2025-03-26T14:35:31.944892Z","shell.execute_reply.started":"2025-03-26T14:35:31.939884Z","shell.execute_reply":"2025-03-26T14:35:31.943965Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Attention","metadata":{}},{"cell_type":"markdown","source":"## Initialize bert and set output attentions to True","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import BertModel, BertTokenizer\n\nclass BERTClassifierWithAttention(nn.Module):\n    def __init__(self, num_classes=4):\n        super(BERTClassifierWithAttention, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)\n        self.fc = nn.Linear(768, num_classes)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        logits = self.fc(outputs.pooler_output)\n        \n        # Extract attention weights (normalized scores)\n        attentions = outputs.attentions  # Tuple of attention weights for each layer\n        \n        # Extract Q, K, V matrices manually\n        qkv_values = []\n        for layer in self.bert.encoder.layer:\n            attn = layer.attention.self\n            q = attn.query(outputs.last_hidden_state)  # Query projection\n            k = attn.key(outputs.last_hidden_state)    # Key projection\n            v = attn.value(outputs.last_hidden_state)  # Value projection\n            qkv_values.append((q.detach(), k.detach(), v.detach()))\n        \n        return logits, attentions, qkv_values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T02:47:45.497335Z","iopub.execute_input":"2025-03-27T02:47:45.497631Z","iopub.status.idle":"2025-03-27T02:47:45.503687Z","shell.execute_reply.started":"2025-03-27T02:47:45.497609Z","shell.execute_reply":"2025-03-27T02:47:45.502688Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load the dataset and set up the finetuning environment","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AdamW\n\n# Custom Dataset\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len=512):\n        self.texts = texts.tolist() if hasattr(texts, 'tolist') else texts\n        self.labels = labels.tolist() if hasattr(labels, 'tolist') else labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, index):\n        text = self.texts[index]\n        label = self.labels[index]\n        encoding = self.tokenizer(\n            text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_len,\n            return_tensors='pt'\n        )\n        return {\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n\n# Prepare Data\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\ntrain_dataset = TextDataset(X_train, y_train, tokenizer)\nval_dataset = TextDataset(X_val, y_val, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n\n# Initialize Model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = BERTClassifierWithAttention(num_classes=4).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = AdamW(model.parameters(), lr=2e-5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T02:47:46.215625Z","iopub.execute_input":"2025-03-27T02:47:46.215907Z","iopub.status.idle":"2025-03-27T02:47:49.347134Z","shell.execute_reply.started":"2025-03-27T02:47:46.215886Z","shell.execute_reply":"2025-03-27T02:47:49.346264Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train it\n> Run a few epochs on our dataset","metadata":{}},{"cell_type":"code","source":"# Training Loop\ndef train_model(epochs=3):\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        for batch in train_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n\n            optimizer.zero_grad()\n            logits, _, _ = model(input_ids, attention_mask)  # Ignore attentions and QKV during training\n            loss = criterion(logits, labels)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}\")\n\n# Run Training\ntrain_model(epochs=20)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T02:47:49.745568Z","iopub.execute_input":"2025-03-27T02:47:49.745863Z","iopub.status.idle":"2025-03-27T03:01:27.782502Z","shell.execute_reply.started":"2025-03-27T02:47:49.745842Z","shell.execute_reply":"2025-03-27T03:01:27.781580Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Generate Embeddings from finetuned model","metadata":{}},{"cell_type":"code","source":"def generate_embeddings(texts, tokenizer, model, batch_size=16, max_len=512):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    model.eval()\n\n    embeddings = []\n\n    with torch.no_grad():\n        for i in range(0, len(texts), batch_size):\n            batch_texts = texts[i:i+batch_size]\n            inputs = tokenizer(batch_texts, padding=True, truncation=True, max_length=max_len, return_tensors='pt')\n            input_ids = inputs['input_ids'].to(device)\n            attention_mask = inputs['attention_mask'].to(device)\n            \n            outputs = model.bert(input_ids=input_ids, attention_mask=attention_mask)\n            batch_embeddings = outputs.pooler_output.cpu().numpy()  # Use pooler_output\n            embeddings.extend(batch_embeddings)\n    \n    return embeddings\n\n# Generate embeddings for train and test sets\nX_train_embeddings = generate_embeddings(X_train.tolist(), tokenizer, model)\nX_val_embeddings = generate_embeddings(X_val.tolist(), tokenizer, model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T03:03:19.191402Z","iopub.execute_input":"2025-03-27T03:03:19.191693Z","iopub.status.idle":"2025-03-27T03:03:21.887802Z","shell.execute_reply.started":"2025-03-27T03:03:19.191672Z","shell.execute_reply":"2025-03-27T03:03:21.886875Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Save model","metadata":{}},{"cell_type":"markdown","source":"## Classify embeddings using SVM","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n\n# Train the SVM\nsvm_classifier = SVC(kernel='linear')\nsvm_classifier.fit(X_train_embeddings, y_train)\n\n# Predict and Evaluate\ny_pred = svm_classifier.predict(X_val_embeddings)\naccuracy = accuracy_score(y_val, y_pred)\n\nprint(f\"Validation Accuracy with SVM: {accuracy * 100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T03:03:24.741902Z","iopub.execute_input":"2025-03-27T03:03:24.742294Z","iopub.status.idle":"2025-03-27T03:03:24.758659Z","shell.execute_reply.started":"2025-03-27T03:03:24.742264Z","shell.execute_reply":"2025-03-27T03:03:24.757553Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Confusion Matrix","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve\n\n\n# Confusion Matrix\nconf_matrix = confusion_matrix(y_val, y_pred)\nplt.figure(figsize=(8,6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()\n\n# Classification Report\nprint(\"Classification Report:\\n\")\nprint(classification_report(y_val, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T03:03:26.873866Z","iopub.execute_input":"2025-03-27T03:03:26.874176Z","iopub.status.idle":"2025-03-27T03:03:27.097346Z","shell.execute_reply.started":"2025-03-27T03:03:26.874152Z","shell.execute_reply":"2025-03-27T03:03:27.096550Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Analyze misclassified","metadata":{}},{"cell_type":"code","source":"misclassified_indices = np.where(y_val != y_pred)[0]\nfor idx in misclassified_indices[:10]:  # Inspect first 10 misclassified samples\n    print(f\"Text: {X_val.iloc[idx]}\")\n    print(f\"True Label: {y_val.iloc[idx]}, Predicted Label: {y_pred[idx]}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T03:03:32.977303Z","iopub.execute_input":"2025-03-27T03:03:32.977606Z","iopub.status.idle":"2025-03-27T03:03:32.985614Z","shell.execute_reply.started":"2025-03-27T03:03:32.977575Z","shell.execute_reply":"2025-03-27T03:03:32.984994Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Extract attentions and Q, K and V values","metadata":{}},{"cell_type":"code","source":"def extract_attention_and_qkv(model, tokenizer, text):\n    encoding = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n    input_ids = encoding['input_ids'].to(device)\n    attention_mask = encoding['attention_mask'].to(device)\n\n    with torch.no_grad():\n        _, attentions, qkv_values = model(input_ids, attention_mask)\n    \n    return attentions, qkv_values\n\n# Example usage\ntext = \"\"\"\n#RussiansAreOurBrothers #UkrainiansAreOurBrothers.\n\"\"\"\nattentions, qkv_values = extract_attention_and_qkv(model, tokenizer, text)\n\n# Inspect Q, K, V values for the first layer\nq, k, v = qkv_values[0]\nprint(\"Query Matrix:\", q)\nprint(\"Key Matrix:\", k)\nprint(\"Value Matrix:\", v)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T03:33:06.099470Z","iopub.execute_input":"2025-03-27T03:33:06.099776Z","iopub.status.idle":"2025-03-27T03:33:06.128625Z","shell.execute_reply.started":"2025-03-27T03:33:06.099748Z","shell.execute_reply":"2025-03-27T03:33:06.127765Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(attentions)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-27T04:07:36.953Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualizing","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nimport torch\n\ndef generate_word_embeddings(text, tokenizer, model):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    model.eval()\n\n    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n    input_ids = inputs['input_ids'].to(device)\n    attention_mask = inputs['attention_mask'].to(device)\n\n    with torch.no_grad():\n        outputs = model.bert(input_ids=input_ids, attention_mask=attention_mask)\n        last_hidden_state = outputs.last_hidden_state.cpu().numpy()[0]\n\n    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n    return tokens, last_hidden_state\n\ndef plot_word_embeddings(tokens, embeddings):\n    # Reduce dimensions to 2D using PCA\n    pca = PCA(n_components=2)\n    reduced_embeddings = pca.fit_transform(embeddings)\n\n    # Plot the embeddings\n    plt.figure(figsize=(12, 8))\n    plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], color='blue')\n\n    for i, token in enumerate(tokens):\n        plt.annotate(token, (reduced_embeddings[i, 0], reduced_embeddings[i, 1]), fontsize=10)\n\n    plt.xlabel('PCA Component 1')\n    plt.ylabel('PCA Component 2')\n    plt.title('2D Word Embedding Visualization')\n    plt.grid()\n    plt.show()\n\n# Example usage\ntext = \"#RussiansAreOurBrothers #UkrainiansAreOurBrothers.\"\ntokens, embeddings = generate_word_embeddings(text, tokenizer, model)\nplot_word_embeddings(tokens, embeddings)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T03:09:06.281120Z","iopub.execute_input":"2025-03-27T03:09:06.281416Z","iopub.status.idle":"2025-03-27T03:09:06.653960Z","shell.execute_reply.started":"2025-03-27T03:09:06.281393Z","shell.execute_reply":"2025-03-27T03:09:06.653073Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import BertTokenizer, BertModel\nimport torch\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n# Load non-finetuned BERT model and tokenizer\nnon_finetuned_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nnon_finetuned_model = BertModel.from_pretrained('bert-base-uncased')\n\ndef generate_non_finetuned_embeddings(text, tokenizer, model):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    model.eval()\n\n    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512, add_special_tokens=True)\n    input_ids = inputs['input_ids'].to(device)\n    attention_mask = inputs['attention_mask'].to(device)\n\n    with torch.no_grad():\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        embeddings = outputs.last_hidden_state.squeeze(0).cpu().numpy()\n\n    tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze(0).tolist())\n    return embeddings, tokens\n\ndef plot_embeddings_with_labels(embeddings, tokens, title):\n    pca = PCA(n_components=2)\n    reduced_embeddings = pca.fit_transform(embeddings)\n\n    plt.figure(figsize=(10, 8))\n    plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], marker='o', c='blue')\n\n    for i, token in enumerate(tokens):\n        plt.annotate(token, (reduced_embeddings[i, 0], reduced_embeddings[i, 1]), fontsize=9)\n\n    plt.title(title)\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n    plt.grid(True)\n    plt.show()\n\n# Example text\ntext = \"#RussiansAreOurBrothers #UkrainiansAreOurBrothers.\"\n\n# Generate and plot embeddings for non-finetuned BERT\nnon_finetuned_embeddings, tokens = generate_non_finetuned_embeddings(text, non_finetuned_tokenizer, non_finetuned_model)\nplot_embeddings_with_labels(non_finetuned_embeddings, tokens, 'Non-Finetuned BERT Embeddings with Annotations')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T03:14:46.591822Z","iopub.execute_input":"2025-03-27T03:14:46.592125Z","iopub.status.idle":"2025-03-27T03:14:47.251800Z","shell.execute_reply.started":"2025-03-27T03:14:46.592102Z","shell.execute_reply":"2025-03-27T03:14:47.251015Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef plot_attention_scores(attentions, layer_idx=0, head_idx=0):\n    \"\"\"\n    Plots attention scores for a specific layer and head.\n    \"\"\"\n    # Extract attention weights for the specified layer and head\n    attention_weights = attentions[layer_idx][0, head_idx].detach().cpu().numpy()  # Shape: (seq_len, seq_len)\n    \n    # Plot heatmap\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(attention_weights, annot=True, fmt=\".2f\", cmap=\"viridis\")\n    plt.title(f\"Attention Scores (Layer {layer_idx}, Head {head_idx})\")\n    plt.xlabel(\"Keys\")\n    plt.ylabel(\"Queries\")\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Markdown","metadata":{}},{"cell_type":"code","source":"\n# Plot attention scores for the first layer and first head\nplot_attention_scores(attentions, layer_idx=0, head_idx=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T03:33:16.241521Z","iopub.execute_input":"2025-03-27T03:33:16.241856Z","iopub.status.idle":"2025-03-27T03:33:17.499598Z","shell.execute_reply.started":"2025-03-27T03:33:16.241827Z","shell.execute_reply":"2025-03-27T03:33:17.498770Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Query Values","metadata":{}},{"cell_type":"code","source":"def plot_query_values(q, tokenized_text, layer_idx=0, head_idx=0):\n    \"\"\"\n    Plots query values for a specific layer and head.\n    \"\"\"\n    # Extract query values for the specified layer and head\n    q_values = q[layer_idx][0, :, head_idx].detach().cpu().numpy()  # Shape: (seq_len,)\n    \n    # Decode tokenized text\n    tokens = tokenizer.convert_ids_to_tokens(tokenized_text['input_ids'][0])\n    \n    # Plot bar chart\n    plt.figure(figsize=(12, 6))\n    plt.bar(tokens, q_values, color='skyblue')\n    plt.title(f\"Query Values (Layer {layer_idx}, Head {head_idx})\")\n    plt.xlabel(\"Tokens\")\n    plt.ylabel(\"Query Value\")\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T03:01:37.103372Z","iopub.execute_input":"2025-03-27T03:01:37.103613Z","iopub.status.idle":"2025-03-27T03:01:37.108781Z","shell.execute_reply.started":"2025-03-27T03:01:37.103590Z","shell.execute_reply":"2025-03-27T03:01:37.107909Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef plot_qkv_matrix_with_tokens(matrix, tokens, title=\"Matrix\", cmap=\"viridis\"):\n    \"\"\"\n    Plots a heatmap of the given Q, K, or V matrix with token labels.\n    \"\"\"\n    # Convert matrix to numpy array if it's a PyTorch tensor\n    if isinstance(matrix, torch.Tensor):\n        matrix = matrix.detach().cpu().numpy()\n    \n    # Plot heatmap\n    plt.figure(figsize=(12, 6))\n    sns.heatmap(matrix, cmap=cmap, yticklabels=tokens)\n    plt.title(title)\n    plt.xlabel(\"Hidden Dimensions\")\n    plt.ylabel(\"Tokens\")\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T03:01:37.109570Z","iopub.execute_input":"2025-03-27T03:01:37.109769Z","iopub.status.idle":"2025-03-27T03:01:37.119383Z","shell.execute_reply.started":"2025-03-27T03:01:37.109751Z","shell.execute_reply":"2025-03-27T03:01:37.118398Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n\n# Tokenize the input text\ntext = \"\"\"\n#RussiansAreOurBrothers #UkrainiansAreOurBrothers.\n\"\"\"\nencoding = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n\n# Extract tokens\ntokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])  # List of tokens\nprint(\"Tokens:\", tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T03:01:37.120158Z","iopub.execute_input":"2025-03-27T03:01:37.120370Z","iopub.status.idle":"2025-03-27T03:01:37.137641Z","shell.execute_reply.started":"2025-03-27T03:01:37.120337Z","shell.execute_reply":"2025-03-27T03:01:37.136816Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extract attention scores and Q, K, V values\nattentions, qkv_values = extract_attention_and_qkv(model, tokenizer, text)\n\n# Inspect Q, K, V values for the first layer\nq, k, v = qkv_values[0]\nprint(\"Query Matrix Shape:\", q.shape)\nprint(\"Key Matrix Shape:\", k.shape)\nprint(\"Value Matrix Shape:\", v.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T03:01:37.138512Z","iopub.execute_input":"2025-03-27T03:01:37.138786Z","iopub.status.idle":"2025-03-27T03:01:37.165077Z","shell.execute_reply.started":"2025-03-27T03:01:37.138760Z","shell.execute_reply":"2025-03-27T03:01:37.164456Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Move tensors to CPU and convert to numpy arrays\nq_matrix = q[0, 0].detach().cpu().numpy()  # Shape: (seq_len, head_dim)\nk_matrix = k[0, 0].detach().cpu().numpy()  # Shape: (seq_len, head_dim)\nv_matrix = v[0, 0].detach().cpu().numpy()  # Shape: (seq_len, head_dim)\n\nprint(\"Query Matrix Shape:\", q_matrix.shape)\nprint(\"Key Matrix Shape:\", k_matrix.shape)\nprint(\"Value Matrix Shape:\", v_matrix.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T03:01:37.165694Z","iopub.execute_input":"2025-03-27T03:01:37.165864Z","iopub.status.idle":"2025-03-27T03:01:37.171629Z","shell.execute_reply.started":"2025-03-27T03:01:37.165849Z","shell.execute_reply":"2025-03-27T03:01:37.170887Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef plot_attention_scores(attentions, tokens, layer_idx=0, head_idx=0):\n    \"\"\"\n    Plots attention scores for a specific layer and head with tokens as labels.\n    attentions: Attention weights from BERT output (shape: [layers, batch, heads, seq_len, seq_len])\n    tokens: Tokenized input (list of token strings)\n    layer_idx: Index of the layer to visualize\n    head_idx: Index of the head to visualize\n    \"\"\"\n    # Extract attention weights for the specified layer and head\n    attention_weights = attentions[layer_idx][0, head_idx].detach().cpu().numpy()\n\n    # Plot heatmap\n    plt.figure(figsize=(12, 10))\n    sns.heatmap(\n        attention_weights, \n        annot=True, \n        fmt=\".2f\", \n        cmap=\"viridis\",\n        xticklabels=tokens, \n        yticklabels=tokens\n    )\n    plt.title(f\"Attention Scores (Layer {layer_idx}, Head {head_idx})\")\n    plt.xlabel(\"Keys (Words being attended to)\")\n    plt.ylabel(\"Queries (Words attending)\")\n    plt.xticks(rotation=45)\n    plt.yticks(rotation=45)\n    plt.show()\n\n# Example usage\n# Assuming `attentions` is the attention tensor from BERT and `tokens` is the tokenized input\nplot_attention_scores(attentions, tokens, layer_idx=0, head_idx=0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T03:43:07.979870Z","iopub.execute_input":"2025-03-27T03:43:07.980301Z","iopub.status.idle":"2025-03-27T03:43:08.947502Z","shell.execute_reply.started":"2025-03-27T03:43:07.980270Z","shell.execute_reply":"2025-03-27T03:43:08.946581Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}