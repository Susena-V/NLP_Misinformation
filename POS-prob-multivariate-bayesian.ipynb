{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10785517,"sourceType":"datasetVersion","datasetId":6693003},{"sourceId":10789676,"sourceType":"datasetVersion","datasetId":6695691}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import spacy\nspacy.cli.download(\"en_core_web_md\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T04:19:45.821985Z","iopub.execute_input":"2025-03-06T04:19:45.822197Z","iopub.status.idle":"2025-03-06T04:19:59.887751Z","shell.execute_reply.started":"2025-03-06T04:19:45.822173Z","shell.execute_reply":"2025-03-06T04:19:59.886486Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport nltk\nimport spacy\nimport gensim\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom gensim.models import Word2Vec\nfrom nltk.tokenize import word_tokenize\nfrom nltk import pos_tag, ngrams, FreqDist, ConditionalFreqDist\nfrom collections import defaultdict\n\n# Load dataset\ndata = pd.read_csv(\"/kaggle/input/filtered-and-translated-nlp/filr.csv\")\ntexts = data[\"Translated\"].astype(str).tolist()\nlabels = data[\"Label\"].tolist()\ntokenized_texts = [word_tokenize(text.lower()) for text in texts]\n\n# POS Tagging\npos_tagged_texts = [pos_tag(tokens) for tokens in tokenized_texts]\n\n### 1. Word2Vec + Classifiers (SVC & RF) ###\nword2vec_model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)\n\ndef get_w2v_features(tokens):\n    return np.mean([word2vec_model.wv[word] for word in tokens if word in word2vec_model.wv], axis=0)\n\nX_w2v = np.array([get_w2v_features(tokens) if len(tokens) > 0 else np.zeros(100) for tokens in tokenized_texts])\nX_train, X_test, y_train, y_test = train_test_split(X_w2v, labels, test_size=0.2, random_state=42)\n\nsvm_classifier = SVC(kernel='linear', probability=True)\nrandom_forest_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\nsvm_classifier.fit(X_train, y_train)\nrandom_forest_classifier.fit(X_train, y_train)\n\ny_pred_svm = svm_classifier.predict(X_test)\ny_pred_rf = random_forest_classifier.predict(X_test)\n\n### 2. GloVe + Classifiers (SVC & RF) ###\nnlp = spacy.load(\"en_core_web_md\")  # Pre-trained GloVe vectors\n\ndef get_glove_features(text):\n    return nlp(text).vector\n\nX_glove = np.array([get_glove_features(text) for text in texts])\nX_train_glove, X_test_glove, y_train_glove, y_test_glove = train_test_split(X_glove, labels, test_size=0.2, random_state=42)\n\nsvm_classifier_glove = SVC(kernel='linear', probability=True)\nrandom_forest_classifier_glove = RandomForestClassifier(n_estimators=100, random_state=42)\nsvm_classifier_glove.fit(X_train_glove, y_train_glove)\nrandom_forest_classifier_glove.fit(X_train_glove, y_train_glove)\n\ny_pred_svm_glove = svm_classifier_glove.predict(X_test_glove)\ny_pred_rf_glove = random_forest_classifier_glove.predict(X_test_glove)\n\npredictions = {\n    \"SVM (Word2Vec)\": (y_pred_svm, svm_classifier, X_w2v),\n    \"Random Forest (Word2Vec)\": (y_pred_rf, random_forest_classifier, X_w2v),\n    \"SVM (GloVe)\": (y_pred_svm_glove, svm_classifier_glove, X_glove),\n    \"Random Forest (GloVe)\": (y_pred_rf_glove, random_forest_classifier_glove, X_glove),\n}\n\ncorrectly_classified_sentences = {}\nmisclassified_sentences = {}\n\nfor model, (preds, classifier, features) in predictions.items():\n    correctly_classified_indices = np.where(preds == y_test)[0]\n    misclassified_indices = np.where(preds != y_test)[0]\n    correctly_classified_sentences[model] = [(texts[i], pos_tagged_texts[i], classifier.predict_proba([features[i]])[0]) for i in correctly_classified_indices]\n    misclassified_sentences[model] = [(texts[i], pos_tagged_texts[i], classifier.predict_proba([features[i]])[0]) for i in misclassified_indices]\n\nfor model in correctly_classified_sentences:\n    for text, correct_tags, correct_probs in correctly_classified_sentences[model]:\n        for other_model in misclassified_sentences:\n            misclassified_entries = [entry for entry in misclassified_sentences[other_model] if entry[0] == text]\n            for misclassified_entry in misclassified_entries:\n                _, misclassified_tags, misclassified_probs = misclassified_entry\n                print(f\"Sentence: {text}\")\n                print(f\"Correctly classified by: {model}\")\n                print(f\"Misclassified by: {other_model}\")\n                print(f\"Correct POS Tags & Probabilities:\")\n                for (word, tag), prob in zip(correct_tags, correct_probs):\n                    print(f\"{word} ({tag}) → {prob:.2f}\")\n                print(f\"Misclassified POS Tags & Probabilities:\")\n                for (word, tag), prob in zip(misclassified_tags, misclassified_probs):\n                    print(f\"{word} ({tag}) → {prob:.2f}\")\n                print(\"-\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T04:25:26.843750Z","iopub.execute_input":"2025-03-06T04:25:26.844053Z","iopub.status.idle":"2025-03-06T04:25:36.364735Z","shell.execute_reply.started":"2025-03-06T04:25:26.844031Z","shell.execute_reply":"2025-03-06T04:25:36.363860Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport nltk\nimport spacy\nimport gensim\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom gensim.models import Word2Vec\nfrom nltk.tokenize import word_tokenize\nfrom nltk import pos_tag, ngrams, FreqDist, ConditionalFreqDist\nfrom collections import defaultdict\n\n# Load dataset\ndata = pd.read_csv(\"/kaggle/input/filtered-and-translated-nlp/filr.csv\")\ntexts = data[\"Translated\"].astype(str).tolist()\nlabels = data[\"Label\"].tolist()\ntokenized_texts = [word_tokenize(text.lower()) for text in texts]\n\n# POS Tagging\npos_tagged_texts = [pos_tag(tokens) for tokens in tokenized_texts]\n\n### 1. Word2Vec + Classifiers (SVC & RF) ###\nword2vec_model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)\n\ndef get_w2v_features(tokens):\n    return np.mean([word2vec_model.wv[word] for word in tokens if word in word2vec_model.wv], axis=0)\n\nX_w2v = np.array([get_w2v_features(tokens) if len(tokens) > 0 else np.zeros(100) for tokens in tokenized_texts])\nX_train, X_test, y_train, y_test = train_test_split(X_w2v, labels, test_size=0.2, random_state=42)\n\nsvm_classifier = SVC(kernel='linear', probability=True)\nrandom_forest_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\nsvm_classifier.fit(X_train, y_train)\nrandom_forest_classifier.fit(X_train, y_train)\n\ny_pred_svm = svm_classifier.predict(X_test)\ny_pred_rf = random_forest_classifier.predict(X_test)\n\ndef evaluate_model(y_test, y_pred, model_name):\n    print(f\"{model_name} Classification Report:\")\n    print(classification_report(y_test, y_pred))\n    conf_matrix = confusion_matrix(y_test, y_pred)\n    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.title(f'{model_name} Confusion Matrix')\n    plt.show()\n\nevaluate_model(y_test, y_pred_svm, \"SVM (Word2Vec)\")\nevaluate_model(y_test, y_pred_rf, \"Random Forest (Word2Vec)\")\n\n### 2. GloVe + Classifiers (SVC & RF) ###\nnlp = spacy.load(\"en_core_web_md\")  # Pre-trained GloVe vectors\n\ndef get_glove_features(text):\n    return nlp(text).vector\n\nX_glove = np.array([get_glove_features(text) for text in texts])\nX_train_glove, X_test_glove, y_train_glove, y_test_glove = train_test_split(X_glove, labels, test_size=0.2, random_state=42)\n\nsvm_classifier_glove = SVC(kernel='linear', probability=True)\nrandom_forest_classifier_glove = RandomForestClassifier(n_estimators=100, random_state=42)\nsvm_classifier_glove.fit(X_train_glove, y_train_glove)\nrandom_forest_classifier_glove.fit(X_train_glove, y_train_glove)\n\ny_pred_svm_glove = svm_classifier_glove.predict(X_test_glove)\ny_pred_rf_glove = random_forest_classifier_glove.predict(X_test_glove)\n\nevaluate_model(y_test_glove, y_pred_svm_glove, \"SVM (GloVe)\")\nevaluate_model(y_test_glove, y_pred_rf_glove, \"Random Forest (GloVe)\")\n\n### 3. Extracting POS Tag Variations ###\ndef extract_possible_tags(pos_tagged_tokens):\n    tag_variants = defaultdict(set)\n    for word, tag in pos_tagged_tokens:\n        tag_variants[word].add(tag)\n    return tag_variants\n\ncorrectly_classified_indices = np.where(y_pred_svm == y_test)[0]\nmisclassified_indices = np.where(y_pred_svm != y_test)[0]\n\ncorrectly_classified_tags = [extract_possible_tags(pos_tagged_texts[i]) for i in correctly_classified_indices]\nmisclassified_tags = [extract_possible_tags(pos_tagged_texts[i]) for i in misclassified_indices]\n\nword_tag_dist = ConditionalFreqDist()\nfor tag_set in correctly_classified_tags:\n    for word, tags in tag_set.items():\n        for tag in tags:\n            word_tag_dist[word][tag] += 1\n\nmisclassified_word_tag_dist = ConditionalFreqDist()\nfor tag_set in misclassified_tags:\n    for word, tags in tag_set.items():\n        for tag in tags:\n            misclassified_word_tag_dist[word][tag] += 1\n\n### 4. Compare Probability Distributions of POS Tag Variations ###\ndef plot_tag_variation_distribution(word_tag_dist, title):\n    plt.figure(figsize=(12,6))\n    words, variations = zip(*[(word, len(tags)) for word, tags in word_tag_dist.items() if len(tags) > 1])\n    plt.barh(words[:20], variations[:20])\n    plt.xlabel(\"Number of POS Tag Variations\")\n    plt.ylabel(\"Words\")\n    plt.title(title)\n    plt.gca().invert_yaxis()\n    plt.show()\n\nplot_tag_variation_distribution(word_tag_dist, \"Correctly Classified: POS Tag Variations\")\nplot_tag_variation_distribution(misclassified_word_tag_dist, \"Misclassified: POS Tag Variations\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T04:26:16.663736Z","iopub.execute_input":"2025-03-06T04:26:16.664028Z","iopub.status.idle":"2025-03-06T04:26:26.148713Z","shell.execute_reply.started":"2025-03-06T04:26:16.664006Z","shell.execute_reply":"2025-03-06T04:26:26.147289Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\n\n# Collect data for visualization\ncorrect_glove = []\nwrong_glove = []\ncorrect_w2v = []\nwrong_w2v = []\ntrue_labels = []\n\nfor text, _, glove_features in correctly_classified_sentences.get(\"SVM (GloVe)\", []):\n    correct_glove.append(glove_features)\n    true_labels.append(data.loc[data[\"Translated\"] == text, \"Label\"].values[0])\nfor text, _, glove_features in misclassified_sentences.get(\"SVM (GloVe)\", []):\n    wrong_glove.append(glove_features)\n    true_labels.append(data.loc[data[\"Translated\"] == text, \"Label\"].values[0])\nfor text, _, w2v_features in correctly_classified_sentences.get(\"SVM (Word2Vec)\", []):\n    correct_w2v.append(w2v_features)\n    true_labels.append(data.loc[data[\"Translated\"] == text, \"Label\"].values[0])\nfor text, _, w2v_features in misclassified_sentences.get(\"SVM (Word2Vec)\", []):\n    wrong_w2v.append(w2v_features)\n    true_labels.append(data.loc[data[\"Translated\"] == text, \"Label\"].values[0])\n\n# Convert lists to arrays\nlabel_map = {\n    \"Correct GloVe\": correct_glove,\n    \"Wrong GloVe\": wrong_glove,\n    \"Correct Word2Vec\": correct_w2v,\n    \"Wrong Word2Vec\": wrong_w2v\n}\n\n# Filter out empty arrays before concatenating\nall_features = []\nlabels = []\nall_true_labels = []  # Store corresponding labels\n\nfor label, array in label_map.items():\n    array = np.array(array)\n    if array.shape[0] > 0:\n        all_features.append(array)\n        labels.extend([label] * len(array))\n        all_true_labels.extend(true_labels[:len(array)])\n        true_labels = true_labels[len(array):]  # Shift to match remaining data\n\n# Ensure all_features is not empty before applying t-SNE\nif all_features:\n    all_features = np.vstack(all_features)\n    \n    # Apply t-SNE\n    tsne = TSNE(n_components=2, random_state=42)\n    reduced_features = tsne.fit_transform(all_features)\n\n    # Define colors for each embedding type\n    palette = {\n        \"Correct GloVe\": \"green\",\n        \"Wrong GloVe\": \"red\",\n        \"Correct Word2Vec\": \"blue\",\n        \"Wrong Word2Vec\": \"purple\"\n    }\n\n    # Define marker styles based on label classes\n    unique_labels = list(set(all_true_labels))\n    markers = ['o', 's', '^', 'D', 'v', 'P', '*', 'X', '<', '>']  # Assign unique markers\n    marker_map = {label: markers[i % len(markers)] for i, label in enumerate(unique_labels)}\n\n    # Plot\n    plt.figure(figsize=(10, 6))\n    sns.scatterplot(x=reduced_features[:, 0], y=reduced_features[:, 1], hue=labels, palette=palette, style=all_true_labels, markers=marker_map)\n    \n    plt.title(\"Comparison of Word Embeddings with True Labels\")\n    plt.xlabel(\"t-SNE Component 1\")\n    plt.ylabel(\"t-SNE Component 2\")\n    plt.legend(title=\"Embedding Type & True Labels\", bbox_to_anchor=(1.05, 1), loc='upper left')\n    \n    plt.show()\nelse:\n    print(\"No data available for visualization.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T04:26:27.723192Z","iopub.execute_input":"2025-03-06T04:26:27.723515Z","iopub.status.idle":"2025-03-06T04:26:28.760442Z","shell.execute_reply.started":"2025-03-06T04:26:27.723492Z","shell.execute_reply":"2025-03-06T04:26:28.759421Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Mutlivariate Bayesian","metadata":{}},{"cell_type":"code","source":"from collections import defaultdict\n\n# Prepare data grouped by POS tag\npos_tagged_vectors = defaultdict(list)\n\nfor tokens, pos_tags in zip(tokenized_texts, pos_tagged_texts):\n    for word, (original_word, pos_tag_label) in zip(tokens, pos_tags):\n        if word in word2vec_model.wv:\n            vector = word2vec_model.wv[word]\n            pos_tagged_vectors[pos_tag_label].append(vector)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T04:36:46.845170Z","iopub.execute_input":"2025-03-06T04:36:46.845501Z","iopub.status.idle":"2025-03-06T04:36:46.885799Z","shell.execute_reply.started":"2025-03-06T04:36:46.845475Z","shell.execute_reply":"2025-03-06T04:36:46.884674Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from scipy.stats import multivariate_normal\n\ngaussian_models = {}\n\nfor pos_tag_label, vectors in pos_tagged_vectors.items():\n    vectors = np.array(vectors)\n    \n    if len(vectors) < 2:\n        # Not enough samples for covariance, handle gracefully\n        print(f\"Skipping POS tag '{pos_tag_label}' due to insufficient data ({len(vectors)} sample).\")\n        continue\n    \n    mean_vector = np.mean(vectors, axis=0)\n    cov_matrix = np.cov(vectors, rowvar=False)\n    \n    # Fix for numerical stability\n    if cov_matrix.ndim == 0:  # In case it's scalar\n        cov_matrix = np.eye(vectors.shape[1]) * 1e-6\n    else:\n        cov_matrix += np.eye(cov_matrix.shape[0]) * 1e-6\n    \n    gaussian_models[pos_tag_label] = multivariate_normal(mean=mean_vector, cov=cov_matrix)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T04:37:23.494117Z","iopub.execute_input":"2025-03-06T04:37:23.494431Z","iopub.status.idle":"2025-03-06T04:37:23.584291Z","shell.execute_reply.started":"2025-03-06T04:37:23.494404Z","shell.execute_reply":"2025-03-06T04:37:23.583448Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if len(vectors) < 2:\n    mean_vector = vectors[0]\n    cov_matrix = np.eye(len(mean_vector)) * 1e-6\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T04:37:43.759614Z","iopub.execute_input":"2025-03-06T04:37:43.759921Z","iopub.status.idle":"2025-03-06T04:37:43.763825Z","shell.execute_reply.started":"2025-03-06T04:37:43.759898Z","shell.execute_reply":"2025-03-06T04:37:43.762958Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Multivariate try again","metadata":{}},{"cell_type":"code","source":"from collections import defaultdict\nimport numpy as np\n\n# Collect embeddings for each POS tag\npos_embeddings = defaultdict(list)\n\nfor sentence in pos_tagged_texts:\n    for word, pos_tag in sentence:\n        embedding = get_word2vec_embedding(word)\n        pos_embeddings[pos_tag].append(embedding)\n\n# Convert lists to arrays\nfor pos_tag in pos_embeddings:\n    pos_embeddings[pos_tag] = np.array(pos_embeddings[pos_tag])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T04:40:26.355012Z","iopub.execute_input":"2025-03-06T04:40:26.355349Z","iopub.status.idle":"2025-03-06T04:40:26.412358Z","shell.execute_reply.started":"2025-03-06T04:40:26.355325Z","shell.execute_reply":"2025-03-06T04:40:26.411308Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from scipy.stats import multivariate_normal\n\ngaussian_models = {}\n\nfor pos_tag, embeddings in pos_embeddings.items():\n    if len(embeddings) < 2:\n        print(f\"Skipping POS tag '{pos_tag}' due to insufficient data ({len(embeddings)} sample(s)).\")\n        continue\n    \n    mean_vector = np.mean(embeddings, axis=0)\n    cov_matrix = np.cov(embeddings, rowvar=False)\n\n    # Handle singular covariance matrices\n    if cov_matrix.ndim == 0 or cov_matrix.shape[0] == 0:\n        print(f\"Skipping POS tag '{pos_tag}' due to invalid covariance matrix.\")\n        continue\n    cov_matrix += np.eye(cov_matrix.shape[0]) * 1e-6\n\n    gaussian_models[pos_tag] = multivariate_normal(mean=mean_vector, cov=cov_matrix)\n\nprint(f\"Trained {len(gaussian_models)} POS tag Gaussian models.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T04:40:32.784310Z","iopub.execute_input":"2025-03-06T04:40:32.784592Z","iopub.status.idle":"2025-03-06T04:40:32.861649Z","shell.execute_reply.started":"2025-03-06T04:40:32.784574Z","shell.execute_reply":"2025-03-06T04:40:32.860783Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def sentence_log_probability(pos_tagged_sentence, gaussian_models):\n    total_log_prob = 0\n\n    for word, pos_tag in pos_tagged_sentence:\n        embedding = get_word2vec_embedding(word)\n        \n        if pos_tag in gaussian_models:\n            prob = gaussian_models[pos_tag].pdf(embedding)\n            if prob > 0:\n                total_log_prob += np.log(prob)\n            else:\n                total_log_prob += -1e6  # Penalize zero probabilities\n        else:\n            total_log_prob += -1e6  # Penalize missing POS tags\n    \n    return total_log_prob\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T04:40:46.079169Z","iopub.execute_input":"2025-03-06T04:40:46.079459Z","iopub.status.idle":"2025-03-06T04:40:46.084433Z","shell.execute_reply.started":"2025-03-06T04:40:46.079439Z","shell.execute_reply":"2025-03-06T04:40:46.083337Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\n\n\nsentence_log_probs = []\n\nfor sentence in tqdm(pos_tagged_texts):\n    log_prob = sentence_log_probability(sentence, gaussian_models)\n    sentence_log_probs.append(log_prob)\n\ndata['Sentence_Log_Probability'] = sentence_log_probs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T04:41:50.288953Z","iopub.execute_input":"2025-03-06T04:41:50.289331Z","iopub.status.idle":"2025-03-06T04:41:51.283515Z","shell.execute_reply.started":"2025-03-06T04:41:50.289301Z","shell.execute_reply":"2025-03-06T04:41:51.282655Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Top probable sentences\nprint(data[['Translated', 'Sentence_Log_Probability']].sort_values(by='Sentence_Log_Probability', ascending=False).head())\n\n# Least probable sentences\nprint(data[['Translated', 'Sentence_Log_Probability']].sort_values(by='Sentence_Log_Probability').head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T04:41:59.387897Z","iopub.execute_input":"2025-03-06T04:41:59.388242Z","iopub.status.idle":"2025-03-06T04:41:59.418935Z","shell.execute_reply.started":"2025-03-06T04:41:59.388214Z","shell.execute_reply":"2025-03-06T04:41:59.418106Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Bayesian and Prior Probabilities","metadata":{}},{"cell_type":"code","source":"\n\nimport numpy as np\nfrom collections import defaultdict, Counter\nfrom tqdm import tqdm\nfrom scipy.stats import multivariate_normal\n\n# 1️⃣ Compute prior probabilities of POS tags\nall_pos_tags = [tag for sent in pos_tagged_texts for (_, tag) in sent]\ntotal_tags = len(all_pos_tags)\ntag_counts = Counter(all_pos_tags)\npos_tag_priors = {tag: count / total_tags for tag, count in tag_counts.items()}\n\n# 2️⃣ Train Multivariate Gaussians for each POS tag\npos_tag_embeddings = defaultdict(list)\n\nfor sentence in tqdm(pos_tagged_texts):\n    for word, tag in sentence:\n        if word in word2vec_model.wv:\n            embedding = word2vec_model.wv[word]\n            pos_tag_embeddings[tag].append(embedding)\n\ngaussian_models = {}\nmin_samples = 5  # Ignore tags with too few samples\n\nfor tag, embeddings in pos_tag_embeddings.items():\n    if len(embeddings) < min_samples:\n        print(f\"Skipping POS tag '{tag}' due to insufficient data ({len(embeddings)} samples).\")\n        continue\n    embeddings_array = np.array(embeddings)\n    mean_vector = np.mean(embeddings_array, axis=0)\n    cov_matrix = np.cov(embeddings_array, rowvar=False)\n\n    cov_matrix += np.eye(cov_matrix.shape[0]) * 1e-6\n\n    gaussian_models[tag] = multivariate_normal(mean=mean_vector, cov=cov_matrix)\n\n\n# 3️⃣ Prediction function with priors\ndef predict_pos_tags_with_bayes(tokens):\n    predicted_tags = []\n    for word in tokens:\n        if word in word2vec_model.wv:\n            embedding = word2vec_model.wv[word]\n            tag_scores = {}\n            for tag, gaussian in gaussian_models.items():\n                likelihood = gaussian.pdf(embedding)\n                prior = pos_tag_priors.get(tag, 1e-6)  # Small value if unseen\n                score = likelihood * prior\n                tag_scores[tag] = score\n            predicted_tag = max(tag_scores, key=tag_scores.get)\n        else:\n            predicted_tag = 'UNK'  # Unknown word\n        predicted_tags.append((word, predicted_tag))\n    return predicted_tags\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T04:55:49.472889Z","iopub.execute_input":"2025-03-06T04:55:49.473185Z","iopub.status.idle":"2025-03-06T04:55:49.603143Z","shell.execute_reply.started":"2025-03-06T04:55:49.473167Z","shell.execute_reply":"2025-03-06T04:55:49.602045Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Take a sample sentence from your dataset\nsample_sentence_from_data = pos_tagged_texts[0]  # First sentence in the dataset\nsample_words = [word for word, tag in sample_sentence_from_data]\n\n# Predict POS tags\npredicted_tags = predict_pos_tags_with_bayes(sample_words)\n\n# Show results\nprint(\"\\nPredicted POS tags (with priors) for a sample sentence from the dataset:\")\nfor word, tag in predicted_tags:\n    print(f\"{word} → {tag}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T04:57:52.333533Z","iopub.execute_input":"2025-03-06T04:57:52.333842Z","iopub.status.idle":"2025-03-06T04:57:52.456382Z","shell.execute_reply.started":"2025-03-06T04:57:52.333815Z","shell.execute_reply":"2025-03-06T04:57:52.455428Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Store predictions for all sentences\nall_predictions = []\n\n# Loop through every sentence in your dataset\nfor sentence in tqdm(pos_tagged_texts):\n    words = [word for word, _ in sentence]\n    predicted_tags = predict_pos_tags_with_bayes(words)\n    \n    all_predictions.append({\n        \"sentence\": \" \".join(words),\n        \"predicted_tags\": \" \".join([f\"{word}/{tag}\" for word, tag in predicted_tags])\n    })\n\n# Convert to DataFrame for easy viewing and saving\npredictions_df = pd.DataFrame(all_predictions)\n\n# Display first few predictions\nprint(predictions_df.head())\n\n# ✅ Save to CSV\npredictions_df.to_csv(\"pos_tag_predictions.csv\", index=False)\nprint(\"Predictions saved to pos_tag_predictions.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T04:59:30.058605Z","iopub.execute_input":"2025-03-06T04:59:30.058912Z","iopub.status.idle":"2025-03-06T05:00:02.562657Z","shell.execute_reply.started":"2025-03-06T04:59:30.058887Z","shell.execute_reply":"2025-03-06T05:00:02.561863Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}